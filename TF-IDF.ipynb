{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"/Users/yohannlefaou/Documents/data/posos/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8028, 2)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(path_data +\"input_train.csv\", sep=\",\")\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(path_data + \"output_train.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bonjour,  je m suis trompé de forum pour ma question alors je la repose ici. je pris pour la première fois hier du paroxétine et ce matin c'est une catastrophe. picotement dasn tous le corps annonciateur de sueur froide très très massive et de vomissement. j'en suis à deux crises depuis 5 heure du mat. la cela semble passer mes mes mains reste moites et chaude estce normal pour la première fois merci a tous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>est ce que le motilium me soulagera contre les nausées?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>mon médecin m'a prescrit adenyl. au 2ème cachet des maux de tête terribles et au 3ème palpitations, sueurs froides, chaleur intense dans la tête, tremblements, fourmillements dans la lèvre supérieure, difficultés à respirer.. dès l'arrêt du médicament tous les symptômes ont disparu. cela est-il déjà arrivé à quelqu'un??</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Est-ce qu'il existe une forme adaptée aux enfant de 5ans du Micropakine ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>mon  medecin  me soigne  pour  une  rhino  pharingite  et  m'a  prescrit du amoxicilline comme  anti  biotique. Est-ce vraiment pour cette indication?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>je viens d'apprendre que je suis enceinte..savez-vous si je peux poursuivre le rubozinc ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>atrax n'est-il pas dangereux au long terme ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>je suis sous mercilon. J'ai des nausées et des saignements ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>L'atenolol, c'est quoi ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>je prend trinordiol et à la fin de ma première plaquette j'ai eu ma première fois ac mn copain. la pilule est donc bien efficace dès le debut ? j'ai des douleur dans la poitrine, j'ai l'impression qu'ils ont un peu grossi ossi de plus, j'ai mal o bas du ventr,e dois je minkiété ou c simplemen, la periode dadaptation ??</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>laroxyl à doses faibles pour le stress ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>mon psy me dit de prendre 50mg de sertraline la matin et 50 mg de sertraline le soir. peut on prendre 100mg soit le matin ou a midi?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>je suis sous antiobiotique depuis bientot une semaine et je me suis chopée je ne sais quoi à ma nénétte, ca gratte,c'est superficiel mais ca démenge à un point, est ce lié à l'antibiotique?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>Quel vaccin contre le cancer du col de l'uterus ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>épilepsie et havlane ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>vaccins dtpe et hépatire b sont ils possible pendant la grossesse?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>je viens d'être mis sous androtardyl depuis 1 semaine. je souhaitais savoir au bout de combien d'injections le produit commençait à faire ses effets ? et au bout de combien de jours après chaque injection ses effets se faisaient sentir ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>Comment traiter une dishydrose, si le  Dermoval et une préparation ( sulfate de cuivre 10mg/10L d'eau en bain ) et dermalibour n’ont pas marché ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>un peu de renseignements sur acomplia. j'ai pris mon 1er cachet ce matin.  avez vous eu des effets indésirables?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>oroken quels risques ??</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  \\\n",
       "0   0    \n",
       "1   1    \n",
       "2   2    \n",
       "3   3    \n",
       "4   4    \n",
       "5   5    \n",
       "6   6    \n",
       "7   7    \n",
       "8   8    \n",
       "9   9    \n",
       "10  10   \n",
       "11  11   \n",
       "12  12   \n",
       "13  13   \n",
       "14  14   \n",
       "15  15   \n",
       "16  16   \n",
       "17  17   \n",
       "18  18   \n",
       "19  19   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                      question  \n",
       "0   bonjour,  je m suis trompé de forum pour ma question alors je la repose ici. je pris pour la première fois hier du paroxétine et ce matin c'est une catastrophe. picotement dasn tous le corps annonciateur de sueur froide très très massive et de vomissement. j'en suis à deux crises depuis 5 heure du mat. la cela semble passer mes mes mains reste moites et chaude estce normal pour la première fois merci a tous  \n",
       "1   est ce que le motilium me soulagera contre les nausées?                                                                                                                                                                                                                                                                                                                                                                     \n",
       "2   mon médecin m'a prescrit adenyl. au 2ème cachet des maux de tête terribles et au 3ème palpitations, sueurs froides, chaleur intense dans la tête, tremblements, fourmillements dans la lèvre supérieure, difficultés à respirer.. dès l'arrêt du médicament tous les symptômes ont disparu. cela est-il déjà arrivé à quelqu'un??                                                                                           \n",
       "3   Est-ce qu'il existe une forme adaptée aux enfant de 5ans du Micropakine ?                                                                                                                                                                                                                                                                                                                                                   \n",
       "4   mon  medecin  me soigne  pour  une  rhino  pharingite  et  m'a  prescrit du amoxicilline comme  anti  biotique. Est-ce vraiment pour cette indication?                                                                                                                                                                                                                                                                      \n",
       "5   je viens d'apprendre que je suis enceinte..savez-vous si je peux poursuivre le rubozinc ?                                                                                                                                                                                                                                                                                                                                   \n",
       "6   atrax n'est-il pas dangereux au long terme ?                                                                                                                                                                                                                                                                                                                                                                                \n",
       "7   je suis sous mercilon. J'ai des nausées et des saignements ?                                                                                                                                                                                                                                                                                                                                                                \n",
       "8   L'atenolol, c'est quoi ?                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "9   je prend trinordiol et à la fin de ma première plaquette j'ai eu ma première fois ac mn copain. la pilule est donc bien efficace dès le debut ? j'ai des douleur dans la poitrine, j'ai l'impression qu'ils ont un peu grossi ossi de plus, j'ai mal o bas du ventr,e dois je minkiété ou c simplemen, la periode dadaptation ??                                                                                            \n",
       "10  laroxyl à doses faibles pour le stress ?                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "11  mon psy me dit de prendre 50mg de sertraline la matin et 50 mg de sertraline le soir. peut on prendre 100mg soit le matin ou a midi?                                                                                                                                                                                                                                                                                        \n",
       "12  je suis sous antiobiotique depuis bientot une semaine et je me suis chopée je ne sais quoi à ma nénétte, ca gratte,c'est superficiel mais ca démenge à un point, est ce lié à l'antibiotique?                                                                                                                                                                                                                               \n",
       "13  Quel vaccin contre le cancer du col de l'uterus ?                                                                                                                                                                                                                                                                                                                                                                           \n",
       "14  épilepsie et havlane ?                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "15  vaccins dtpe et hépatire b sont ils possible pendant la grossesse?                                                                                                                                                                                                                                                                                                                                                          \n",
       "16  je viens d'être mis sous androtardyl depuis 1 semaine. je souhaitais savoir au bout de combien d'injections le produit commençait à faire ses effets ? et au bout de combien de jours après chaque injection ses effets se faisaient sentir ?                                                                                                                                                                               \n",
       "17  Comment traiter une dishydrose, si le  Dermoval et une préparation ( sulfate de cuivre 10mg/10L d'eau en bain ) et dermalibour n’ont pas marché ?                                                                                                                                                                                                                                                                           \n",
       "18  un peu de renseignements sur acomplia. j'ai pris mon 1er cachet ce matin.  avez vous eu des effets indésirables?                                                                                                                                                                                                                                                                                                            \n",
       "19  oroken quels risques ??                                                                                                                                                                                                                                                                                                                                                                                                     "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>intention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  intention\n",
       "0   0   28       \n",
       "1   1   31       \n",
       "2   2   28       \n",
       "3   3   44       \n",
       "4   4   31       \n",
       "5   5   44       \n",
       "6   6   48       \n",
       "7   7   28       \n",
       "8   8   22       \n",
       "9   9   23       \n",
       "10  10  31       \n",
       "11  11  42       \n",
       "12  12  28       \n",
       "13  13  32       \n",
       "14  14  28       \n",
       "15  15  44       \n",
       "16  16  26       \n",
       "17  17  0        \n",
       "18  18  28       \n",
       "19  19  48       "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28    1796\n",
       "31    565 \n",
       "44    560 \n",
       "22    461 \n",
       "48    387 \n",
       "34    382 \n",
       "32    321 \n",
       "14    317 \n",
       "42    315 \n",
       "23    208 \n",
       "0     204 \n",
       "11    195 \n",
       "21    180 \n",
       "33    144 \n",
       "8     134 \n",
       "37    126 \n",
       "38    120 \n",
       "13    119 \n",
       "27    104 \n",
       "5     102 \n",
       "43    91  \n",
       "12    89  \n",
       "47    82  \n",
       "29    77  \n",
       "4     76  \n",
       "26    72  \n",
       "10    68  \n",
       "24    64  \n",
       "9     54  \n",
       "30    52  \n",
       "35    50  \n",
       "45    50  \n",
       "41    45  \n",
       "46    45  \n",
       "39    40  \n",
       "50    33  \n",
       "25    30  \n",
       "6     26  \n",
       "36    25  \n",
       "15    23  \n",
       "1     23  \n",
       "18    23  \n",
       "2     22  \n",
       "49    22  \n",
       "7     20  \n",
       "19    20  \n",
       "20    19  \n",
       "16    17  \n",
       "17    15  \n",
       "3     8   \n",
       "40    7   \n",
       "Name: intention, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[\"intention\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_stop_words_fr = []\n",
    "with open('stop-words-fr.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        list_stop_words_fr.append(line.rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'abord',\n",
       " 'absolument',\n",
       " 'afin',\n",
       " 'ah',\n",
       " 'ai',\n",
       " 'aie',\n",
       " 'aient',\n",
       " 'aies',\n",
       " 'ainsi',\n",
       " 'ait',\n",
       " 'allaient',\n",
       " 'allo',\n",
       " 'allons',\n",
       " 'allô',\n",
       " 'alors',\n",
       " 'as',\n",
       " 'assez',\n",
       " 'au',\n",
       " 'aupres',\n",
       " 'auquel',\n",
       " 'aura',\n",
       " 'aurai',\n",
       " 'auraient',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'auras',\n",
       " 'aurez',\n",
       " 'auriez',\n",
       " 'aurions',\n",
       " 'aurons',\n",
       " 'auront',\n",
       " 'aussi',\n",
       " 'autre',\n",
       " 'autrefois',\n",
       " 'autrement',\n",
       " 'autres',\n",
       " 'autrui',\n",
       " 'aux',\n",
       " 'auxquelles',\n",
       " 'auxquels',\n",
       " 'avaient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avez',\n",
       " 'aviez',\n",
       " 'avions',\n",
       " 'avoir',\n",
       " 'avons',\n",
       " 'ayant',\n",
       " 'ayez',\n",
       " 'ayons',\n",
       " 'b',\n",
       " 'bah',\n",
       " 'bas',\n",
       " 'basee',\n",
       " 'bat',\n",
       " 'bien',\n",
       " 'bigre',\n",
       " 'bon',\n",
       " 'boum',\n",
       " 'bravo',\n",
       " 'brrr',\n",
       " 'c',\n",
       " 'car',\n",
       " 'ce',\n",
       " 'ceci',\n",
       " 'cela',\n",
       " 'celle',\n",
       " 'celle-ci',\n",
       " 'celle-là',\n",
       " 'celles',\n",
       " 'celles-ci',\n",
       " 'celles-là',\n",
       " 'celui',\n",
       " 'celui-ci',\n",
       " 'celui-là',\n",
       " 'celà',\n",
       " 'cent',\n",
       " 'cependant',\n",
       " 'certain',\n",
       " 'certaine',\n",
       " 'certaines',\n",
       " 'certains',\n",
       " 'certes',\n",
       " 'ces',\n",
       " 'cet',\n",
       " 'cette',\n",
       " 'ceux',\n",
       " 'ceux-ci',\n",
       " 'ceux-là',\n",
       " 'chacun',\n",
       " 'chacune',\n",
       " 'chaque',\n",
       " 'cher',\n",
       " 'chers',\n",
       " 'chez',\n",
       " 'chiche',\n",
       " 'chut',\n",
       " 'chère',\n",
       " 'chères',\n",
       " 'ci',\n",
       " 'cinq',\n",
       " 'cinquantaine',\n",
       " 'cinquante',\n",
       " 'cinquantième',\n",
       " 'cinquième',\n",
       " 'clac',\n",
       " 'clic',\n",
       " 'comme',\n",
       " 'concernant',\n",
       " 'couic',\n",
       " 'crac',\n",
       " 'd',\n",
       " 'da',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'deja',\n",
       " 'delà',\n",
       " 'des',\n",
       " 'desquelles',\n",
       " 'desquels',\n",
       " 'deux',\n",
       " 'deuxième',\n",
       " 'deuxièmement',\n",
       " 'dire',\n",
       " 'dit',\n",
       " 'dite',\n",
       " 'dits',\n",
       " 'dix',\n",
       " 'dix-huit',\n",
       " 'dix-neuf',\n",
       " 'dix-sept',\n",
       " 'dixième',\n",
       " 'donc',\n",
       " 'dont',\n",
       " 'douze',\n",
       " 'douzième',\n",
       " 'dring',\n",
       " 'du',\n",
       " 'duquel',\n",
       " 'dès',\n",
       " 'e',\n",
       " 'eh',\n",
       " 'elle',\n",
       " 'elle-même',\n",
       " 'elles',\n",
       " 'elles-mêmes',\n",
       " 'en',\n",
       " 'es',\n",
       " 'est',\n",
       " 'et',\n",
       " 'etant',\n",
       " 'etc',\n",
       " 'etre',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'euh',\n",
       " 'eurent',\n",
       " 'eus',\n",
       " 'eusse',\n",
       " 'eussent',\n",
       " 'eusses',\n",
       " 'eussiez',\n",
       " 'eussions',\n",
       " 'eut',\n",
       " 'eux',\n",
       " 'eux-mêmes',\n",
       " 'eûmes',\n",
       " 'eût',\n",
       " 'eûtes',\n",
       " 'f',\n",
       " 'fais',\n",
       " 'faisaient',\n",
       " 'faisant',\n",
       " 'fait',\n",
       " 'faites',\n",
       " 'feront',\n",
       " 'fi',\n",
       " 'font',\n",
       " 'furent',\n",
       " 'fus',\n",
       " 'fusse',\n",
       " 'fussent',\n",
       " 'fusses',\n",
       " 'fussiez',\n",
       " 'fussions',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fût',\n",
       " 'fûtes',\n",
       " 'g',\n",
       " 'h',\n",
       " 'ha',\n",
       " 'hein',\n",
       " 'hem',\n",
       " 'hep',\n",
       " 'hi',\n",
       " 'ho',\n",
       " 'holà',\n",
       " 'hop',\n",
       " 'hou',\n",
       " 'houp',\n",
       " 'hue',\n",
       " 'hui',\n",
       " 'huit',\n",
       " 'huitième',\n",
       " 'hum',\n",
       " 'hurrah',\n",
       " 'hé',\n",
       " 'i',\n",
       " 'ici',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'j',\n",
       " 'je',\n",
       " 'k',\n",
       " 'l',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'leurs',\n",
       " 'lui',\n",
       " 'lui-meme',\n",
       " 'lui-même',\n",
       " 'là',\n",
       " 'lès',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'merci',\n",
       " 'mes',\n",
       " 'mille',\n",
       " 'moi',\n",
       " 'moi-meme',\n",
       " 'moi-même',\n",
       " 'mon',\n",
       " 'mot',\n",
       " 'moyennant',\n",
       " 'n',\n",
       " 'na',\n",
       " 'ne',\n",
       " 'neuf',\n",
       " 'neuvième',\n",
       " 'ni',\n",
       " 'non',\n",
       " 'nos',\n",
       " 'notamment',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'nous-mêmes',\n",
       " 'nôtre',\n",
       " 'nôtres',\n",
       " 'o',\n",
       " 'oh',\n",
       " 'ohé',\n",
       " 'ollé',\n",
       " 'olé',\n",
       " 'on',\n",
       " 'ont',\n",
       " 'onze',\n",
       " 'onzième',\n",
       " 'ouias',\n",
       " 'o|',\n",
       " 'p',\n",
       " 'paf',\n",
       " 'pan',\n",
       " 'par',\n",
       " 'parce',\n",
       " 'parle',\n",
       " 'parmi',\n",
       " 'pas',\n",
       " 'pense',\n",
       " 'peu',\n",
       " 'peut',\n",
       " 'peuvent',\n",
       " 'peux',\n",
       " 'pff',\n",
       " 'pfft',\n",
       " 'pfut',\n",
       " 'pif',\n",
       " 'plouf',\n",
       " 'plutôt',\n",
       " 'pouah',\n",
       " 'pour',\n",
       " 'pourrais',\n",
       " 'pourrait',\n",
       " 'pouvait',\n",
       " 'precisement',\n",
       " 'pres',\n",
       " 'procedant',\n",
       " 'psitt',\n",
       " 'pu',\n",
       " 'puis',\n",
       " 'puisque',\n",
       " 'q',\n",
       " 'qu',\n",
       " 'quand',\n",
       " 'quant',\n",
       " 'quant-à-soi',\n",
       " 'quanta',\n",
       " 'quarante',\n",
       " 'quatorze',\n",
       " 'quatre',\n",
       " 'quatre-vingt',\n",
       " 'quatrième',\n",
       " 'quatrièmement',\n",
       " 'que',\n",
       " \"quelqu'un\",\n",
       " 'qui',\n",
       " 'quinze',\n",
       " 'quoi',\n",
       " 'quoique',\n",
       " 'r',\n",
       " 'relative',\n",
       " 'relativement',\n",
       " 'rend',\n",
       " 'rendre',\n",
       " 'revoici',\n",
       " 'revoilà',\n",
       " 'rien',\n",
       " 's',\n",
       " 'sa',\n",
       " 'sacrebleu',\n",
       " 'sait',\n",
       " 'sans',\n",
       " 'sapristi',\n",
       " 'se',\n",
       " 'seize',\n",
       " 'selon',\n",
       " 'sept',\n",
       " 'septième',\n",
       " 'sera',\n",
       " 'serai',\n",
       " 'seraient',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'seras',\n",
       " 'serez',\n",
       " 'seriez',\n",
       " 'serions',\n",
       " 'serons',\n",
       " 'seront',\n",
       " 'ses',\n",
       " 'si',\n",
       " 'sien',\n",
       " 'sienne',\n",
       " 'siennes',\n",
       " 'siens',\n",
       " 'sinon',\n",
       " 'six',\n",
       " 'sixième',\n",
       " 'soi',\n",
       " 'soi-même',\n",
       " 'soient',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soixante',\n",
       " 'sommes',\n",
       " 'son',\n",
       " 'sont',\n",
       " 'sous',\n",
       " 'soyez',\n",
       " 'soyons',\n",
       " 'suis',\n",
       " 'sur',\n",
       " 'surtout',\n",
       " 't',\n",
       " 'ta',\n",
       " 'tac',\n",
       " 'tandis',\n",
       " 'tant',\n",
       " 'te',\n",
       " 'tel',\n",
       " 'telle',\n",
       " 'tellement',\n",
       " 'telles',\n",
       " 'tels',\n",
       " 'tenant',\n",
       " 'tend',\n",
       " 'tes',\n",
       " 'tic',\n",
       " 'tien',\n",
       " 'tienne',\n",
       " 'tiennes',\n",
       " 'tiens',\n",
       " 'toc',\n",
       " 'toi',\n",
       " 'toi-même',\n",
       " 'ton',\n",
       " 'toujours',\n",
       " 'tous',\n",
       " 'tout',\n",
       " 'toute',\n",
       " 'toutefois',\n",
       " 'toutes',\n",
       " 'treize',\n",
       " 'trente',\n",
       " 'tres',\n",
       " 'trois',\n",
       " 'troisième',\n",
       " 'troisièmement',\n",
       " 'très',\n",
       " 'tsoin',\n",
       " 'tsouin',\n",
       " 'tu',\n",
       " 'té',\n",
       " 'u',\n",
       " 'un',\n",
       " 'une',\n",
       " 'unes',\n",
       " 'uns',\n",
       " 'v',\n",
       " 'va',\n",
       " 'vais',\n",
       " 'vas',\n",
       " 'vers',\n",
       " 'via',\n",
       " 'vingt',\n",
       " 'vlan',\n",
       " 'voici',\n",
       " 'voie',\n",
       " 'voient',\n",
       " 'voilà',\n",
       " 'vont',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'vous-mêmes',\n",
       " 'vu',\n",
       " 'vé',\n",
       " 'vôtre',\n",
       " 'vôtres',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " 'zut',\n",
       " 'à',\n",
       " 'â',\n",
       " 'ça',\n",
       " 'ès',\n",
       " 'étaient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étant',\n",
       " 'état',\n",
       " 'étiez',\n",
       " 'étions',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'êtes',\n",
       " 'être',\n",
       " 'ô']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_stop_words_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf1 = TfidfVectorizer(sublinear_tf=False,\n",
    "                         min_df=3,\n",
    "                        ngram_range=(1, 3), # (1, 2), elliot faisait 1 a 5 grams lui, + Tf-idf sur les characters\n",
    "                        stop_words=list_stop_words_fr,\n",
    "                        strip_accents=\"unicode\",\n",
    "                        lowercase=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yohannlefaou/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ca', 'chere', 'cheres', 'cinquantieme', 'cinquieme', 'dela', 'deuxieme', 'deuxiemement', 'dixieme', 'douzieme', 'etaient', 'etais', 'etait', 'etat', 'ete', 'etee', 'etees', 'etes', 'etiez', 'etions', 'eumes', 'eutes', 'fumes', 'futes', 'he', 'hola', 'huitieme', 'meme', 'memes', 'neuvieme', 'notres', 'ohe', 'ole', 'olle', 'onzieme', 'plutot', 'quatrieme', 'quatriemement', 'quelqu', 'revoila', 'septieme', 'sixieme', 'troisieme', 'troisiemement', 've', 'voila', 'votres'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "features_tfidf1 = tfidf1.fit_transform(train[\"question\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8028, 6136)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_tfidf1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add cosine similarity features\n",
    "# for each questions, we compute the similarities with each class\n",
    "# to do so, for a given class, we do the average of the similarities with all the questions of the class\n",
    "# (using leave one out approach)\n",
    "# this improve the accuracy score of the random forest by 0.1\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6a3faeaa344a439f48cb967ea55d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8028), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_similarities = []\n",
    "for i in tqdm(range(len(labels))):\n",
    "    similarities = cosine_similarity(np.delete(features, obj=i, axis=0), features[i].reshape(1, -1))\n",
    "    df_similarities = pd.DataFrame({\"class\": labels[\"intention\"].drop(index=i, axis=0).values,\n",
    "                                    \"similarity\": similarities[:,0]})\n",
    "    class_similarities = df_similarities.groupby(\"class\")[\"similarity\"].mean()\n",
    "    list_similarities.append(class_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_similarity_features1 = pd.DataFrame(list_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>similarity</td>\n",
       "      <td>0.003805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.003590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006081</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015519</td>\n",
       "      <td>0.004961</td>\n",
       "      <td>0.002613</td>\n",
       "      <td>0.003611</td>\n",
       "      <td>0.002518</td>\n",
       "      <td>0.003203</td>\n",
       "      <td>0.006982</td>\n",
       "      <td>0.011482</td>\n",
       "      <td>0.003508</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>similarity</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>0.013389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>similarity</td>\n",
       "      <td>0.008940</td>\n",
       "      <td>0.003756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004321</td>\n",
       "      <td>0.002538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009363</td>\n",
       "      <td>0.017346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.005149</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.003754</td>\n",
       "      <td>0.006788</td>\n",
       "      <td>0.005306</td>\n",
       "      <td>0.007533</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>0.001282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>similarity</td>\n",
       "      <td>0.008886</td>\n",
       "      <td>0.018919</td>\n",
       "      <td>0.009587</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.021782</td>\n",
       "      <td>0.008984</td>\n",
       "      <td>0.004116</td>\n",
       "      <td>0.003169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012514</td>\n",
       "      <td>0.004318</td>\n",
       "      <td>0.017599</td>\n",
       "      <td>0.019347</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>similarity</td>\n",
       "      <td>0.007183</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015257</td>\n",
       "      <td>0.002971</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.013460</td>\n",
       "      <td>0.020901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007442</td>\n",
       "      <td>0.017103</td>\n",
       "      <td>0.006202</td>\n",
       "      <td>0.005364</td>\n",
       "      <td>0.003176</td>\n",
       "      <td>0.015592</td>\n",
       "      <td>0.005003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "class              0         1         2    3         4         5         6  \\\n",
       "similarity  0.003805  0.000000  0.000000  0.0  0.000628  0.003590  0.000000   \n",
       "similarity  0.000451  0.000000  0.000000  0.0  0.000000  0.000000  0.000000   \n",
       "similarity  0.008940  0.003756  0.000000  0.0  0.004321  0.002538  0.000000   \n",
       "similarity  0.008886  0.018919  0.009587  0.0  0.000000  0.001880  0.021782   \n",
       "similarity  0.007183  0.003935  0.000000  0.0  0.015257  0.002971  0.002663   \n",
       "\n",
       "class              7         8         9  ...        41        42        43  \\\n",
       "similarity  0.006081  0.000939  0.000875  ...  0.015519  0.004961  0.002613   \n",
       "similarity  0.000000  0.000000  0.000000  ...  0.000000  0.000362  0.000000   \n",
       "similarity  0.000000  0.009363  0.017346  ...  0.000742  0.005149  0.002930   \n",
       "similarity  0.008984  0.004116  0.003169  ...  0.000000  0.012514  0.004318   \n",
       "similarity  0.017241  0.013460  0.020901  ...  0.000000  0.007442  0.017103   \n",
       "\n",
       "class             44        45        46        47        48        49  \\\n",
       "similarity  0.003611  0.002518  0.003203  0.006982  0.011482  0.003508   \n",
       "similarity  0.001813  0.013389  0.000000  0.000000  0.002101  0.000000   \n",
       "similarity  0.003754  0.006788  0.005306  0.007533  0.005900  0.001422   \n",
       "similarity  0.017599  0.019347  0.000000  0.000000  0.001650  0.000000   \n",
       "similarity  0.006202  0.005364  0.003176  0.015592  0.005003  0.000000   \n",
       "\n",
       "class             50  \n",
       "similarity  0.000000  \n",
       "similarity  0.000000  \n",
       "similarity  0.001282  \n",
       "similarity  0.009630  \n",
       "similarity  0.002470  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_similarity_features1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3237419033383159"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(labels[\"intention\"].values,\n",
    "               np.argmax(np.array(df_similarity_features1), axis=1)) #voir si ça marche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> label 0:\n",
      "  * Most Correlated Unigrams are: equivalent, alternative, remplacer\n",
      "  * Most Correlated Bigrams are: remplacer contramal, alternative therapeutique, quelle alternative\n",
      "  * Most Correlated Bigrams are: seroplex depuis semaines, methadone remplacer contramal, quelle alternative therapeutique\n",
      "\n",
      "==> label 1:\n",
      "  * Most Correlated Unigrams are: memantine, veratran, remicad\n",
      "  * Most Correlated Bigrams are: mois apres, quels symptomes, quels problemes\n",
      "  * Most Correlated Bigrams are: quels effets secondaires, quels effets indesirables, quelle forme galenique\n",
      "\n",
      "==> label 2:\n",
      "  * Most Correlated Unigrams are: sanguin, biologique, bilan\n",
      "  * Most Correlated Bigrams are: bilan sanguin, avant instauration, quel bilan\n",
      "  * Most Correlated Bigrams are: combien temps faut, combien temps apres, temps faut attendre\n",
      "\n",
      "==> label 3:\n",
      "  * Most Correlated Unigrams are: pourquoi, manque, fabricant\n",
      "  * Most Correlated Bigrams are: rupture stock, quelle etait, manque fabricant\n",
      "  * Most Correlated Bigrams are: quels effets indesirables, bout combien temps, quels effets secondaires\n",
      "\n",
      "==> label 4:\n",
      "  * Most Correlated Unigrams are: somnifere, appartient, classe\n",
      "  * Most Correlated Bigrams are: quel type, classe therapeutique, quelle classe\n",
      "  * Most Correlated Bigrams are: bout combien temps, quels effets secondaires, quelle classe therapeutique\n",
      "\n",
      "==> label 5:\n",
      "  * Most Correlated Unigrams are: difference, contient, composition\n",
      "  * Most Correlated Bigrams are: principe actif, quelle composition, difference entre\n",
      "  * Most Correlated Bigrams are: maladie auto immune, generique diane 35, quel principe actif\n",
      "\n",
      "==> label 6:\n",
      "  * Most Correlated Unigrams are: volume, boite, conditionnements\n",
      "  * Most Correlated Bigrams are: mg ml, rivotril mg, quels conditionnements\n",
      "  * Most Correlated Bigrams are: pharmacienne souhaite savoir, savoir possible prendre, voudrais savoir possible\n",
      "\n",
      "==> label 7:\n",
      "  * Most Correlated Unigrams are: conserver, conservation, frigo\n",
      "  * Most Correlated Bigrams are: duree conservation, temps conserver, vaccin frigo\n",
      "  * Most Correlated Bigrams are: quels effets secondaires, vaccin contre hepatite, combien temps conserver\n",
      "\n",
      "==> label 8:\n",
      "  * Most Correlated Unigrams are: liste, delivrer, ordonnance\n",
      "  * Most Correlated Bigrams are: patient presente, posologie jour, vente libre\n",
      "  * Most Correlated Bigrams are: pharmacien souhaite savoir, vaccin dt polio, faut ordonnance acheter\n",
      "\n",
      "==> label 9:\n",
      "  * Most Correlated Unigrams are: prescrire, conditions, prescription\n",
      "  * Most Correlated Bigrams are: comment prescrire, prescription facile, facile deroxat\n",
      "  * Most Correlated Bigrams are: contre cancer col, cancer col uterus, prescription facile deroxat\n",
      "\n",
      "==> label 10:\n",
      "  * Most Correlated Unigrams are: toujeo, passage, passer\n",
      "  * Most Correlated Bigrams are: passer deroxat, jasmine jasminelle, comment passer\n",
      "  * Most Correlated Bigrams are: apres jours arret, dois faire pause, dernier comprime actif\n",
      "\n",
      "==> label 11:\n",
      "  * Most Correlated Unigrams are: enervee, help, faire\n",
      "  * Most Correlated Bigrams are: faut continuer, faire cas, dois faire\n",
      "  * Most Correlated Bigrams are: medecin traitant prescrit, apres prise pilule, perdre poids pris\n",
      "\n",
      "==> label 12:\n",
      "  * Most Correlated Unigrams are: aubagio, alat, faire\n",
      "  * Most Correlated Bigrams are: patient aubagio, fois normale, faire patient\n",
      "  * Most Correlated Bigrams are: hier soir pris, patient aubagio presente, faire patient aubagio\n",
      "\n",
      "==> label 13:\n",
      "  * Most Correlated Unigrams are: 12h, oublie, oubli\n",
      "  * Most Correlated Bigrams are: oubli cerazette, oubli pilule, oublie prendre\n",
      "  * Most Correlated Bigrams are: prendre comprime oublie, oubli melodia faire, oublie prendre pilule\n",
      "\n",
      "==> label 14:\n",
      "  * Most Correlated Unigrams are: decaler, comment, plaquettes\n",
      "  * Most Correlated Bigrams are: comment prendre, comment arreter, comment utiliser\n",
      "  * Most Correlated Bigrams are: comment decaler regles, faire test grossesse, decaler regles avec\n",
      "\n",
      "==> label 15:\n",
      "  * Most Correlated Unigrams are: decolle, sparadrap, patch\n",
      "  * Most Correlated Bigrams are: patch neupro, patch evra, mettre patch\n",
      "  * Most Correlated Bigrams are: quels effets secondaires, utilise patch evra, combien temps avant\n",
      "\n",
      "==> label 16:\n",
      "  * Most Correlated Unigrams are: conservation, ouvert, ouverture\n",
      "  * Most Correlated Bigrams are: duree conservation, conservation apres, apres ouverture\n",
      "  * Most Correlated Bigrams are: savoir combien temps, combien temps conserver, conservation apres ouverture\n",
      "\n",
      "==> label 17:\n",
      "  * Most Correlated Unigrams are: temperature, refrigerateur, ambiante\n",
      "  * Most Correlated Bigrams are: ou plus, plus risque, temperature ambiante\n",
      "  * Most Correlated Bigrams are: contre fievre jaune, vaccin contre fievre, combien temps conserver\n",
      "\n",
      "==> label 18:\n",
      "  * Most Correlated Unigrams are: commercialise, recent, france\n",
      "  * Most Correlated Bigrams are: connaitre date, nouvelle pilule, depuis ca\n",
      "  * Most Correlated Bigrams are: quels effets secondaires, pharmacien souhaite savoir, vaccin contre grippe\n",
      "\n",
      "==> label 19:\n",
      "  * Most Correlated Unigrams are: date, logiflox, peremption\n",
      "  * Most Correlated Bigrams are: utiliser creme, peremption passee, date peremption\n",
      "  * Most Correlated Bigrams are: bout combien temps, quels effets secondaires, date peremption passee\n",
      "\n",
      "==> label 20:\n",
      "  * Most Correlated Unigrams are: generique, specialite, nom\n",
      "  * Most Correlated Bigrams are: quel nom, quel medicament, generique quel\n",
      "  * Most Correlated Bigrams are: quels effets secondaires, daily ge generique, generique quel medicament\n",
      "\n",
      "==> label 21:\n",
      "  * Most Correlated Unigrams are: arrete, arret, sevrage\n",
      "  * Most Correlated Bigrams are: symptomes sevrage, effets sevrage, arret deroxat\n",
      "  * Most Correlated Bigrams are: arrete pilule trinordiol, arret diane 35, quels symptomes sevrage\n",
      "\n",
      "==> label 22:\n",
      "  * Most Correlated Unigrams are: connaissez, question, connait\n",
      "  * Most Correlated Bigrams are: petite question, question lutenyl, quelqu connait\n",
      "  * Most Correlated Bigrams are: voudrais savoir pilule, millepertuis fleurs bach, vaccin hpv faire\n",
      "\n",
      "==> label 23:\n",
      "  * Most Correlated Unigrams are: temps, combien, efficace\n",
      "  * Most Correlated Bigrams are: delai action, bout combien, combien temps\n",
      "  * Most Correlated Bigrams are: combien temps met, efficace combien temps, bout combien temps\n",
      "\n",
      "==> label 24:\n",
      "  * Most Correlated Unigrams are: rupture, trouver, disponible\n",
      "  * Most Correlated Bigrams are: cialis disponible, disponible pharmacie, ou trouver\n",
      "  * Most Correlated Bigrams are: vaccin tp ou, trouver vaccin tp, ou trouver vaccin\n",
      "\n",
      "==> label 25:\n",
      "  * Most Correlated Unigrams are: pareil, existent, dosages\n",
      "  * Most Correlated Bigrams are: 60 mg, differents dosages, quels dosages\n",
      "  * Most Correlated Bigrams are: quels effets secondaires, pharmacienne recoit prescription, voudrais savoir quel\n",
      "\n",
      "==> label 26:\n",
      "  * Most Correlated Unigrams are: validite, 24h, dure\n",
      "  * Most Correlated Bigrams are: dure effet, combien temps, protegee pendant\n",
      "  * Most Correlated Bigrams are: temps apres arret, combien temps dure, temps dure effet\n",
      "\n",
      "==> label 27:\n",
      "  * Most Correlated Unigrams are: combien, temps, durer\n",
      "  * Most Correlated Bigrams are: durer longtemps, vaccin h1n1, temps durer\n",
      "  * Most Correlated Bigrams are: combien temps dure, temps effets secondaires, combien temps durer\n",
      "\n",
      "==> label 28:\n",
      "  * Most Correlated Unigrams are: normal, effets, secondaires\n",
      "  * Most Correlated Bigrams are: effets indesirables, quels effets, effets secondaires\n",
      "  * Most Correlated Bigrams are: quels effets secondaire, quels effets indesirables, quels effets secondaires\n",
      "\n",
      "==> label 29:\n",
      "  * Most Correlated Unigrams are: trouver, allemagne, existe\n",
      "  * Most Correlated Bigrams are: ca existe, existe viagra, existe vaccin\n",
      "  * Most Correlated Bigrams are: ou dtp italie, existe vaccin homeopathique, existe vaccin contre\n",
      "\n",
      "==> label 30:\n",
      "  * Most Correlated Unigrams are: suppositoire, galenique, forme\n",
      "  * Most Correlated Bigrams are: forme presente, forme galenique, quelle forme\n",
      "  * Most Correlated Bigrams are: souhaite savoir comprimes, quelle forme presente, quelle forme galenique\n",
      "\n",
      "==> label 31:\n",
      "  * Most Correlated Unigrams are: adresse, anxiete, sert\n",
      "  * Most Correlated Bigrams are: ca sert, phobie sociale, quelle indication\n",
      "  * Most Correlated Bigrams are: mildac anxiete genralisee, champix arreter fumer, quels effets therapeutiques\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> label 32:\n",
      "  * Most Correlated Unigrams are: varicelle, conseillez, solution\n",
      "  * Most Correlated Bigrams are: quel medicament, vaccin varicelle, quel traitement\n",
      "  * Most Correlated Bigrams are: crise angoisse solutions, quel vaccin contre, vaccin contre varicelle\n",
      "\n",
      "==> label 33:\n",
      "  * Most Correlated Unigrams are: soleil, boire, alcool\n",
      "  * Most Correlated Bigrams are: consommer alcool, boire alcool, avec alcool\n",
      "  * Most Correlated Bigrams are: desobel ge 20, jours plus tard, compatible avec millepertuis\n",
      "\n",
      "==> label 34:\n",
      "  * Most Correlated Unigrams are: association, interaction, associer\n",
      "  * Most Correlated Bigrams are: interaction entre, skenan doliprane, meme temps\n",
      "  * Most Correlated Bigrams are: melanger solupred ibuprofen, homeopathie huile essentielle, meme temps effexor\n",
      "\n",
      "==> label 35:\n",
      "  * Most Correlated Unigrams are: eau, smecta, administrer\n",
      "  * Most Correlated Bigrams are: subutex lieu, comment prendre, comment administrer\n",
      "  * Most Correlated Bigrams are: pose sterilet cuivre, possible faire injection, avaler subutex lieu\n",
      "\n",
      "==> label 36:\n",
      "  * Most Correlated Unigrams are: lait, melanger, diluer\n",
      "  * Most Correlated Bigrams are: lait ou, comment diluer, melanger sirop\n",
      "  * Most Correlated Bigrams are: quels effets secondaires, comment diluer teinture, diluer teinture mere\n",
      "\n",
      "==> label 37:\n",
      "  * Most Correlated Unigrams are: mecanisme, agit, fonctionne\n",
      "  * Most Correlated Bigrams are: comment agit, mode action, comment fonctionne\n",
      "  * Most Correlated Bigrams are: methadone traverse barriere, comment ca fonctionne, comment ca marche\n",
      "\n",
      "==> label 38:\n",
      "  * Most Correlated Unigrams are: heure, matin, repas\n",
      "  * Most Correlated Bigrams are: quel moment, matin ou, ou soir\n",
      "  * Most Correlated Bigrams are: deroxat matin ou, quel moment journee, matin ou soir\n",
      "\n",
      "==> label 39:\n",
      "  * Most Correlated Unigrams are: faudrait, garder, usage\n",
      "  * Most Correlated Bigrams are: question medicament, commence plaquette, encore regles\n",
      "  * Most Correlated Bigrams are: minidril depuis mois, prend pilule minidril, prends trinordiol depuis\n",
      "\n",
      "==> label 40:\n",
      "  * Most Correlated Unigrams are: composant, composants, origine\n",
      "  * Most Correlated Bigrams are: souhaite connaitre, progesterone naturelle, ou vient\n",
      "  * Most Correlated Bigrams are: quels effets indesirables, bout combien temps, quels effets secondaires\n",
      "\n",
      "==> label 41:\n",
      "  * Most Correlated Unigrams are: vie, p450, elimination\n",
      "  * Most Correlated Bigrams are: question pilule, quelle demi, demi vie\n",
      "  * Most Correlated Bigrams are: transpiration excessive jaborandi, temps dois attendre, quelle demi vie\n",
      "\n",
      "==> label 42:\n",
      "  * Most Correlated Unigrams are: dose, dosage, posologie\n",
      "  * Most Correlated Bigrams are: quel dosage, quelle posologie, quelle dose\n",
      "  * Most Correlated Bigrams are: arnica quelle dose, quelle dose quelle, quelle dose maximale\n",
      "\n",
      "==> label 43:\n",
      "  * Most Correlated Unigrams are: injecter, conduire, precautions\n",
      "  * Most Correlated Bigrams are: patient avec, contre indications, precautions emploi\n",
      "  * Most Correlated Bigrams are: dois faire operer, voudrais savoir bout, voulais savoir risques\n",
      "\n",
      "==> label 44:\n",
      "  * Most Correlated Unigrams are: pendant, enceinte, grossesse\n",
      "  * Most Correlated Bigrams are: enceinte mois, femme enceinte, pendant grossesse\n",
      "  * Most Correlated Bigrams are: essentielles pendant grossesse, possible pendant grossesse, deroxat pendant grossesse\n",
      "\n",
      "==> label 45:\n",
      "  * Most Correlated Unigrams are: version, dci, generique\n",
      "  * Most Correlated Bigrams are: generique doliprane, generique deroxat, quel generique\n",
      "  * Most Correlated Bigrams are: quel principe actif, medecin prescrit deroxat, medicament generique propecia\n",
      "\n",
      "==> label 46:\n",
      "  * Most Correlated Unigrams are: cout, coute, prix\n",
      "  * Most Correlated Bigrams are: jaborandi quel, quel prix, combien coute\n",
      "  * Most Correlated Bigrams are: quels effets secondaires, voudrais savoir combien, transpiration excessive jaborandi\n",
      "\n",
      "==> label 47:\n",
      "  * Most Correlated Unigrams are: remboursee, charge, rembourse\n",
      "  * Most Correlated Bigrams are: exomuc derembourse, securite sociale, pris charge\n",
      "  * Most Correlated Bigrams are: vaccin anti grippe, vaccin contre grippe, remboursable securite sociale\n",
      "\n",
      "==> label 48:\n",
      "  * Most Correlated Unigrams are: risques, dangereux, risque\n",
      "  * Most Correlated Bigrams are: dangereux sante, risque grossesse, tomber enceinte\n",
      "  * Most Correlated Bigrams are: possible tomber enceinte, quel risque tomber, risque tomber enceinte\n",
      "\n",
      "==> label 49:\n",
      "  * Most Correlated Unigrams are: ecraser, previscan, secable\n",
      "  * Most Correlated Bigrams are: jour souhaite, bactrim forte, rivotril mg\n",
      "  * Most Correlated Bigrams are: pharmacien souhaite savoir, pharmacienne recoit prescription, jour souhaite savoir\n",
      "\n",
      "==> label 50:\n",
      "  * Most Correlated Unigrams are: substituable, substitue, substituer\n",
      "  * Most Correlated Bigrams are: oui quel, existe generique, deroxat ca\n",
      "  * Most Correlated Bigrams are: bout combien temps, quels effets secondaires, pharmacienne souhaite savoir\n"
     ]
    }
   ],
   "source": [
    "# Finding the three most correlated terms with each of the product categories\n",
    "N = 3\n",
    "for label in range(51):\n",
    "    features_chi2 = chi2(features_tfidf1, labels[\"intention\"] == label)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf1.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    trigrams = [v for v in feature_names if len(v.split(' ')) == 3]\n",
    "    #quatregrams = [v for v in feature_names if len(v.split(' ')) == 4]\n",
    "    #cinqgrams = [v for v in feature_names if len(v.split(' ')) == 5]\n",
    "\n",
    "    print(\"\\n==> label %s:\" %(label))\n",
    "    print(\"  * Most Correlated Unigrams are: %s\" %(', '.join(unigrams[-N:])))\n",
    "    print(\"  * Most Correlated Bigrams are: %s\" %(', '.join(bigrams[-N:])))\n",
    "    print(\"  * Most Correlated Bigrams are: %s\" %(', '.join(trigrams[-N:])))\n",
    "    #print(\"  * Most Correlated Bigrams are: %s\" %(', '.join(quatregrams[-N:])))\n",
    "    #print(\"  * Most Correlated Bigrams are: %s\" %(', '.join(cinqgrams[-N:])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf2 = TfidfVectorizer(sublinear_tf=False,\n",
    "                         min_df=10,\n",
    "                         analyzer = \"char\",\n",
    "                         ngram_range=(2, 7),\n",
    "                         strip_accents=\"unicode\",\n",
    "                         lowercase=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_tfidf2 = tfidf2.fit_transform(train[\"question\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8028, 49288)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_tfidf2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other features\n",
    "# voir si autres features intéressant !\n",
    "\n",
    "def count_words(s):\n",
    "    return len(s.split())\n",
    "\n",
    "df_engineered_features = pd.DataFrame({\"count_characters\" : train[\"question\"].apply(len).values,\n",
    "                                       \"count_words\" : train[\"question\"].apply(count_words).values})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(np.concatenate([features_tfidf1\n",
    "                                                                      #, np.array(df_similarity_features)\n",
    "                                                                      #, np.array(df_engineered_features)\n",
    "                                                                     ], axis=1),\n",
    "                                                      labels[\"intention\"].values, test_size=0.2,\n",
    "                                                      random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random forest\n",
    "\n",
    "rf = RandomForestClassifier(max_depth = 30, n_estimators=100) # , max_features=None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.7 s, sys: 172 ms, total: 14.9 s\n",
      "Wall time: 15.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43586550435865506"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_valid, rf.predict(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var</th>\n",
       "      <th>imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2516</td>\n",
       "      <td>tf_idf_sim_28</td>\n",
       "      <td>0.035189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2532</td>\n",
       "      <td>tf_idf_sim_44</td>\n",
       "      <td>0.029256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2536</td>\n",
       "      <td>tf_idf_sim_48</td>\n",
       "      <td>0.026044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2519</td>\n",
       "      <td>tf_idf_sim_31</td>\n",
       "      <td>0.021188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2499</td>\n",
       "      <td>tf_idf_sim_11</td>\n",
       "      <td>0.020096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2509</td>\n",
       "      <td>tf_idf_sim_21</td>\n",
       "      <td>0.019489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2530</td>\n",
       "      <td>tf_idf_sim_42</td>\n",
       "      <td>0.019197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>tf_idf_sim_32</td>\n",
       "      <td>0.019158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2510</td>\n",
       "      <td>tf_idf_sim_22</td>\n",
       "      <td>0.018501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2522</td>\n",
       "      <td>tf_idf_sim_34</td>\n",
       "      <td>0.018021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2515</td>\n",
       "      <td>tf_idf_sim_27</td>\n",
       "      <td>0.017718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2488</td>\n",
       "      <td>tf_idf_sim_0</td>\n",
       "      <td>0.016611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2521</td>\n",
       "      <td>tf_idf_sim_33</td>\n",
       "      <td>0.016342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2511</td>\n",
       "      <td>tf_idf_sim_23</td>\n",
       "      <td>0.016210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2502</td>\n",
       "      <td>tf_idf_sim_14</td>\n",
       "      <td>0.016124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2514</td>\n",
       "      <td>tf_idf_sim_26</td>\n",
       "      <td>0.015486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>tf_idf_sim_12</td>\n",
       "      <td>0.015312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2496</td>\n",
       "      <td>tf_idf_sim_8</td>\n",
       "      <td>0.014715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2535</td>\n",
       "      <td>tf_idf_sim_47</td>\n",
       "      <td>0.014111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2501</td>\n",
       "      <td>tf_idf_sim_13</td>\n",
       "      <td>0.014108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2525</td>\n",
       "      <td>tf_idf_sim_37</td>\n",
       "      <td>0.014089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2531</td>\n",
       "      <td>tf_idf_sim_43</td>\n",
       "      <td>0.013769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2498</td>\n",
       "      <td>tf_idf_sim_10</td>\n",
       "      <td>0.013743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2527</td>\n",
       "      <td>tf_idf_sim_39</td>\n",
       "      <td>0.013521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2526</td>\n",
       "      <td>tf_idf_sim_38</td>\n",
       "      <td>0.013446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2493</td>\n",
       "      <td>tf_idf_sim_5</td>\n",
       "      <td>0.012495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2523</td>\n",
       "      <td>tf_idf_sim_35</td>\n",
       "      <td>0.011868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2529</td>\n",
       "      <td>tf_idf_sim_41</td>\n",
       "      <td>0.010596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2497</td>\n",
       "      <td>tf_idf_sim_9</td>\n",
       "      <td>0.010416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2512</td>\n",
       "      <td>tf_idf_sim_24</td>\n",
       "      <td>0.010048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2517</td>\n",
       "      <td>tf_idf_sim_29</td>\n",
       "      <td>0.009952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2492</td>\n",
       "      <td>tf_idf_sim_4</td>\n",
       "      <td>0.009952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1034</td>\n",
       "      <td>tf_idf_feat_1034</td>\n",
       "      <td>0.009870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2507</td>\n",
       "      <td>tf_idf_sim_19</td>\n",
       "      <td>0.009643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2490</td>\n",
       "      <td>tf_idf_sim_2</td>\n",
       "      <td>0.009197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2513</td>\n",
       "      <td>tf_idf_sim_25</td>\n",
       "      <td>0.009185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2534</td>\n",
       "      <td>tf_idf_sim_46</td>\n",
       "      <td>0.008981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2495</td>\n",
       "      <td>tf_idf_sim_7</td>\n",
       "      <td>0.008764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2518</td>\n",
       "      <td>tf_idf_sim_30</td>\n",
       "      <td>0.008532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2504</td>\n",
       "      <td>tf_idf_sim_16</td>\n",
       "      <td>0.008277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2505</td>\n",
       "      <td>tf_idf_sim_17</td>\n",
       "      <td>0.008138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2503</td>\n",
       "      <td>tf_idf_sim_15</td>\n",
       "      <td>0.008119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2524</td>\n",
       "      <td>tf_idf_sim_36</td>\n",
       "      <td>0.008082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2533</td>\n",
       "      <td>tf_idf_sim_45</td>\n",
       "      <td>0.008070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2537</td>\n",
       "      <td>tf_idf_sim_49</td>\n",
       "      <td>0.007699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2538</td>\n",
       "      <td>tf_idf_sim_50</td>\n",
       "      <td>0.007547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2494</td>\n",
       "      <td>tf_idf_sim_6</td>\n",
       "      <td>0.007223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2506</td>\n",
       "      <td>tf_idf_sim_18</td>\n",
       "      <td>0.007064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2489</td>\n",
       "      <td>tf_idf_sim_1</td>\n",
       "      <td>0.006577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2508</td>\n",
       "      <td>tf_idf_sim_20</td>\n",
       "      <td>0.004331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   var       imp\n",
       "2516  tf_idf_sim_28     0.035189\n",
       "2532  tf_idf_sim_44     0.029256\n",
       "2536  tf_idf_sim_48     0.026044\n",
       "2519  tf_idf_sim_31     0.021188\n",
       "2499  tf_idf_sim_11     0.020096\n",
       "2509  tf_idf_sim_21     0.019489\n",
       "2530  tf_idf_sim_42     0.019197\n",
       "2520  tf_idf_sim_32     0.019158\n",
       "2510  tf_idf_sim_22     0.018501\n",
       "2522  tf_idf_sim_34     0.018021\n",
       "2515  tf_idf_sim_27     0.017718\n",
       "2488  tf_idf_sim_0      0.016611\n",
       "2521  tf_idf_sim_33     0.016342\n",
       "2511  tf_idf_sim_23     0.016210\n",
       "2502  tf_idf_sim_14     0.016124\n",
       "2514  tf_idf_sim_26     0.015486\n",
       "2500  tf_idf_sim_12     0.015312\n",
       "2496  tf_idf_sim_8      0.014715\n",
       "2535  tf_idf_sim_47     0.014111\n",
       "2501  tf_idf_sim_13     0.014108\n",
       "2525  tf_idf_sim_37     0.014089\n",
       "2531  tf_idf_sim_43     0.013769\n",
       "2498  tf_idf_sim_10     0.013743\n",
       "2527  tf_idf_sim_39     0.013521\n",
       "2526  tf_idf_sim_38     0.013446\n",
       "2493  tf_idf_sim_5      0.012495\n",
       "2523  tf_idf_sim_35     0.011868\n",
       "2529  tf_idf_sim_41     0.010596\n",
       "2497  tf_idf_sim_9      0.010416\n",
       "2512  tf_idf_sim_24     0.010048\n",
       "2517  tf_idf_sim_29     0.009952\n",
       "2492  tf_idf_sim_4      0.009952\n",
       "1034  tf_idf_feat_1034  0.009870\n",
       "2507  tf_idf_sim_19     0.009643\n",
       "2490  tf_idf_sim_2      0.009197\n",
       "2513  tf_idf_sim_25     0.009185\n",
       "2534  tf_idf_sim_46     0.008981\n",
       "2495  tf_idf_sim_7      0.008764\n",
       "2518  tf_idf_sim_30     0.008532\n",
       "2504  tf_idf_sim_16     0.008277\n",
       "2505  tf_idf_sim_17     0.008138\n",
       "2503  tf_idf_sim_15     0.008119\n",
       "2524  tf_idf_sim_36     0.008082\n",
       "2533  tf_idf_sim_45     0.008070\n",
       "2537  tf_idf_sim_49     0.007699\n",
       "2538  tf_idf_sim_50     0.007547\n",
       "2494  tf_idf_sim_6      0.007223\n",
       "2506  tf_idf_sim_18     0.007064\n",
       "2489  tf_idf_sim_1      0.006577\n",
       "2508  tf_idf_sim_20     0.004331"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"var\": [\"tf_idf_feat_\" + str(i) for i in range(features.shape[1])] \\\n",
    "              + [\"tf_idf_sim_\" + str(i) for i in range(df_similarity_features.shape[1])]#+ \\\n",
    "              #list(df_engineered_features.columns)\n",
    "              , \"imp\":rf.feature_importances_}).sort_values(\"imp\", ascending=False).head(50)\n",
    "                      \n",
    "                      \n",
    "                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yohannlefaou/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "# one hot encode the target\n",
    "ohe = OneHotEncoder()\n",
    "one_hot_encode_labels = ohe.fit_transform(labels[\"intention\"].values.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(np.concatenate([#features_tfidf1,\n",
    "                                                                      #, np.array(df_similarity_features)\n",
    "                                                                      #, np.array(df_engineered_features)\n",
    "                                                                      features_tfidf2\n",
    "], axis=1),\n",
    "                                                      one_hot_encode_labels,\n",
    "                                                      test_size=0.2,\n",
    "                                                      random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Sequential()\n",
    "nn.add(Dropout(0.5, input_shape=(X_train.shape[1],)))\n",
    "#nn.add(Dense(1024, activation=\"relu\")) # input_dim= 768\n",
    "#nn.add(Dropout(0.2))\n",
    "nn.add(Dense(256, activation=\"relu\"))\n",
    "#nn.add(Dropout(0.2))\n",
    "nn.add(Dense(51, activation=\"softmax\")) #a completer\n",
    "\n",
    "nn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6422 samples, validate on 1606 samples\n",
      "Epoch 1/30\n",
      "6422/6422 [==============================] - 237s 37ms/step - loss: 2.1920 - accuracy: 0.4785 - val_loss: 1.4464 - val_accuracy: 0.6314\n",
      "Epoch 2/30\n",
      "6422/6422 [==============================] - 204s 32ms/step - loss: 0.7936 - accuracy: 0.8026 - val_loss: 1.1680 - val_accuracy: 0.6930\n",
      "Epoch 3/30\n",
      "6422/6422 [==============================] - 191s 30ms/step - loss: 0.2961 - accuracy: 0.9340 - val_loss: 1.1537 - val_accuracy: 0.7042\n",
      "Epoch 4/30\n",
      "6422/6422 [==============================] - 186s 29ms/step - loss: 0.1332 - accuracy: 0.9749 - val_loss: 1.2081 - val_accuracy: 0.6949\n",
      "Epoch 5/30\n",
      " 100/6422 [..............................] - ETA: 3:02 - loss: 0.0321 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-4748d6d128fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m nn.fit(np.array(X_train), y_train, epochs=30, batch_size=10,\n\u001b[0;32m----> 4\u001b[0;31m        validation_data=(np.array(X_valid), y_valid), shuffle=True) #, callbacks=[es]\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=5)\n",
    "\n",
    "nn.fit(np.array(X_train), y_train, epochs=30, batch_size=10,\n",
    "       validation_data=(np.array(X_valid), y_valid), shuffle=True) #, callbacks=[es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6422 samples, validate on 1606 samples\n",
      "Epoch 1/5\n",
      "6422/6422 [==============================] - 2s 280us/step - loss: 0.9382 - accuracy: 0.7191 - val_loss: 1.4268 - val_accuracy: 0.6308\n",
      "Epoch 2/5\n",
      "6422/6422 [==============================] - 2s 262us/step - loss: 0.9139 - accuracy: 0.7239 - val_loss: 1.4210 - val_accuracy: 0.6314\n",
      "Epoch 3/5\n",
      "6422/6422 [==============================] - 2s 273us/step - loss: 0.9126 - accuracy: 0.7222 - val_loss: 1.4216 - val_accuracy: 0.6308\n",
      "Epoch 4/5\n",
      "6422/6422 [==============================] - 2s 274us/step - loss: 0.8981 - accuracy: 0.7278 - val_loss: 1.4206 - val_accuracy: 0.6308\n",
      "Epoch 5/5\n",
      "6422/6422 [==============================] - 2s 278us/step - loss: 0.9029 - accuracy: 0.7317 - val_loss: 1.4202 - val_accuracy: 0.6308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a6f7e9290>"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(np.array(X_train), y_train, epochs=5, batch_size=256,\n",
    "       validation_data=(np.array(X_valid), y_valid), shuffle=True) #, callbacks=[es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(np.argmax(nn.predict(np.array(X_valid)), axis=1), np.argmax(y_valid, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_predictions = np.sum(cm, axis=1)\n",
    "count_predictions[count_predictions == 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a64fd3b10>"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we represent the precisions of the models\n",
    "import seaborn as sns\n",
    "fig=plt.figure(figsize=(18, 15))\n",
    "sns.heatmap(cm / count_predictions[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.638298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.521739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0.431818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0.395349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>0.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>0.711111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>0.653465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>0.646154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>0.435897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>0.491228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>0.762887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>0.548387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    class  precision\n",
       "0   0      0.638298 \n",
       "1   1      0.000000 \n",
       "2   2      1.000000 \n",
       "3   3      0.000000 \n",
       "4   4      0.466667 \n",
       "5   5      0.521739 \n",
       "6   6      0.666667 \n",
       "7   7      0.000000 \n",
       "8   8      0.615385 \n",
       "9   9      0.666667 \n",
       "10  10     0.384615 \n",
       "11  11     0.431818 \n",
       "12  12     0.400000 \n",
       "13  13     0.583333 \n",
       "14  14     0.395349 \n",
       "15  15     0.500000 \n",
       "16  16     0.500000 \n",
       "17  17     0.000000 \n",
       "18  18     0.400000 \n",
       "19  19     1.000000 \n",
       "20  20     0.666667 \n",
       "21  21     0.526316 \n",
       "22  22     0.737500 \n",
       "23  23     0.650000 \n",
       "24  24     0.500000 \n",
       "25  25     1.000000 \n",
       "26  26     0.272727 \n",
       "27  27     0.600000 \n",
       "28  28     0.711111 \n",
       "29  29     0.444444 \n",
       "30  30     0.500000 \n",
       "31  31     0.653465 \n",
       "32  32     0.646154 \n",
       "33  33     0.435897 \n",
       "34  34     0.500000 \n",
       "35  35     0.333333 \n",
       "36  36     0.800000 \n",
       "37  37     0.461538 \n",
       "38  38     0.705882 \n",
       "39  39     0.000000 \n",
       "40  40     1.000000 \n",
       "41  41     1.000000 \n",
       "42  42     0.491228 \n",
       "43  43     0.375000 \n",
       "44  44     0.762887 \n",
       "45  45     0.600000 \n",
       "46  46     0.666667 \n",
       "47  47     0.882353 \n",
       "48  48     0.548387 \n",
       "49  49     0.333333 \n",
       "50  50     0.666667 "
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"class\": list(range(51)), \"precision\":np.diag(cm / count_predictions[:, None])} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
