{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset # used load data\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers.data.processors.utils import InputExample, InputFeatures\n",
    "from transformers import (AdamW,\n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          get_cosine_with_hard_restarts_schedule_with_warmup)\n",
    "\n",
    "from transformers import (CamembertConfig,\n",
    "                          CamembertForSequenceClassification,\n",
    "                          CamembertTokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data=\"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from glue.py and utils.py\n",
    "\n",
    "class InputExample(object):\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
    "\n",
    "    \n",
    "class DataProcessor():\n",
    "    \n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                line = list(cell for cell in line)\n",
    "                lines.append(line)\n",
    "            return lines\n",
    "\n",
    "        \n",
    "        \n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(tensor_dict['ID'].numpy(),\n",
    "                            tensor_dict['question'].numpy().decode('utf-8'),\n",
    "                            str(tensor_dict['intention'].numpy()))\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "    \n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [str(j) for j in range(51)]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                print(line)\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, line[0])\n",
    "            if len(line) == 3:\n",
    "                utterance = line[1]\n",
    "                label = line[2]\n",
    "                examples.append(InputExample(guid=guid, text_a=utterance, text_b=None, label=label))\n",
    "            else:\n",
    "                utterance = line[1]\n",
    "                examples.append(InputExample(guid=guid, text_a=utterance, text_b=None, label=\"0\"))\n",
    "        return examples\n",
    "\n",
    "\n",
    "def convert_examples_to_features(examples,\n",
    "                                 tokenizer,\n",
    "                                 max_length=512,\n",
    "                                 label_list=None,\n",
    "                                      pad_on_left=False,\n",
    "                                      pad_token=0,\n",
    "                                      pad_token_segment_id=0,\n",
    "                                      mask_padding_with_zero=True):\n",
    "    \"\"\"\n",
    "    Loads a data file into a list of ``InputFeatures``\n",
    "\n",
    "    Args:\n",
    "        examples: List of ``InputExamples`` or ``tf.data.Dataset`` containing the examples.\n",
    "        tokenizer: Instance of a tokenizer that will tokenize the examples\n",
    "        max_length: Maximum example length\n",
    "        label_list: List of labels. Can be obtained from the processor using the ``processor.get_labels()`` method\n",
    "        pad_on_left: If set to ``True``, the examples will be padded on the left rather than on the right (default)\n",
    "        pad_token: Padding token\n",
    "        pad_token_segment_id: The segment ID for the padding token (It is usually 0, but can vary such as for XLNet where it is 4)\n",
    "        mask_padding_with_zero: If set to ``True``, the attention mask will be filled by ``1`` for actual values\n",
    "            and by ``0`` for padded values. If set to ``False``, inverts it (``1`` for padded values, ``0`` for\n",
    "            actual values)\n",
    "\n",
    "    Returns:\n",
    "        If the ``examples`` input is a ``tf.data.Dataset``, will return a ``tf.data.Dataset``\n",
    "        containing the task-specific features. If the input is a list of ``InputExamples``, will return\n",
    "        a list of task-specific ``InputFeatures`` which can be fed to the model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    processor = DataProcessor()\n",
    "    if label_list is None:\n",
    "        label_list = processor.get_labels()\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "    print(label_map)\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 10000 == 0:\n",
    "            print(\"Writing example %d\" % (ex_index))\n",
    "\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            example.text_a,\n",
    "            example.text_b,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = max_length - len(input_ids)\n",
    "        if pad_on_left:\n",
    "            input_ids = ([pad_token] * padding_length) + input_ids\n",
    "            attention_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + attention_mask\n",
    "            token_type_ids = ([pad_token_segment_id] * padding_length) + token_type_ids\n",
    "        else:\n",
    "            input_ids = input_ids + ([pad_token] * padding_length)\n",
    "            attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "            token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
    "\n",
    "        assert len(input_ids) == max_length, \"Error with input length {} vs {}\".format(len(input_ids), max_length)\n",
    "        assert len(attention_mask) == max_length, \"Error with input length {} vs {}\".format(len(attention_mask), max_length)\n",
    "        assert len(token_type_ids) == max_length, \"Error with input length {} vs {}\".format(len(token_type_ids), max_length)\n",
    "\n",
    "        label = label_map[example.label]\n",
    "            \n",
    "        #if ex_index < 5:\n",
    "            #print(\"*** Example ***\")\n",
    "            #print(\"guid: %s\" % (example.guid))\n",
    "            #print(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            #print(\"attention_mask: %s\" % \" \".join([str(x) for x in attention_mask]))\n",
    "            #print(\"token_type_ids: %s\" % \" \".join([str(x) for x in token_type_ids]))\n",
    "            #print(\"label: %s (id = %d)\" % (example.label, label))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              attention_mask=attention_mask,\n",
    "                              token_type_ids=token_type_ids,\n",
    "                              label=label))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(tokenizer, \n",
    "                            max_seq_length,\n",
    "                            data_dir,\n",
    "                            which_data=\"train\",\n",
    "                            overwrite_cache = False,\n",
    "                            verbose=1):\n",
    "\n",
    "    processor = DataProcessor()\n",
    "    # Load data features from cache or dataset file\n",
    "    cached_features_file = os.path.join(data_dir, 'cached_{}_{}'.format(which_data,\n",
    "                                                                        str(max_seq_length)))\n",
    "    if os.path.exists(cached_features_file) and not overwrite_cache:\n",
    "        if verbose > 0:\n",
    "            print(\"Loading features from cached file %s\" % cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "    else:\n",
    "        if verbose > 0:\n",
    "            print(\"Creating features from dataset file at %s\" % data_dir)\n",
    "        label_list = processor.get_labels()\n",
    "        \n",
    "        if which_data == \"train\":\n",
    "            examples = processor.get_train_examples(data_dir)\n",
    "        elif which_data == \"test\":\n",
    "            examples = processor.get_test_examples(data_dir)\n",
    "        elif which_data == \"dev\":\n",
    "            examples = processor.get_dev_examples(data_dir)\n",
    "        \n",
    "        features = convert_examples_to_features(examples,\n",
    "                                                tokenizer,\n",
    "                                                label_list=label_list,\n",
    "                                                max_length=max_seq_length,\n",
    "                                                pad_on_left=False,\n",
    "                                                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "                                                pad_token_segment_id=0,\n",
    "        )\n",
    "        if verbose > 0:\n",
    "            print(\"Saving features into cached file %s\" % cached_features_file)\n",
    "        torch.save(features, cached_features_file)\n",
    "\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
    "    all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def train(model, train_dataset, tokenizer, num_train_epochs, train_batch_size, learning_rate, adam_epsilon=1e-8,\n",
    "          logging_steps=None, gradient_accumulation_steps=1, max_grad_norm=1.0, weight_decay=0.0,\n",
    "          warmup_steps=0, save_steps=-1, output_dir=None, evaluate_during_training=False,\n",
    "          seed=None, max_steps=-1, num_cycles=1.0):\n",
    "    \n",
    "    \"\"\" Train the model \"\"\"\n",
    "    if output_dir is None:\n",
    "        output_dir = \"model_\" + str(datetime.datetime.now()).split(\".\")[0].replace(\" \",\"_\") + \"/\"\n",
    "        \n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size)\n",
    "    \n",
    "    if logging_steps is None:\n",
    "        logging_steps = len(train_dataloader) // (gradient_accumulation_steps * 5)\n",
    "        \n",
    "    if max_steps > 0:\n",
    "        t_total = max_steps\n",
    "        num_train_epochs = max_steps // (len(train_dataloader) // gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    \n",
    "    # pas besoin de la partie custom ci-dessous à priori\n",
    "    \n",
    "    #no_decay = ['bias', 'LayerNorm.weight']\n",
    "    #optimizer_grouped_parameters = [\n",
    "    #    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    #    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    #    ]\n",
    "    \n",
    "    # change l'optimizer pour voir\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon) # optimizer_grouped_parameters\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9) #eps=adam_epsilon , momentum=0.9\n",
    "    #scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
    "    scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer,\n",
    "                                                                   num_warmup_steps=warmup_steps,\n",
    "                                                                   num_training_steps=t_total,\n",
    "                                                                   num_cycles=num_cycles)\n",
    "    #for i, tensor in enumerate(model.parameters()):\n",
    "    #    if i > 1:\n",
    "    #        tensor.requires_grad = False\n",
    "\n",
    "    # Train!\n",
    "    print(\"***** Running training *****\")\n",
    "    print(\"  Num examples = %d\" % len(train_dataset))\n",
    "    print(\"  Num Epochs = %d\" % num_train_epochs)\n",
    "    print(\" Batch size = %d\" % train_batch_size)\n",
    "    print(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\" %\n",
    "                   train_batch_size * gradient_accumulation_steps)\n",
    "    print(\"  Gradient Accumulation steps = %d\" % gradient_accumulation_steps)\n",
    "    print(\"  Total optimization steps = %d\" % t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(num_train_epochs), desc=\"Epoch\")\n",
    "    if seed is not None:\n",
    "        set_seed(seed)\n",
    "        \n",
    "    for epoch, _ in enumerate(train_iterator):\n",
    "        # print(\"Epoch %d / %d\" % (epoch, num_train_epochs))\n",
    "        epoch_iterator = train_dataloader\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'labels':         batch[3]}\n",
    "            inputs['token_type_ids'] = batch[2] #or None\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            #print(outputs)\n",
    "            #print(outputs[0].size())\n",
    "            #print(outputs[1].size())\n",
    "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "            #print(loss)\n",
    "            \n",
    "            if gradient_accumulation_steps > 1:\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            print(\"lr:\",scheduler.get_lr()[0], \"loss:\", loss.item())\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                #params = list(model.parameters())[:1]\n",
    "                #print(params)\n",
    "                #print([torch.sum(par, 0) for par in params])\n",
    "                #print([par.size() for par in params])\n",
    "                #print([par.grad for par in params])\n",
    "                #print([torch.sum(par.grad, 0) for par in params])\n",
    "                #print([torch.sum(par.grad, 1) for par in params])\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "                \n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if logging_steps > 0 and global_step % logging_steps == 0:\n",
    "                    print(\"\\n\\nEval\")\n",
    "                    # Log metrics\n",
    "                    dict_print = {'step':global_step,\n",
    "                                  'lr': scheduler.get_lr()[0],\n",
    "                                  'tr_loss': (tr_loss - logging_loss)/logging_steps}\n",
    "                    if evaluate_during_training:\n",
    "                        results = evaluate(model, tokenizer, eval_output_dir=output_dir, verbose=-1)\n",
    "                        for key, value in results.items():\n",
    "                            dict_print['eval_{}'.format(key)] = value\n",
    "                    print(dict_print)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if save_steps > 0 and global_step % save_steps == 0:\n",
    "                    # Save model checkpoint\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    save_model_dir = os.path.join(output_dir, 'checkpoint-{}'.format(global_step))\n",
    "                    os.makedirs(save_model_dir)\n",
    "                    model.save_pretrained(save_model_dir)\n",
    "                    #torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
    "                    print(\"Saving model checkpoint to %s\" % save_model_dir)\n",
    "\n",
    "            if max_steps > 0 and global_step > max_steps:\n",
    "                #epoch_iterator.close() #deleted since no tqdm anymore\n",
    "                break\n",
    "                \n",
    "        if max_steps > 0 and global_step > max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    if global_step == 0:\n",
    "        global_step= 1\n",
    "\n",
    "    return global_step, tr_loss / global_step\n",
    "\n",
    "\n",
    "def evaluate(model, tokenizer, eval_batch_size=8, prefix=\"\", eval_output_dir=None,\n",
    "             verbose=1):\n",
    "    \n",
    "    #results = {}\n",
    "    eval_dataset = load_and_cache_examples(tokenizer=tokenizer, max_seq_length=128, data_dir=\"data/\",\n",
    "                                           which_data=\"test\", verbose=verbose)\n",
    "    \n",
    "    eval_batch_size = eval_batch_size\n",
    "    # Note that DistributedSampler samples randomly\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "    \n",
    "    # Eval!\n",
    "    if verbose > 0:\n",
    "        print(\"***** Running evaluation {} *****\".format(prefix))\n",
    "        print(\"  Num examples = %d\", len(eval_dataset))\n",
    "        print(\"  Batch size = %d\", eval_batch_size)\n",
    "        \n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'labels':         batch[3]}\n",
    "            inputs['token_type_ids'] = batch[2] #or None\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    preds_class = np.argmax(preds, axis=1)\n",
    "    acc = accuracy_score(out_label_ids, preds_class)\n",
    "    \n",
    "    result = {\"val_loss\": eval_loss, \"val_acc\" : acc}\n",
    "    #results.update(result)\n",
    "\n",
    "    if eval_output_dir is not None:\n",
    "        if not os.path.exists(eval_output_dir):\n",
    "            os.makedirs(eval_output_dir)\n",
    "        \n",
    "        output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"a\") as writer:\n",
    "            writer.write(\"***** Eval results {} *****\".format(prefix))\n",
    "            for key in sorted(result.keys()):\n",
    "                writer.write(\"  %s = %s\" % (key, str(result[key])))\n",
    "            writer.write(\"\\n\")\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels=51 # 1081\n",
    "model_name=\"camembert-base\"\n",
    "\n",
    "config = CamembertConfig.from_pretrained(model_name,\n",
    "                                         num_labels=num_labels,\n",
    "                                         finetuning_task=\"classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained(model_name,\n",
    "                                               do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CamembertForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features from cached file data/cached_train_128\n"
     ]
    }
   ],
   "source": [
    "# rmk : we can see all sequences of tokens begin with 5 and end with 6\n",
    "train_dataset = load_and_cache_examples(tokenizer=tokenizer, max_seq_length=128, data_dir=\"data/\", which_data=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [torch.sum(train_dataset[i][1]) for i in range(len(train_dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(l, bins=100)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features from cached file data/cached_dev_128\n"
     ]
    }
   ],
   "source": [
    "dev_dataset = load_and_cache_examples(tokenizer=tokenizer, max_seq_length=128, data_dir=\"data/\", which_data=\"dev\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "l = [torch.sum(train_dataset[i][1]) for i in range(len(dev_dataset))]\n",
    "plt.hist(l, bins=100)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2035"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 6422\n",
      "  Num Epochs = 10\n",
      " Batch size = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2010\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f4cd95a7be45609536f6765c562075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 3e-05 loss: 3.8994977474212646\n",
      "lr: 2.999954195653399e-05 loss: 3.8898026943206787\n",
      "lr: 2.9998167854109796e-05 loss: 3.8786516189575195\n",
      "lr: 2.999587777664724e-05 loss: 3.874788761138916\n",
      "lr: 2.9992671864006992e-05 loss: 3.8684329986572266\n",
      "lr: 2.9988550311982027e-05 loss: 3.7951114177703857\n",
      "lr: 2.9983513372285672e-05 loss: 3.8379433155059814\n",
      "lr: 2.9977561352536243e-05 loss: 3.810689687728882\n",
      "lr: 2.997069461623824e-05 loss: 3.7759265899658203\n",
      "lr: 2.9962913582760155e-05 loss: 3.7477684020996094\n",
      "lr: 2.9954218727308853e-05 loss: 3.7648766040802\n",
      "lr: 2.9944610580900574e-05 loss: 3.854386806488037\n",
      "lr: 2.9934089730328472e-05 loss: 3.7600817680358887\n",
      "lr: 2.9922656818126796e-05 loss: 3.619133710861206\n",
      "lr: 2.991031254253164e-05 loss: 3.625623941421509\n",
      "lr: 2.9897057657438315e-05 loss: 3.742122173309326\n",
      "lr: 2.988289297235528e-05 loss: 3.693572521209717\n",
      "lr: 2.9867819352354736e-05 loss: 3.7593836784362793\n",
      "lr: 2.9851837718019762e-05 loss: 3.545797109603882\n",
      "lr: 2.983494904538813e-05 loss: 3.5979347229003906\n",
      "lr: 2.9817154365892643e-05 loss: 3.677074909210205\n",
      "lr: 2.97984547662982e-05 loss: 3.702458143234253\n",
      "lr: 2.9778851388635386e-05 loss: 3.7153708934783936\n",
      "lr: 2.975834543013074e-05 loss: 3.636321783065796\n",
      "lr: 2.973693814313364e-05 loss: 3.679741382598877\n",
      "lr: 2.971463083503981e-05 loss: 3.4948911666870117\n",
      "lr: 2.9691424868211482e-05 loss: 3.758843421936035\n",
      "lr: 2.9667321659894178e-05 loss: 3.5066211223602295\n",
      "lr: 2.964232268213018e-05 loss: 3.648733139038086\n",
      "lr: 2.961642946166861e-05 loss: 3.6608827114105225\n",
      "lr: 2.9589643579872193e-05 loss: 3.6120245456695557\n",
      "lr: 2.956196667262069e-05 loss: 3.532653331756592\n",
      "lr: 2.9533400430210955e-05 loss: 3.469273090362549\n",
      "lr: 2.950394659725376e-05 loss: 3.458500623703003\n",
      "lr: 2.9473606972567196e-05 loss: 3.3370933532714844\n",
      "lr: 2.9442383409066845e-05 loss: 3.4562437534332275\n",
      "lr: 2.94102778136526e-05 loss: 3.6579489707946777\n",
      "lr: 2.9377292147092234e-05 loss: 3.6143574714660645\n",
      "lr: 2.9343428423901614e-05 loss: 3.5986616611480713\n",
      "lr: 2.9308688712221687e-05 loss: 3.581294298171997\n",
      "lr: 2.927307513369218e-05 loss: 3.714938163757324\n",
      "lr: 2.9236589863322025e-05 loss: 3.6533803939819336\n",
      "lr: 2.9199235129356513e-05 loss: 3.4842886924743652\n",
      "lr: 2.9161013213141218e-05 loss: 3.399075984954834\n",
      "lr: 2.912192644898267e-05 loss: 3.4921250343322754\n",
      "lr: 2.90819772240058e-05 loss: 3.5040764808654785\n",
      "lr: 2.9041167978008123e-05 loss: 3.2808127403259277\n",
      "lr: 2.8999501203310786e-05 loss: 3.5759506225585938\n",
      "lr: 2.8956979444606303e-05 loss: 3.4860172271728516\n",
      "lr: 2.891360529880317e-05 loss: 3.3010289669036865\n",
      "lr: 2.886938141486726e-05 loss: 3.485865831375122\n",
      "lr: 2.8824310493660063e-05 loss: 3.492504835128784\n",
      "lr: 2.8778395287773696e-05 loss: 3.514078140258789\n",
      "lr: 2.873163860136284e-05 loss: 3.3961830139160156\n",
      "lr: 2.8684043289973452e-05 loss: 3.2993435859680176\n",
      "lr: 2.8635612260368384e-05 loss: 3.1974074840545654\n",
      "lr: 2.858634847034986e-05 loss: 3.5613460540771484\n",
      "lr: 2.8536254928578826e-05 loss: 3.5023016929626465\n",
      "lr: 2.848533469439122e-05 loss: 3.3045053482055664\n",
      "lr: 2.8433590877611112e-05 loss: 3.390936851501465\n",
      "lr: 2.8381026638360796e-05 loss: 3.369154691696167\n",
      "lr: 2.8327645186867787e-05 loss: 3.4415359497070312\n",
      "lr: 2.827344978326875e-05 loss: 3.1794776916503906\n",
      "lr: 2.8218443737410426e-05 loss: 3.421030044555664\n",
      "lr: 2.8162630408647467e-05 loss: 3.4648125171661377\n",
      "lr: 2.8106013205637282e-05 loss: 3.429175615310669\n",
      "lr: 2.8048595586131855e-05 loss: 3.394056797027588\n",
      "lr: 2.7990381056766583e-05 loss: 3.278604745864868\n",
      "lr: 2.79313731728461e-05 loss: 3.5902037620544434\n",
      "lr: 2.7871575538127164e-05 loss: 3.114434242248535\n",
      "lr: 2.7810991804598566e-05 loss: 3.1232552528381348\n",
      "lr: 2.7749625672258064e-05 loss: 3.3672585487365723\n",
      "lr: 2.7687480888886462e-05 loss: 3.383324384689331\n",
      "lr: 2.7624561249818683e-05 loss: 3.36008882522583\n",
      "lr: 2.7560870597712008e-05 loss: 3.262422561645508\n",
      "lr: 2.7496412822311375e-05 loss: 3.46838641166687\n",
      "lr: 2.743119186021183e-05 loss: 3.387228488922119\n",
      "lr: 2.7365211694618103e-05 loss: 3.195277690887451\n",
      "lr: 2.729847635510137e-05 loss: 3.518077850341797\n",
      "lr: 2.723098991735312e-05 loss: 3.335344076156616\n",
      "lr: 2.7162756502936278e-05 loss: 3.4477169513702393\n",
      "lr: 2.7093780279033446e-05 loss: 3.4752471446990967\n",
      "lr: 2.7024065458192463e-05 loss: 3.3220810890197754\n",
      "lr: 2.695361629806907e-05 loss: 3.3067924976348877\n",
      "lr: 2.688243710116694e-05 loss: 3.492764711380005\n",
      "lr: 2.681053221457488e-05 loss: 3.078016996383667\n",
      "lr: 2.6737906029701353e-05 loss: 3.358487606048584\n",
      "lr: 2.6664562982006284e-05 loss: 3.259817123413086\n",
      "lr: 2.6590507550730175e-05 loss: 2.9233458042144775\n",
      "lr: 2.6515744258620546e-05 loss: 2.9409189224243164\n",
      "lr: 2.6440277671655733e-05 loss: 3.2989702224731445\n",
      "lr: 2.6364112398765998e-05 loss: 3.133916139602661\n",
      "lr: 2.6287253091552098e-05 loss: 3.2724833488464355\n",
      "lr: 2.6209704444001157e-05 loss: 3.378087282180786\n",
      "lr: 2.613147119220002e-05 loss: 3.339853525161743\n",
      "lr: 2.605255811404598e-05 loss: 3.180027484893799\n",
      "lr: 2.5972970028955037e-05 loss: 3.5492818355560303\n",
      "lr: 2.5892711797567487e-05 loss: 3.1061201095581055\n",
      "lr: 2.581178832145114e-05 loss: 3.2499279975891113\n",
      "lr: 2.5730204542801915e-05 loss: 3.598649024963379\n",
      "lr: 2.564796544414205e-05 loss: 3.2786948680877686\n",
      "lr: 2.5565076048015785e-05 loss: 3.1999526023864746\n",
      "lr: 2.5481541416682624e-05 loss: 2.9596645832061768\n",
      "lr: 2.539736665180818e-05 loss: 3.323906660079956\n",
      "lr: 2.531255689415259e-05 loss: 3.2232933044433594\n",
      "lr: 2.522711732325657e-05 loss: 3.3819069862365723\n",
      "lr: 2.514105315712507e-05 loss: 3.2513208389282227\n",
      "lr: 2.505436965190863e-05 loss: 3.249783992767334\n",
      "lr: 2.4967072101582327e-05 loss: 3.3322925567626953\n",
      "lr: 2.4879165837622508e-05 loss: 3.088700294494629\n",
      "lr: 2.4790656228681145e-05 loss: 3.189023733139038\n",
      "lr: 2.470154868025798e-05 loss: 3.0949716567993164\n",
      "lr: 2.461184863437039e-05 loss: 3.4162120819091797\n",
      "lr: 2.4521561569221034e-05 loss: 3.347566843032837\n",
      "lr: 2.4430692998863272e-05 loss: 3.2218475341796875\n",
      "lr: 2.4339248472864434e-05 loss: 3.1765921115875244\n",
      "lr: 2.424723357596687e-05 loss: 3.1970021724700928\n",
      "lr: 2.4154653927746885e-05 loss: 3.0954413414001465\n",
      "lr: 2.4061515182271534e-05 loss: 3.1536197662353516\n",
      "lr: 2.3967823027753333e-05 loss: 3.123100519180298\n",
      "lr: 2.3873583186202837e-05 loss: 2.917821168899536\n",
      "lr: 2.3778801413079198e-05 loss: 3.284780740737915\n",
      "lr: 2.3683483496938673e-05 loss: 3.195112466812134\n",
      "lr: 2.358763525908108e-05 loss: 3.1765494346618652\n",
      "lr: 2.3491262553194297e-05 loss: 3.3413214683532715\n",
      "lr: 2.339437126499676e-05 loss: 3.179826498031616\n",
      "lr: 2.3296967311877992e-05 loss: 3.1729958057403564\n",
      "lr: 2.3199056642537238e-05 loss: 3.030813694000244\n",
      "lr: 2.3100645236620135e-05 loss: 3.208850860595703\n",
      "lr: 2.3001739104353555e-05 loss: 3.0971643924713135\n",
      "lr: 2.2902344286178514e-05 loss: 2.915194034576416\n",
      "lr: 2.280246685238127e-05 loss: 3.040727376937866\n",
      "lr: 2.270211290272263e-05 loss: 3.309417963027954\n",
      "lr: 2.260128856606538e-05 loss: 3.0132288932800293\n",
      "lr: 2.25e-05 loss: 2.8215978145599365\n",
      "lr: 2.2398253390468613e-05 loss: 2.534893751144409\n",
      "lr: 2.2296054951387168e-05 loss: 2.956123113632202\n",
      "lr: 2.2193410924265968e-05 loss: 3.138045310974121\n",
      "lr: 2.209032757782848e-05 loss: 2.850658416748047\n",
      "lr: 2.1986811207628468e-05 loss: 2.934598445892334\n",
      "lr: 2.1882868135665536e-05 loss: 3.2771525382995605\n",
      "lr: 2.1778504709999008e-05 loss: 2.9689929485321045\n",
      "lr: 2.167372730436025e-05 loss: 3.005380630493164\n",
      "lr: 2.1568542317763397e-05 loss: 3.330352544784546\n",
      "lr: 2.1462956174114555e-05 loss: 2.8378827571868896\n",
      "lr: 2.1356975321819493e-05 loss: 2.910034418106079\n",
      "lr: 2.12506062333898e-05 loss: 2.9428153038024902\n",
      "lr: 2.1143855405047594e-05 loss: 2.9190304279327393\n",
      "lr: 2.10367293563288e-05 loss: 2.822561264038086\n",
      "lr: 2.092923462968498e-05 loss: 3.0027759075164795\n",
      "lr: 2.0821377790083748e-05 loss: 3.123687982559204\n",
      "lr: 2.0713165424607856e-05 loss: 3.094517707824707\n",
      "lr: 2.06046041420529e-05 loss: 3.347917318344116\n",
      "lr: 2.0495700572523695e-05 loss: 3.0617167949676514\n",
      "lr: 2.038646136702937e-05 loss: 2.8324310779571533\n",
      "lr: 2.0276893197077167e-05 loss: 3.01798415184021\n",
      "lr: 2.0167002754264995e-05 loss: 3.006282329559326\n",
      "lr: 2.0056796749872762e-05 loss: 2.7380223274230957\n",
      "lr: 1.9946281914452498e-05 loss: 3.111698865890503\n",
      "lr: 1.9835464997417306e-05 loss: 2.8548412322998047\n",
      "lr: 1.9724352766629157e-05 loss: 2.8959639072418213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 1.9612952007985554e-05 loss: 3.141493320465088\n",
      "lr: 1.9501269525005113e-05 loss: 2.9826714992523193\n",
      "lr: 1.9389312138412048e-05 loss: 3.0562903881073\n",
      "lr: 1.9277086685719606e-05 loss: 2.9038913249969482\n",
      "lr: 1.9164600020812507e-05 loss: 3.13905668258667\n",
      "lr: 1.9051859013528333e-05 loss: 3.025085926055908\n",
      "lr: 1.8938870549237976e-05 loss: 2.646761178970337\n",
      "lr: 1.8825641528425147e-05 loss: 2.9008121490478516\n",
      "lr: 1.8712178866264932e-05 loss: 2.7262747287750244\n",
      "lr: 1.859848949220147e-05 loss: 3.090519905090332\n",
      "lr: 1.848458034952475e-05 loss: 3.1595470905303955\n",
      "lr: 1.8370458394946576e-05 loss: 3.274609327316284\n",
      "lr: 1.8256130598175702e-05 loss: 2.900063991546631\n",
      "lr: 1.8141603941492165e-05 loss: 3.274812698364258\n",
      "lr: 1.802688541932087e-05 loss: 2.8855886459350586\n",
      "lr: 1.7911982037804417e-05 loss: 2.999274969100952\n",
      "lr: 1.779690081437522e-05 loss: 2.902615785598755\n",
      "lr: 1.768164877732694e-05 loss: 3.0955069065093994\n",
      "lr: 1.756623296538525e-05 loss: 2.7919762134552\n",
      "lr: 1.7450660427277957e-05 loss: 2.8096799850463867\n",
      "lr: 1.7334938221304504e-05 loss: 3.0983223915100098\n",
      "lr: 1.721907341490495e-05 loss: 2.905311346054077\n",
      "lr: 1.710307308422829e-05 loss: 2.499553918838501\n",
      "lr: 1.6986944313700324e-05 loss: 2.555957794189453\n",
      "lr: 1.6870694195590995e-05 loss: 3.036648750305176\n",
      "lr: 1.6754329829581242e-05 loss: 2.9140141010284424\n",
      "lr: 1.6637858322329393e-05 loss: 2.7881710529327393\n",
      "lr: 1.652128678703718e-05 loss: 2.651143789291382\n",
      "lr: 1.6404622343015264e-05 loss: 2.850102663040161\n",
      "lr: 1.628787211524849e-05 loss: 2.463404417037964\n",
      "lr: 1.6171043233960725e-05 loss: 2.8651809692382812\n",
      "lr: 1.6054142834179396e-05 loss: 2.677767276763916\n",
      "lr: 1.5937178055299746e-05 loss: 3.181518316268921\n",
      "lr: 1.582015604064879e-05 loss: 2.5063421726226807\n",
      "lr: 1.57030839370491e-05 loss: 2.671006441116333\n",
      "lr: 1.5585968894382294e-05 loss: 2.6786606311798096\n",
      "lr: 1.5468818065152364e-05 loss: 2.8561553955078125\n",
      "lr: 1.5351638604048906e-05 loss: 3.0809733867645264\n",
      "lr: 1.5234437667510119e-05 loss: 2.920475721359253\n",
      "\n",
      "\n",
      "Eval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e01bca3072455191afbf9fc0cd070c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=201, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'step': 200, 'lr': 1.5117222413285759e-05, 'tr_loss': 3.25203120470047, 'eval_val_loss': 2.7574466573658274, 'eval_val_acc': 0.3493150684931507}\n",
      "lr: 1.5117222413285759e-05 loss: 2.7136170864105225\n",
      "lr: 1.5e-05 loss: 2.511777639389038\n",
      "lr: 1.488277758671424e-05 loss: 2.877239465713501\n",
      "lr: 1.4765562332489879e-05 loss: 2.8443100452423096\n",
      "lr: 1.4648361395951101e-05 loss: 2.5716476440429688\n",
      "lr: 1.453118193484764e-05 loss: 2.6022329330444336\n",
      "lr: 1.441403110561771e-05 loss: 2.4898176193237305\n",
      "lr: 1.4296916062950905e-05 loss: 2.9614145755767822\n",
      "lr: 1.4179843959351214e-05 loss: 3.109053134918213\n",
      "lr: 1.4062821944700258e-05 loss: 2.7403931617736816\n",
      "lr: 1.3945857165820612e-05 loss: 2.680091381072998\n",
      "lr: 1.3828956766039278e-05 loss: 2.5370399951934814\n",
      "lr: 1.371212788475151e-05 loss: 2.292161226272583\n",
      "lr: 1.359537765698474e-05 loss: 2.4624736309051514\n",
      "lr: 1.3478713212962825e-05 loss: 2.526261329650879\n",
      "lr: 1.3362141677670611e-05 loss: 2.650351047515869\n",
      "lr: 1.3245670170418764e-05 loss: 2.5551509857177734\n",
      "lr: 1.312930580440901e-05 loss: 2.337523937225342\n",
      "lr: 1.3013055686299683e-05 loss: 2.6840004920959473\n",
      "lr: 1.2896926915771714e-05 loss: 2.987779140472412\n",
      "lr: 1.2780926585095053e-05 loss: 3.087477445602417\n",
      "lr: 1.2665061778695496e-05 loss: 2.5839271545410156\n",
      "lr: 1.2549339572722044e-05 loss: 2.7941622734069824\n",
      "lr: 1.2433767034614751e-05 loss: 2.9188613891601562\n",
      "lr: 1.231835122267306e-05 loss: 2.1365628242492676\n",
      "lr: 1.2203099185624779e-05 loss: 2.1510095596313477\n",
      "lr: 1.2088017962195586e-05 loss: 2.716210126876831\n",
      "lr: 1.197311458067913e-05 loss: 2.659043788909912\n",
      "lr: 1.185839605850783e-05 loss: 2.6372556686401367\n",
      "lr: 1.1743869401824295e-05 loss: 3.0366992950439453\n",
      "lr: 1.162954160505342e-05 loss: 2.307495355606079\n",
      "lr: 1.1515419650475254e-05 loss: 2.8867576122283936\n",
      "lr: 1.1401510507798539e-05 loss: 2.7466416358947754\n",
      "lr: 1.128782113373507e-05 loss: 2.741701364517212\n",
      "lr: 1.1174358471574854e-05 loss: 3.07006573677063\n",
      "lr: 1.1061129450762028e-05 loss: 2.5419514179229736\n",
      "lr: 1.0948140986471667e-05 loss: 2.732208490371704\n",
      "lr: 1.083539997918749e-05 loss: 2.6989479064941406\n",
      "lr: 1.0722913314280395e-05 loss: 2.8034653663635254\n",
      "lr: 1.0610687861587954e-05 loss: 2.550462007522583\n",
      "lr: 1.0498730474994891e-05 loss: 2.5069563388824463\n",
      "lr: 1.0387047992014452e-05 loss: 2.2215981483459473\n",
      "lr: 1.0275647233370842e-05 loss: 2.5490379333496094\n",
      "lr: 1.0164535002582695e-05 loss: 2.7096829414367676\n",
      "lr: 1.0053718085547503e-05 loss: 2.784306526184082\n",
      "lr: 9.943203250127243e-06 loss: 2.349069595336914\n",
      "lr: 9.832997245735008e-06 loss: 2.4897565841674805\n",
      "lr: 9.723106802922834e-06 loss: 2.580812931060791\n",
      "lr: 9.613538632970634e-06 loss: 2.64762806892395\n",
      "lr: 9.504299427476306e-06 loss: 2.901165723800659\n",
      "lr: 9.395395857947104e-06 loss: 2.4932141304016113\n",
      "lr: 9.286834575392148e-06 loss: 2.5695159435272217\n",
      "lr: 9.17862220991626e-06 loss: 2.9557604789733887\n",
      "lr: 9.070765370315017e-06 loss: 2.363410472869873\n",
      "lr: 8.9632706436712e-06 loss: 2.8295624256134033\n",
      "lr: 8.856144594952402e-06 loss: 2.894728422164917\n",
      "lr: 8.7493937666102e-06 loss: 2.8712472915649414\n",
      "lr: 8.643024678180503e-06 loss: 3.1228132247924805\n",
      "lr: 8.537043825885446e-06 loss: 2.7462563514709473\n",
      "lr: 8.43145768223661e-06 loss: 2.782935857772827\n",
      "lr: 8.326272695639759e-06 loss: 2.7663347721099854\n",
      "lr: 8.221495290000995e-06 loss: 2.8651201725006104\n",
      "lr: 8.117131864334465e-06 loss: 2.8472585678100586\n",
      "lr: 8.013188792371528e-06 loss: 2.9441211223602295\n",
      "lr: 7.909672422171519e-06 loss: 2.5837137699127197\n",
      "lr: 7.806589075734031e-06 loss: 3.005296230316162\n",
      "lr: 7.703945048612838e-06 loss: 2.696722984313965\n",
      "lr: 7.601746609531392e-06 loss: 2.5773072242736816\n",
      "lr: 7.500000000000004e-06 loss: 2.471510648727417\n",
      "lr: 7.398711433934615e-06 loss: 2.3870551586151123\n",
      "lr: 7.297887097277373e-06 loss: 2.85398006439209\n",
      "lr: 7.19753314761873e-06 loss: 2.983961582183838\n",
      "lr: 7.097655713821491e-06 loss: 2.664708137512207\n",
      "lr: 6.998260895646449e-06 loss: 2.0968520641326904\n",
      "lr: 6.899354763379866e-06 loss: 2.6059980392456055\n",
      "lr: 6.800943357462768e-06 loss: 2.8542392253875732\n",
      "lr: 6.703032688122009e-06 loss: 2.8912193775177\n",
      "lr: 6.605628735003245e-06 loss: 3.2048895359039307\n",
      "lr: 6.508737446805703e-06 loss: 2.574587345123291\n",
      "lr: 6.412364740918924e-06 loss: 2.705329656600952\n",
      "lr: 6.316516503061328e-06 loss: 2.4334633350372314\n",
      "lr: 6.221198586920804e-06 loss: 2.2716267108917236\n",
      "lr: 6.126416813797164e-06 loss: 2.6824119091033936\n",
      "lr: 6.032176972246669e-06 loss: 2.934514045715332\n",
      "lr: 5.9384848177284656e-06 loss: 2.556117057800293\n",
      "lr: 5.8453460722531206e-06 loss: 2.6265697479248047\n",
      "lr: 5.752766424033135e-06 loss: 2.5969090461730957\n",
      "lr: 5.660751527135566e-06 loss: 2.459496021270752\n",
      "lr: 5.569307001136725e-06 loss: 2.4814419746398926\n",
      "lr: 5.478438430778968e-06 loss: 2.0179758071899414\n",
      "lr: 5.388151365629606e-06 loss: 2.633514642715454\n",
      "lr: 5.29845131974202e-06 loss: 2.54917049407959\n",
      "lr: 5.209343771318858e-06 loss: 2.8102986812591553\n",
      "lr: 5.120834162377498e-06 loss: 2.358464479446411\n",
      "lr: 5.032927898417671e-06 loss: 2.5685365200042725\n",
      "lr: 4.945630348091379e-06 loss: 2.8588106632232666\n",
      "lr: 4.858946842874928e-06 loss: 2.473423719406128\n",
      "lr: 4.772882676743432e-06 loss: 2.2637832164764404\n",
      "lr: 4.687443105847413e-06 loss: 2.6489875316619873\n",
      "lr: 4.602633348191823e-06 loss: 2.3783960342407227\n",
      "lr: 4.51845858331738e-06 loss: 2.695582866668701\n",
      "lr: 4.434923951984218e-06 loss: 2.686727285385132\n",
      "lr: 4.3520345558579555e-06 loss: 2.6949992179870605\n",
      "lr: 4.2697954571980855e-06 loss: 2.5643136501312256\n",
      "lr: 4.188211678548867e-06 loss: 2.3556253910064697\n",
      "lr: 4.107288202432515e-06 loss: 2.479962110519409\n",
      "lr: 4.027029971044969e-06 loss: 2.454263925552368\n",
      "lr: 3.94744188595402e-06 loss: 3.1701693534851074\n",
      "lr: 3.868528807799987e-06 loss: 2.6761722564697266\n",
      "lr: 3.7902955559988437e-06 loss: 2.683730125427246\n",
      "lr: 3.712746908447905e-06 loss: 1.9907774925231934\n",
      "lr: 3.6358876012340013e-06 loss: 2.670663356781006\n",
      "lr: 3.5597223283442724e-06 loss: 2.7115962505340576\n",
      "lr: 3.4842557413794548e-06 loss: 2.557906150817871\n",
      "lr: 3.4094924492698314e-06 loss: 3.055661916732788\n",
      "lr: 3.335437017993719e-06 loss: 2.6879611015319824\n",
      "lr: 3.26209397029865e-06 loss: 2.407841682434082\n",
      "lr: 3.189467785425119e-06 loss: 2.776014566421509\n",
      "lr: 3.11756289883306e-06 loss: 2.2978060245513916\n",
      "lr: 3.0463837019309293e-06 loss: 2.395169258117676\n",
      "lr: 2.975934541807544e-06 loss: 2.9222629070281982\n",
      "lr: 2.9062197209665508e-06 loss: 2.5168120861053467\n",
      "lr: 2.8372434970637247e-06 loss: 2.83193302154541\n",
      "lr: 2.7690100826468757e-06 loss: 2.8324239253997803\n",
      "lr: 2.7015236448986313e-06 loss: 2.53760027885437\n",
      "lr: 2.6347883053818996e-06 loss: 2.665958881378174\n",
      "lr: 2.5688081397881793e-06 loss: 2.4036173820495605\n",
      "lr: 2.5035871776886323e-06 loss: 2.8667657375335693\n",
      "lr: 2.439129402287991e-06 loss: 2.7623562812805176\n",
      "lr: 2.3754387501813196e-06 loss: 2.759381055831909\n",
      "lr: 2.3125191111135387e-06 loss: 2.984370470046997\n",
      "lr: 2.250374327741937e-06 loss: 2.0680782794952393\n",
      "lr: 2.1890081954014397e-06 loss: 2.7456681728363037\n",
      "lr: 2.1284244618728386e-06 loss: 2.293093681335449\n",
      "lr: 2.068626827153904e-06 loss: 2.4854650497436523\n",
      "lr: 2.009618943233423e-06 loss: 3.018216133117676\n",
      "lr: 1.951404413868147e-06 loss: 2.904984474182129\n",
      "lr: 1.8939867943627177e-06 loss: 2.4842138290405273\n",
      "lr: 1.8373695913525318e-06 loss: 2.86836314201355\n",
      "lr: 1.781556262589576e-06 loss: 2.8345465660095215\n",
      "lr: 1.7265502167312496e-06 loss: 2.8061282634735107\n",
      "lr: 1.6723548131322176e-06 loss: 2.375521421432495\n",
      "lr: 1.6189733616392027e-06 loss: 2.6895391941070557\n",
      "lr: 1.5664091223888893e-06 loss: 2.147416591644287\n",
      "lr: 1.5146653056087811e-06 loss: 2.608544111251831\n",
      "lr: 1.4637450714211752e-06 loss: 2.295759439468384\n",
      "lr: 1.4136515296501418e-06 loss: 2.6870546340942383\n",
      "lr: 1.3643877396316167e-06 loss: 2.74737286567688\n",
      "lr: 1.3159567100265474e-06 loss: 2.5470056533813477\n",
      "lr: 1.2683613986371602e-06 loss: 2.733785629272461\n",
      "lr: 1.2216047122263018e-06 loss: 2.4204039573669434\n",
      "lr: 1.1756895063399392e-06 loss: 3.008206605911255\n",
      "lr: 1.1306185851327404e-06 loss: 2.8530521392822266\n",
      "lr: 1.086394701196835e-06 loss: 2.753288984298706\n",
      "lr: 1.0430205553937038e-06 loss: 2.7169792652130127\n",
      "lr: 1.0004987966892153e-06 loss: 2.8880677223205566\n",
      "lr: 9.588320219918777e-07 loss: 2.7960777282714844\n",
      "lr: 9.180227759942045e-07 loss: 2.6860342025756836\n",
      "lr: 8.780735510173316e-07 loss: 2.676518201828003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 8.389867868587847e-07 loss: 2.4385290145874023\n",
      "lr: 8.0076487064349e-07 loss: 2.9543426036834717\n",
      "lr: 7.634101366779755e-07 loss: 2.8176615238189697\n",
      "lr: 7.269248663078187e-07 loss: 3.1260740756988525\n",
      "lr: 6.913112877783146e-07 loss: 2.7649805545806885\n",
      "lr: 6.565715760983875e-07 loss: 2.824313163757324\n",
      "lr: 6.227078529077651e-07 loss: 2.608370542526245\n",
      "lr: 5.89722186347399e-07 loss: 2.652054786682129\n",
      "lr: 5.576165909331582e-07 loss: 2.748356342315674\n",
      "lr: 5.263930274328061e-07 loss: 2.4118731021881104\n",
      "lr: 4.960534027462387e-07 loss: 2.4062788486480713\n",
      "lr: 4.6659956978904405e-07 loss: 2.6877706050872803\n",
      "lr: 4.38033327379313e-07 loss: 2.752072811126709\n",
      "lr: 4.1035642012780484e-07 loss: 2.2659661769866943\n",
      "lr: 3.8357053833139034e-07 loss: 2.5313711166381836\n",
      "lr: 3.5767731786982117e-07 loss: 2.5465307235717773\n",
      "lr: 3.326783401058231e-07 loss: 2.504688024520874\n",
      "lr: 3.085751317885199e-07 loss: 2.5700783729553223\n",
      "lr: 2.8536916496018803e-07 loss: 2.8314921855926514\n",
      "lr: 2.630618568663584e-07 loss: 2.2327940464019775\n",
      "lr: 2.4165456986925904e-07 loss: 2.6299233436584473\n",
      "lr: 2.2114861136461662e-07 loss: 2.75390887260437\n",
      "lr: 2.0154523370180366e-07 loss: 2.319119453430176\n",
      "lr: 1.8284563410735978e-07 loss: 2.8211567401885986\n",
      "lr: 1.6505095461187337e-07 loss: 2.64762544631958\n",
      "lr: 1.4816228198023585e-07 loss: 2.6530370712280273\n",
      "lr: 1.321806476452664e-07 loss: 2.647080659866333\n",
      "lr: 1.1710702764472059e-07 loss: 2.5955586433410645\n",
      "lr: 1.0294234256168821e-07 loss: 2.663972854614258\n",
      "lr: 8.968745746835982e-08 loss: 2.5492937564849854\n",
      "lr: 7.734318187320566e-08 loss: 2.8166086673736572\n",
      "lr: 6.591026967152858e-08 loss: 2.2203617095947266\n",
      "lr: 5.538941909942585e-08 loss: 2.8251309394836426\n",
      "lr: 4.578127269114829e-08 loss: 2.5202343463897705\n",
      "lr: 3.7086417239848294e-08 loss: 2.69212007522583\n",
      "lr: 2.9305383761760175e-08 loss: 2.473867416381836\n",
      "lr: 2.2438647463757788e-08 loss: 2.485234260559082\n",
      "lr: 1.648662771432774e-08 loss: 2.5380141735076904\n",
      "lr: 1.1449688017976522e-08 loss: 2.639503240585327\n",
      "lr: 7.328135993011631e-09 loss: 3.2461793422698975\n",
      "lr: 4.1222233527615786e-09 loss: 2.7232470512390137\n",
      "\n",
      "\n",
      "Eval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13eade31478646bf997887ee4279d15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=201, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'step': 400, 'lr': 1.8321458902048616e-09, 'tr_loss': 2.6493070411682127, 'eval_val_loss': 2.5445440960167653, 'eval_val_acc': 0.4576587795765878}\n",
      "lr: 1.8321458902048616e-09 loss: 2.643852710723877\n",
      "lr: 4.580434660111932e-10 loss: 2.702704429626465\n",
      "lr: 3e-05 loss: 2.7976865768432617\n",
      "lr: 2.999954195653399e-05 loss: 2.641946315765381\n",
      "lr: 2.9998167854109796e-05 loss: 2.690819263458252\n",
      "lr: 2.999587777664724e-05 loss: 2.423555850982666\n",
      "lr: 2.9992671864006992e-05 loss: 2.4039835929870605\n",
      "lr: 2.9988550311982027e-05 loss: 2.873582124710083\n",
      "lr: 2.9983513372285672e-05 loss: 2.932002067565918\n",
      "lr: 2.9977561352536243e-05 loss: 2.6980550289154053\n",
      "lr: 2.997069461623824e-05 loss: 2.69049072265625\n",
      "lr: 2.9962913582760155e-05 loss: 2.165360927581787\n",
      "lr: 2.9954218727308853e-05 loss: 2.627767324447632\n",
      "lr: 2.9944610580900574e-05 loss: 2.307570695877075\n",
      "lr: 2.9934089730328472e-05 loss: 2.4167251586914062\n",
      "lr: 2.9922656818126796e-05 loss: 2.465825319290161\n",
      "lr: 2.991031254253164e-05 loss: 2.2763400077819824\n",
      "lr: 2.9897057657438315e-05 loss: 2.9288201332092285\n",
      "lr: 2.988289297235528e-05 loss: 2.4819140434265137\n",
      "lr: 2.9867819352354733e-05 loss: 2.642120361328125\n",
      "lr: 2.9851837718019766e-05 loss: 2.6142473220825195\n",
      "lr: 2.983494904538813e-05 loss: 2.115997791290283\n",
      "lr: 2.9817154365892643e-05 loss: 2.6432487964630127\n",
      "lr: 2.97984547662982e-05 loss: 2.8625028133392334\n",
      "lr: 2.9778851388635386e-05 loss: 2.1322591304779053\n",
      "lr: 2.975834543013074e-05 loss: 2.524678945541382\n",
      "lr: 2.9736938143133644e-05 loss: 2.9675180912017822\n",
      "lr: 2.971463083503981e-05 loss: 2.340254545211792\n",
      "lr: 2.9691424868211482e-05 loss: 2.398491144180298\n",
      "lr: 2.9667321659894178e-05 loss: 1.957450270652771\n",
      "lr: 2.9642322682130184e-05 loss: 2.3111488819122314\n",
      "lr: 2.961642946166861e-05 loss: 2.5989344120025635\n",
      "lr: 2.9589643579872193e-05 loss: 2.244227886199951\n",
      "lr: 2.9561966672620687e-05 loss: 2.3422319889068604\n",
      "lr: 2.9533400430210955e-05 loss: 2.232548236846924\n",
      "lr: 2.9503946597253763e-05 loss: 2.6091105937957764\n",
      "lr: 2.9473606972567196e-05 loss: 2.1272709369659424\n",
      "lr: 2.9442383409066845e-05 loss: 2.1949141025543213\n",
      "lr: 2.94102778136526e-05 loss: 2.384453773498535\n",
      "lr: 2.937729214709224e-05 loss: 2.422794818878174\n",
      "lr: 2.9343428423901614e-05 loss: 2.9324138164520264\n",
      "lr: 2.9308688712221687e-05 loss: 2.678931474685669\n",
      "lr: 2.927307513369218e-05 loss: 2.6728100776672363\n",
      "lr: 2.9236589863322025e-05 loss: 2.28945255279541\n",
      "lr: 2.9199235129356513e-05 loss: 2.3013222217559814\n",
      "lr: 2.916101321314122e-05 loss: 2.7365238666534424\n",
      "lr: 2.912192644898267e-05 loss: 2.849482536315918\n",
      "lr: 2.90819772240058e-05 loss: 2.5396745204925537\n",
      "lr: 2.9041167978008123e-05 loss: 2.6769282817840576\n",
      "lr: 2.899950120331079e-05 loss: 1.9359201192855835\n",
      "lr: 2.89569794446063e-05 loss: 2.3123209476470947\n",
      "lr: 2.891360529880317e-05 loss: 2.352090358734131\n",
      "lr: 2.886938141486726e-05 loss: 2.3940610885620117\n",
      "lr: 2.882431049366006e-05 loss: 1.9488599300384521\n",
      "lr: 2.8778395287773696e-05 loss: 2.017967700958252\n",
      "lr: 2.8731638601362844e-05 loss: 2.3481438159942627\n",
      "lr: 2.868404328997345e-05 loss: 2.406437397003174\n",
      "lr: 2.8635612260368384e-05 loss: 2.5301334857940674\n",
      "lr: 2.858634847034986e-05 loss: 2.8196654319763184\n",
      "lr: 2.8536254928578826e-05 loss: 2.497042655944824\n",
      "lr: 2.8485334694391216e-05 loss: 2.377384662628174\n",
      "lr: 2.8433590877611112e-05 loss: 2.353999376296997\n",
      "lr: 2.8381026638360796e-05 loss: 2.721696615219116\n",
      "lr: 2.8327645186867783e-05 loss: 2.6068501472473145\n",
      "lr: 2.8273449783268754e-05 loss: 2.482280731201172\n",
      "lr: 2.8218443737410426e-05 loss: 2.3689441680908203\n",
      "lr: 2.816263040864747e-05 loss: 2.329552412033081\n",
      "lr: 2.8106013205637282e-05 loss: 2.729478359222412\n",
      "lr: 2.8048595586131855e-05 loss: 2.439870595932007\n",
      "lr: 2.799038105676658e-05 loss: 2.5129528045654297\n",
      "lr: 2.79313731728461e-05 loss: 2.512707233428955\n",
      "lr: 2.7871575538127164e-05 loss: 2.1524527072906494\n",
      "lr: 2.7810991804598566e-05 loss: 2.3829309940338135\n",
      "lr: 2.7749625672258064e-05 loss: 1.9762669801712036\n",
      "lr: 2.7687480888886462e-05 loss: 1.9366059303283691\n",
      "lr: 2.7624561249818683e-05 loss: 2.547457695007324\n",
      "lr: 2.756087059771201e-05 loss: 2.4584696292877197\n",
      "lr: 2.7496412822311375e-05 loss: 1.988420009613037\n",
      "lr: 2.7431191860211826e-05 loss: 1.85216224193573\n",
      "lr: 2.73652116946181e-05 loss: 1.9677464962005615\n",
      "lr: 2.7298476355101372e-05 loss: 2.1975059509277344\n",
      "lr: 2.7230989917353125e-05 loss: 2.4752211570739746\n",
      "lr: 2.7162756502936278e-05 loss: 2.2296481132507324\n",
      "lr: 2.7093780279033446e-05 loss: 2.2792975902557373\n",
      "lr: 2.7024065458192463e-05 loss: 2.338102102279663\n",
      "lr: 2.6953616298069067e-05 loss: 2.289166212081909\n",
      "lr: 2.6882437101166945e-05 loss: 2.396589517593384\n",
      "lr: 2.6810532214574883e-05 loss: 2.216010093688965\n",
      "lr: 2.6737906029701353e-05 loss: 2.566984176635742\n",
      "lr: 2.6664562982006287e-05 loss: 2.393751621246338\n",
      "lr: 2.6590507550730175e-05 loss: 2.4566385746002197\n",
      "lr: 2.651574425862055e-05 loss: 2.3683929443359375\n",
      "lr: 2.6440277671655733e-05 loss: 2.093352794647217\n",
      "lr: 2.6364112398765998e-05 loss: 2.294569969177246\n",
      "lr: 2.6287253091552094e-05 loss: 2.5044515132904053\n",
      "lr: 2.6209704444001164e-05 loss: 2.467191457748413\n",
      "lr: 2.613147119220002e-05 loss: 2.3953075408935547\n",
      "lr: 2.6052558114045983e-05 loss: 2.449733018875122\n",
      "lr: 2.5972970028955037e-05 loss: 1.9528861045837402\n",
      "lr: 2.5892711797567494e-05 loss: 2.5992088317871094\n",
      "lr: 2.5811788321451134e-05 loss: 2.012876510620117\n",
      "lr: 2.5730204542801915e-05 loss: 2.109851837158203\n",
      "lr: 2.564796544414205e-05 loss: 2.5140597820281982\n",
      "lr: 2.5565076048015785e-05 loss: 1.8502399921417236\n",
      "lr: 2.548154141668263e-05 loss: 1.9706412553787231\n",
      "lr: 2.5397366651808176e-05 loss: 2.421031951904297\n",
      "lr: 2.5312556894152582e-05 loss: 1.916711688041687\n",
      "lr: 2.522711732325657e-05 loss: 2.262385129928589\n",
      "lr: 2.5141053157125076e-05 loss: 2.387274742126465\n",
      "lr: 2.5054369651908622e-05 loss: 2.449861526489258\n",
      "lr: 2.4967072101582324e-05 loss: 2.335721731185913\n",
      "lr: 2.4879165837622514e-05 loss: 2.2157626152038574\n",
      "lr: 2.4790656228681145e-05 loss: 2.1807756423950195\n",
      "lr: 2.470154868025798e-05 loss: 2.3594441413879395\n",
      "lr: 2.461184863437039e-05 loss: 2.2447190284729004\n",
      "lr: 2.452156156922103e-05 loss: 2.238861322402954\n",
      "lr: 2.4430692998863276e-05 loss: 2.644338369369507\n",
      "lr: 2.4339248472864444e-05 loss: 2.1915063858032227\n",
      "lr: 2.424723357596687e-05 loss: 2.4973931312561035\n",
      "lr: 2.4154653927746882e-05 loss: 2.3409101963043213\n",
      "lr: 2.4061515182271537e-05 loss: 2.1823174953460693\n",
      "lr: 2.3967823027753333e-05 loss: 2.106487274169922\n",
      "lr: 2.3873583186202837e-05 loss: 2.415820598602295\n",
      "lr: 2.3778801413079198e-05 loss: 2.771085023880005\n",
      "lr: 2.368348349693867e-05 loss: 2.5228185653686523\n",
      "lr: 2.3587635259081087e-05 loss: 2.4583940505981445\n",
      "lr: 2.349126255319429e-05 loss: 2.7144575119018555\n",
      "lr: 2.339437126499676e-05 loss: 1.989790916442871\n",
      "lr: 2.329696731187799e-05 loss: 2.368008613586426\n",
      "lr: 2.3199056642537245e-05 loss: 2.1359925270080566\n",
      "lr: 2.3100645236620135e-05 loss: 2.0359573364257812\n",
      "lr: 2.3001739104353558e-05 loss: 1.9996141195297241\n",
      "lr: 2.2902344286178514e-05 loss: 2.383098840713501\n",
      "lr: 2.280246685238127e-05 loss: 2.3514068126678467\n",
      "lr: 2.270211290272263e-05 loss: 2.0894675254821777\n",
      "lr: 2.2601288566065377e-05 loss: 2.348071336746216\n",
      "lr: 2.2500000000000008e-05 loss: 2.1342036724090576\n",
      "lr: 2.2398253390468613e-05 loss: 2.0707879066467285\n",
      "lr: 2.2296054951387158e-05 loss: 2.0293407440185547\n",
      "lr: 2.2193410924265968e-05 loss: 1.8337651491165161\n",
      "lr: 2.2090327577828487e-05 loss: 2.0047149658203125\n",
      "lr: 2.198681120762847e-05 loss: 1.8969817161560059\n",
      "lr: 2.188286813566553e-05 loss: 2.615447521209717\n",
      "lr: 2.1778504709999008e-05 loss: 2.056291341781616\n",
      "lr: 2.167372730436025e-05 loss: 2.013578176498413\n",
      "lr: 2.15685423177634e-05 loss: 2.4430291652679443\n",
      "lr: 2.1462956174114562e-05 loss: 2.4779186248779297\n",
      "lr: 2.1356975321819486e-05 loss: 1.6397403478622437\n",
      "lr: 2.12506062333898e-05 loss: 2.378506898880005\n",
      "lr: 2.1143855405047604e-05 loss: 2.11834716796875\n",
      "lr: 2.103672935632881e-05 loss: 1.9430922269821167\n",
      "lr: 2.0929234629684973e-05 loss: 2.5980942249298096\n",
      "lr: 2.0821377790083748e-05 loss: 2.0338752269744873\n",
      "lr: 2.0713165424607853e-05 loss: 2.4840357303619385\n",
      "lr: 2.0604604142052907e-05 loss: 1.950480341911316\n",
      "lr: 2.049570057252369e-05 loss: 2.1035449504852295\n",
      "lr: 2.0386461367029363e-05 loss: 2.6307926177978516\n",
      "lr: 2.0276893197077167e-05 loss: 2.504209518432617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 2.0167002754265005e-05 loss: 1.7704766988754272\n",
      "lr: 2.0056796749872762e-05 loss: 2.3569536209106445\n",
      "lr: 1.9946281914452498e-05 loss: 2.048210859298706\n",
      "lr: 1.9835464997417313e-05 loss: 2.1447672843933105\n",
      "lr: 1.9724352766629157e-05 loss: 2.5132648944854736\n",
      "lr: 1.9612952007985554e-05 loss: 2.1755142211914062\n",
      "lr: 1.9501269525005113e-05 loss: 1.8429111242294312\n",
      "lr: 1.938931213841204e-05 loss: 1.7633916139602661\n",
      "lr: 1.9277086685719612e-05 loss: 1.940122127532959\n",
      "lr: 1.91646000208125e-05 loss: 2.11788272857666\n",
      "lr: 1.9051859013528333e-05 loss: 2.011408805847168\n",
      "lr: 1.8938870549237973e-05 loss: 1.9885951280593872\n",
      "lr: 1.8825641528425153e-05 loss: 2.0904362201690674\n",
      "lr: 1.8712178866264935e-05 loss: 2.0205647945404053\n",
      "lr: 1.8598489492201472e-05 loss: 2.722196340560913\n",
      "lr: 1.848458034952475e-05 loss: 2.281071424484253\n",
      "lr: 1.8370458394946576e-05 loss: 1.749297022819519\n",
      "lr: 1.825613059817571e-05 loss: 1.8995946645736694\n",
      "lr: 1.8141603941492158e-05 loss: 2.2822086811065674\n",
      "lr: 1.8026885419320873e-05 loss: 2.4805679321289062\n",
      "lr: 1.7911982037804417e-05 loss: 2.1650240421295166\n",
      "lr: 1.7796900814375225e-05 loss: 2.219736337661743\n",
      "lr: 1.7681648777326935e-05 loss: 2.1089890003204346\n",
      "lr: 1.7566232965385255e-05 loss: 1.9834545850753784\n",
      "lr: 1.7450660427277957e-05 loss: 2.0617568492889404\n",
      "lr: 1.7334938221304504e-05 loss: 2.188145399093628\n",
      "lr: 1.721907341490495e-05 loss: 2.2206456661224365\n",
      "lr: 1.7103073084228284e-05 loss: 2.3664324283599854\n",
      "lr: 1.6986944313700328e-05 loss: 2.2113304138183594\n",
      "lr: 1.6870694195590995e-05 loss: 2.281010627746582\n",
      "lr: 1.675432982958123e-05 loss: 1.647344708442688\n",
      "lr: 1.663785832232939e-05 loss: 1.5471395254135132\n",
      "lr: 1.6521286787037186e-05 loss: 2.0820016860961914\n",
      "lr: 1.6404622343015268e-05 loss: 2.1057684421539307\n",
      "lr: 1.628787211524848e-05 loss: 2.392853021621704\n",
      "lr: 1.6171043233960725e-05 loss: 2.089704990386963\n",
      "lr: 1.6054142834179393e-05 loss: 2.381767511367798\n",
      "lr: 1.593717805529975e-05 loss: 2.363431453704834\n",
      "lr: 1.5820156040648784e-05 loss: 2.32034969329834\n",
      "lr: 1.5703083937049096e-05 loss: 1.8361833095550537\n",
      "lr: 1.5585968894382294e-05 loss: 2.2773194313049316\n",
      "lr: 1.5468818065152374e-05 loss: 2.356536388397217\n",
      "\n",
      "\n",
      "Eval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41abb07f4f284c658434f0cf52967636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=201, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'step': 600, 'lr': 1.5351638604048917e-05, 'tr_loss': 2.3106106197834015, 'eval_val_loss': 2.0587230867414332, 'eval_val_acc': 0.5603985056039851}\n",
      "lr: 1.5351638604048917e-05 loss: 2.11313796043396\n",
      "lr: 1.5234437667510115e-05 loss: 2.079392433166504\n",
      "lr: 1.5117222413285762e-05 loss: 2.5158443450927734\n",
      "lr: 1.5e-05 loss: 2.1724960803985596\n",
      "lr: 1.4882777586714254e-05 loss: 2.104306697845459\n",
      "lr: 1.4765562332489879e-05 loss: 2.03843092918396\n",
      "lr: 1.4648361395951088e-05 loss: 2.0526037216186523\n",
      "lr: 1.453118193484764e-05 loss: 1.772128701210022\n",
      "lr: 1.4414031105617723e-05 loss: 1.7104392051696777\n",
      "lr: 1.4296916062950898e-05 loss: 1.9053449630737305\n",
      "lr: 1.4179843959351207e-05 loss: 2.4754979610443115\n",
      "lr: 1.4062821944700265e-05 loss: 2.0161631107330322\n",
      "lr: 1.3945857165820612e-05 loss: 2.251940965652466\n",
      "lr: 1.3828956766039278e-05 loss: 1.751309871673584\n",
      "lr: 1.371212788475151e-05 loss: 1.6622234582901\n",
      "lr: 1.3595377656984737e-05 loss: 1.8342458009719849\n",
      "lr: 1.3478713212962832e-05 loss: 2.117121696472168\n",
      "lr: 1.3362141677670601e-05 loss: 1.9565951824188232\n",
      "lr: 1.3245670170418764e-05 loss: 2.347297191619873\n",
      "lr: 1.312930580440901e-05 loss: 1.8385303020477295\n",
      "lr: 1.301305568629969e-05 loss: 1.8704031705856323\n",
      "lr: 1.289692691577171e-05 loss: 1.7600116729736328\n",
      "lr: 1.2780926585095053e-05 loss: 2.153559923171997\n",
      "lr: 1.2665061778695496e-05 loss: 2.25180721282959\n",
      "lr: 1.2549339572722044e-05 loss: 1.8489376306533813\n",
      "lr: 1.2433767034614756e-05 loss: 2.19732666015625\n",
      "lr: 1.2318351222673054e-05 loss: 2.2151713371276855\n",
      "lr: 1.2203099185624786e-05 loss: 1.941233515739441\n",
      "lr: 1.2088017962195586e-05 loss: 2.325434684753418\n",
      "lr: 1.197311458067914e-05 loss: 1.9573876857757568\n",
      "lr: 1.185839605850783e-05 loss: 1.9655516147613525\n",
      "lr: 1.1743869401824302e-05 loss: 2.220890998840332\n",
      "lr: 1.1629541605053423e-05 loss: 2.2230355739593506\n",
      "lr: 1.1515419650475254e-05 loss: 1.9111212491989136\n",
      "lr: 1.1401510507798532e-05 loss: 2.275698661804199\n",
      "lr: 1.1287821133735067e-05 loss: 2.10394549369812\n",
      "lr: 1.1174358471574857e-05 loss: 1.5324110984802246\n",
      "lr: 1.1061129450762028e-05 loss: 1.6443095207214355\n",
      "lr: 1.0948140986471659e-05 loss: 1.829089641571045\n",
      "lr: 1.083539997918749e-05 loss: 1.8039106130599976\n",
      "lr: 1.0722913314280398e-05 loss: 1.9290519952774048\n",
      "lr: 1.061068786158796e-05 loss: 2.045564889907837\n",
      "lr: 1.0498730474994883e-05 loss: 2.0951755046844482\n",
      "lr: 1.0387047992014452e-05 loss: 2.056053638458252\n",
      "lr: 1.0275647233370842e-05 loss: 2.2007315158843994\n",
      "lr: 1.0164535002582701e-05 loss: 1.6288127899169922\n",
      "lr: 1.0053718085547496e-05 loss: 2.055506467819214\n",
      "lr: 9.943203250127233e-06 loss: 1.8738517761230469\n",
      "lr: 9.832997245735008e-06 loss: 1.9245524406433105\n",
      "lr: 9.723106802922844e-06 loss: 1.62992525100708\n",
      "lr: 9.61353863297063e-06 loss: 1.7457711696624756\n",
      "lr: 9.504299427476304e-06 loss: 2.158937692642212\n",
      "lr: 9.395395857947107e-06 loss: 1.591139316558838\n",
      "lr: 9.286834575392148e-06 loss: 2.466325044631958\n",
      "lr: 9.178622209916263e-06 loss: 2.226925849914551\n",
      "lr: 9.070765370315017e-06 loss: 1.878441572189331\n",
      "lr: 8.96327064367119e-06 loss: 2.231152057647705\n",
      "lr: 8.856144594952408e-06 loss: 1.5207124948501587\n",
      "lr: 8.749393766610214e-06 loss: 1.747797966003418\n",
      "lr: 8.643024678180503e-06 loss: 2.171715259552002\n",
      "lr: 8.537043825885446e-06 loss: 1.8570027351379395\n",
      "lr: 8.43145768223661e-06 loss: 2.1383423805236816\n",
      "lr: 8.326272695639759e-06 loss: 2.2637195587158203\n",
      "lr: 8.221495290000995e-06 loss: 2.2371973991394043\n",
      "lr: 8.117131864334465e-06 loss: 1.9129194021224976\n",
      "lr: 8.013188792371528e-06 loss: 2.5331404209136963\n",
      "lr: 7.909672422171526e-06 loss: 1.8564627170562744\n",
      "lr: 7.806589075734026e-06 loss: 1.9510140419006348\n",
      "lr: 7.703945048612838e-06 loss: 2.088330030441284\n",
      "lr: 7.601746609531392e-06 loss: 1.522264003753662\n",
      "lr: 7.50000000000001e-06 loss: 1.7527475357055664\n",
      "lr: 7.398711433934615e-06 loss: 1.6597105264663696\n",
      "lr: 7.297887097277373e-06 loss: 1.5268077850341797\n",
      "lr: 7.19753314761873e-06 loss: 2.2793128490448\n",
      "lr: 7.097655713821491e-06 loss: 1.4313840866088867\n",
      "lr: 6.998260895646444e-06 loss: 1.9684994220733643\n",
      "lr: 6.899354763379859e-06 loss: 1.533909797668457\n",
      "lr: 6.800943357462768e-06 loss: 2.020960807800293\n",
      "lr: 6.703032688122009e-06 loss: 2.3142757415771484\n",
      "lr: 6.6056287350032345e-06 loss: 2.0241305828094482\n",
      "lr: 6.508737446805703e-06 loss: 2.072970390319824\n",
      "lr: 6.412364740918924e-06 loss: 1.7345298528671265\n",
      "lr: 6.3165165030613335e-06 loss: 2.3259787559509277\n",
      "lr: 6.221198586920804e-06 loss: 2.0407004356384277\n",
      "lr: 6.126416813797164e-06 loss: 1.691810965538025\n",
      "lr: 6.032176972246664e-06 loss: 1.8759568929672241\n",
      "lr: 5.938484817728471e-06 loss: 1.6869395971298218\n",
      "lr: 5.8453460722531206e-06 loss: 2.0336391925811768\n",
      "lr: 5.7527664240331245e-06 loss: 1.5431292057037354\n",
      "lr: 5.660751527135566e-06 loss: 2.5268309116363525\n",
      "lr: 5.569307001136735e-06 loss: 1.7663129568099976\n",
      "lr: 5.478438430778973e-06 loss: 2.21859073638916\n",
      "lr: 5.388151365629606e-06 loss: 1.6308062076568604\n",
      "lr: 5.29845131974202e-06 loss: 1.6565017700195312\n",
      "lr: 5.209343771318858e-06 loss: 2.0387916564941406\n",
      "lr: 5.120834162377498e-06 loss: 2.000542402267456\n",
      "lr: 5.032927898417671e-06 loss: 1.8872102499008179\n",
      "lr: 4.945630348091369e-06 loss: 1.940972924232483\n",
      "lr: 4.858946842874933e-06 loss: 1.5886707305908203\n",
      "lr: 4.7728826767434424e-06 loss: 2.172396659851074\n",
      "lr: 4.687443105847413e-06 loss: 1.8372400999069214\n",
      "lr: 4.602633348191818e-06 loss: 1.7059370279312134\n",
      "lr: 4.51845858331738e-06 loss: 2.1631906032562256\n",
      "lr: 4.434923951984218e-06 loss: 2.0648796558380127\n",
      "lr: 4.3520345558579505e-06 loss: 1.8260691165924072\n",
      "lr: 4.2697954571980855e-06 loss: 2.054701805114746\n",
      "lr: 4.188211678548863e-06 loss: 2.0053775310516357\n",
      "lr: 4.107288202432515e-06 loss: 1.9906765222549438\n",
      "lr: 4.027029971044973e-06 loss: 1.8424733877182007\n",
      "lr: 3.94744188595402e-06 loss: 1.9808294773101807\n",
      "lr: 3.868528807799984e-06 loss: 1.829872727394104\n",
      "lr: 3.790295555998849e-06 loss: 1.8585665225982666\n",
      "lr: 3.712746908447905e-06 loss: 2.146646022796631\n",
      "lr: 3.6358876012340013e-06 loss: 2.287733554840088\n",
      "lr: 3.5597223283442724e-06 loss: 1.744099497795105\n",
      "lr: 3.4842557413794548e-06 loss: 1.891939401626587\n",
      "lr: 3.409492449269835e-06 loss: 1.993011713027954\n",
      "lr: 3.3354370179937155e-06 loss: 1.7508502006530762\n",
      "lr: 3.26209397029865e-06 loss: 2.0043585300445557\n",
      "lr: 3.189467785425119e-06 loss: 1.8296160697937012\n",
      "lr: 3.1175628988330653e-06 loss: 1.677677869796753\n",
      "lr: 3.0463837019309293e-06 loss: 1.7979729175567627\n",
      "lr: 2.975934541807544e-06 loss: 1.976457118988037\n",
      "lr: 2.906219720966554e-06 loss: 2.0268852710723877\n",
      "lr: 2.8372434970637247e-06 loss: 1.9751570224761963\n",
      "lr: 2.7690100826468757e-06 loss: 1.943193793296814\n",
      "lr: 2.7015236448986313e-06 loss: 1.615459680557251\n",
      "lr: 2.6347883053818996e-06 loss: 1.71738600730896\n",
      "lr: 2.5688081397881793e-06 loss: 1.7957414388656616\n",
      "lr: 2.5035871776886204e-06 loss: 1.900274634361267\n",
      "lr: 2.439129402287991e-06 loss: 1.9276769161224365\n",
      "lr: 2.3754387501813196e-06 loss: 2.260991096496582\n",
      "lr: 2.3125191111135417e-06 loss: 2.0962700843811035\n",
      "lr: 2.2503743277419338e-06 loss: 2.1254687309265137\n",
      "lr: 2.1890081954014397e-06 loss: 2.2075040340423584\n",
      "lr: 2.1284244618728356e-06 loss: 2.1868467330932617\n",
      "lr: 2.0686268271539076e-06 loss: 1.9985939264297485\n",
      "lr: 2.009618943233423e-06 loss: 1.7184220552444458\n",
      "lr: 1.9514044138681434e-06 loss: 1.4053833484649658\n",
      "lr: 1.8939867943627177e-06 loss: 2.128021717071533\n",
      "lr: 1.8373695913525352e-06 loss: 1.6594030857086182\n",
      "lr: 1.781556262589576e-06 loss: 1.873623251914978\n",
      "lr: 1.726550216731248e-06 loss: 2.04152774810791\n",
      "lr: 1.6723548131322176e-06 loss: 1.7343848943710327\n",
      "lr: 1.6189733616392027e-06 loss: 2.1321606636047363\n",
      "lr: 1.5664091223888927e-06 loss: 2.0548274517059326\n",
      "lr: 1.5146653056087811e-06 loss: 1.830937385559082\n",
      "lr: 1.463745071421172e-06 loss: 1.675115704536438\n",
      "lr: 1.4136515296501452e-06 loss: 1.9924874305725098\n",
      "lr: 1.36438773963162e-06 loss: 2.065863847732544\n",
      "lr: 1.3159567100265474e-06 loss: 1.6446337699890137\n",
      "lr: 1.2683613986371585e-06 loss: 1.946851134300232\n",
      "lr: 1.2216047122263052e-06 loss: 1.6780986785888672\n",
      "lr: 1.1756895063399392e-06 loss: 2.3415987491607666\n",
      "lr: 1.1306185851327404e-06 loss: 1.8969902992248535\n",
      "lr: 1.086394701196835e-06 loss: 1.5314998626708984\n",
      "lr: 1.043020555393699e-06 loss: 1.8087280988693237\n",
      "lr: 1.000498796689217e-06 loss: 1.9997345209121704\n",
      "lr: 9.588320219918729e-07 loss: 1.8870775699615479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 9.180227759942045e-07 loss: 2.0019166469573975\n",
      "lr: 8.780735510173316e-07 loss: 2.4437618255615234\n",
      "lr: 8.389867868587847e-07 loss: 2.142629861831665\n",
      "lr: 8.0076487064349e-07 loss: 2.25730562210083\n",
      "lr: 7.634101366779755e-07 loss: 1.7647266387939453\n",
      "lr: 7.269248663078187e-07 loss: 1.7709234952926636\n",
      "lr: 6.913112877783146e-07 loss: 1.7446008920669556\n",
      "lr: 6.565715760983892e-07 loss: 2.2036612033843994\n",
      "lr: 6.227078529077634e-07 loss: 2.3322224617004395\n",
      "lr: 5.89722186347399e-07 loss: 1.7101751565933228\n",
      "lr: 5.576165909331582e-07 loss: 1.600913643836975\n",
      "lr: 5.263930274328077e-07 loss: 2.2469913959503174\n",
      "lr: 4.960534027462387e-07 loss: 2.0486159324645996\n",
      "lr: 4.6659956978904405e-07 loss: 1.8746590614318848\n",
      "lr: 4.38033327379313e-07 loss: 1.9268059730529785\n",
      "lr: 4.1035642012780484e-07 loss: 1.5947704315185547\n",
      "lr: 3.8357053833139034e-07 loss: 1.7821272611618042\n",
      "lr: 3.576773178698195e-07 loss: 1.755131483078003\n",
      "lr: 3.3267834010582475e-07 loss: 1.8555731773376465\n",
      "lr: 3.085751317885199e-07 loss: 2.039821147918701\n",
      "lr: 2.853691649601864e-07 loss: 1.6737438440322876\n",
      "lr: 2.630618568663584e-07 loss: 2.1800692081451416\n",
      "lr: 2.4165456986926073e-07 loss: 1.968151569366455\n",
      "lr: 2.2114861136461662e-07 loss: 2.1519997119903564\n",
      "lr: 2.0154523370180033e-07 loss: 2.1202800273895264\n",
      "lr: 1.8284563410735978e-07 loss: 2.292341947555542\n",
      "lr: 1.6505095461187337e-07 loss: 1.7379511594772339\n",
      "lr: 1.4816228198023585e-07 loss: 1.525359869003296\n",
      "lr: 1.3218064764526472e-07 loss: 2.0592856407165527\n",
      "lr: 1.1710702764471893e-07 loss: 1.6819878816604614\n",
      "lr: 1.0294234256168821e-07 loss: 1.7721318006515503\n",
      "lr: 8.968745746836149e-08 loss: 1.577742576599121\n",
      "lr: 7.734318187320566e-08 loss: 2.033026695251465\n",
      "lr: 6.591026967152858e-08 loss: 1.8719482421875\n",
      "lr: 5.538941909942585e-08 loss: 2.113537549972534\n",
      "lr: 4.578127269114829e-08 loss: 1.9661678075790405\n",
      "lr: 3.7086417239848294e-08 loss: 1.8160103559494019\n",
      "lr: 2.9305383761760175e-08 loss: 1.9490902423858643\n",
      "lr: 2.2438647463756124e-08 loss: 1.8429197072982788\n",
      "lr: 1.6486627714329405e-08 loss: 1.416670560836792\n",
      "lr: 1.1449688017978188e-08 loss: 2.1149797439575195\n",
      "\n",
      "\n",
      "Eval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2506457f55994f349f6047dcde11a210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=201, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'step': 800, 'lr': 7.328135993011631e-09, 'tr_loss': 1.9463878750801087, 'eval_val_loss': 1.9291358053387695, 'eval_val_acc': 0.6133250311332503}\n",
      "lr: 7.328135993011631e-09 loss: 2.445767641067505\n",
      "lr: 4.1222233527615786e-09 loss: 1.94623601436615\n",
      "lr: 1.8321458902048616e-09 loss: 1.905029535293579\n",
      "lr: 4.580434660111932e-10 loss: 2.5703797340393066\n",
      "lr: 3e-05 loss: 1.9817432165145874\n",
      "lr: 2.999954195653399e-05 loss: 1.9042155742645264\n",
      "lr: 2.9998167854109796e-05 loss: 1.9069492816925049\n",
      "lr: 2.999587777664724e-05 loss: 1.3235276937484741\n",
      "lr: 2.9992671864006992e-05 loss: 1.8423658609390259\n",
      "lr: 2.9988550311982024e-05 loss: 2.256005048751831\n",
      "lr: 2.9983513372285672e-05 loss: 1.5807685852050781\n",
      "lr: 2.9977561352536243e-05 loss: 2.000577211380005\n",
      "lr: 2.997069461623824e-05 loss: 1.6932159662246704\n",
      "lr: 2.9962913582760155e-05 loss: 1.733465552330017\n",
      "lr: 2.9954218727308853e-05 loss: 1.7836902141571045\n",
      "lr: 2.9944610580900574e-05 loss: 2.0066723823547363\n",
      "lr: 2.9934089730328475e-05 loss: 1.9707152843475342\n",
      "lr: 2.9922656818126796e-05 loss: 1.9444063901901245\n",
      "lr: 2.991031254253164e-05 loss: 1.7322994470596313\n",
      "lr: 2.9897057657438315e-05 loss: 1.7144451141357422\n",
      "lr: 2.988289297235528e-05 loss: 1.8217887878417969\n",
      "lr: 2.9867819352354736e-05 loss: 2.303729295730591\n",
      "lr: 2.9851837718019766e-05 loss: 2.1833853721618652\n",
      "lr: 2.983494904538813e-05 loss: 1.7130892276763916\n",
      "lr: 2.9817154365892643e-05 loss: 1.6222463846206665\n",
      "lr: 2.97984547662982e-05 loss: 2.159411668777466\n",
      "lr: 2.9778851388635386e-05 loss: 2.031710386276245\n",
      "lr: 2.975834543013074e-05 loss: 1.9027016162872314\n",
      "lr: 2.9736938143133647e-05 loss: 1.7051570415496826\n",
      "lr: 2.971463083503981e-05 loss: 2.42818284034729\n",
      "lr: 2.9691424868211482e-05 loss: 1.874045968055725\n",
      "lr: 2.9667321659894178e-05 loss: 1.8666925430297852\n",
      "lr: 2.9642322682130184e-05 loss: 1.594178557395935\n",
      "lr: 2.961642946166861e-05 loss: 1.5887190103530884\n",
      "lr: 2.9589643579872193e-05 loss: 1.8684511184692383\n",
      "lr: 2.9561966672620687e-05 loss: 2.1226775646209717\n",
      "lr: 2.9533400430210955e-05 loss: 1.948729157447815\n",
      "lr: 2.9503946597253756e-05 loss: 1.6710011959075928\n",
      "lr: 2.9473606972567192e-05 loss: 2.053342580795288\n",
      "lr: 2.944238340906685e-05 loss: 1.899225115776062\n",
      "lr: 2.9410277813652608e-05 loss: 1.5085363388061523\n",
      "lr: 2.9377292147092234e-05 loss: 1.3685270547866821\n",
      "lr: 2.9343428423901614e-05 loss: 1.654499888420105\n",
      "lr: 2.9308688712221687e-05 loss: 1.9987339973449707\n",
      "lr: 2.927307513369218e-05 loss: 1.8729897737503052\n",
      "lr: 2.9236589863322025e-05 loss: 1.4466865062713623\n",
      "lr: 2.9199235129356513e-05 loss: 1.6967246532440186\n",
      "lr: 2.9161013213141214e-05 loss: 1.7604657411575317\n",
      "lr: 2.912192644898267e-05 loss: 1.9338293075561523\n",
      "lr: 2.90819772240058e-05 loss: 1.3091669082641602\n",
      "lr: 2.904116797800812e-05 loss: 1.4721450805664062\n",
      "lr: 2.899950120331079e-05 loss: 1.6948474645614624\n",
      "lr: 2.8956979444606303e-05 loss: 1.7887516021728516\n",
      "lr: 2.891360529880317e-05 loss: 1.6673905849456787\n",
      "lr: 2.886938141486726e-05 loss: 1.412535309791565\n",
      "lr: 2.882431049366006e-05 loss: 2.051408529281616\n",
      "lr: 2.8778395287773696e-05 loss: 1.8091843128204346\n",
      "lr: 2.8731638601362847e-05 loss: 1.3571158647537231\n",
      "lr: 2.868404328997345e-05 loss: 2.010420322418213\n",
      "lr: 2.8635612260368384e-05 loss: 1.7571135759353638\n",
      "lr: 2.8586348470349863e-05 loss: 1.7536356449127197\n",
      "lr: 2.853625492857883e-05 loss: 1.7816336154937744\n",
      "lr: 2.8485334694391223e-05 loss: 2.2172210216522217\n",
      "lr: 2.8433590877611112e-05 loss: 1.9370886087417603\n",
      "lr: 2.8381026638360796e-05 loss: 1.7761203050613403\n",
      "lr: 2.8327645186867783e-05 loss: 1.436600923538208\n",
      "lr: 2.8273449783268748e-05 loss: 1.8937861919403076\n",
      "lr: 2.8218443737410426e-05 loss: 1.6533987522125244\n",
      "lr: 2.8162630408647467e-05 loss: 1.9647254943847656\n",
      "lr: 2.8106013205637286e-05 loss: 2.2191150188446045\n",
      "lr: 2.8048595586131862e-05 loss: 1.7708157300949097\n",
      "lr: 2.7990381056766573e-05 loss: 2.060182809829712\n",
      "lr: 2.79313731728461e-05 loss: 1.7432310581207275\n",
      "lr: 2.7871575538127164e-05 loss: 1.887712001800537\n",
      "lr: 2.7810991804598566e-05 loss: 1.7494803667068481\n",
      "lr: 2.7749625672258064e-05 loss: 1.6003913879394531\n",
      "lr: 2.7687480888886462e-05 loss: 2.0145936012268066\n",
      "lr: 2.7624561249818683e-05 loss: 1.5374186038970947\n",
      "lr: 2.7560870597712018e-05 loss: 2.150832414627075\n",
      "lr: 2.749641282231137e-05 loss: 1.759209156036377\n",
      "lr: 2.743119186021183e-05 loss: 1.6419756412506104\n",
      "lr: 2.7365211694618107e-05 loss: 1.673154354095459\n",
      "lr: 2.7298476355101372e-05 loss: 2.196946859359741\n",
      "lr: 2.7230989917353125e-05 loss: 1.9555134773254395\n",
      "lr: 2.7162756502936278e-05 loss: 1.7924690246582031\n",
      "lr: 2.7093780279033446e-05 loss: 2.0649361610412598\n",
      "lr: 2.7024065458192463e-05 loss: 1.4208604097366333\n",
      "lr: 2.6953616298069067e-05 loss: 2.119197130203247\n",
      "lr: 2.6882437101166935e-05 loss: 1.5746325254440308\n",
      "lr: 2.6810532214574873e-05 loss: 2.043186902999878\n",
      "lr: 2.673790602970136e-05 loss: 1.8669631481170654\n",
      "lr: 2.6664562982006273e-05 loss: 1.9475347995758057\n",
      "lr: 2.6590507550730175e-05 loss: 1.4088557958602905\n",
      "lr: 2.651574425862055e-05 loss: 1.8850255012512207\n",
      "lr: 2.6440277671655733e-05 loss: 1.8056004047393799\n",
      "lr: 2.6364112398765998e-05 loss: 1.6455624103546143\n",
      "lr: 2.6287253091552094e-05 loss: 1.8390132188796997\n",
      "lr: 2.6209704444001154e-05 loss: 1.4523911476135254\n",
      "lr: 2.6131471192200025e-05 loss: 1.814998745918274\n",
      "lr: 2.605255811404599e-05 loss: 1.9837524890899658\n",
      "lr: 2.5972970028955027e-05 loss: 1.720766305923462\n",
      "lr: 2.5892711797567494e-05 loss: 1.7247799634933472\n",
      "lr: 2.581178832145114e-05 loss: 1.8530353307724\n",
      "lr: 2.5730204542801915e-05 loss: 2.041818380355835\n",
      "lr: 2.564796544414205e-05 loss: 1.898766279220581\n",
      "lr: 2.5565076048015785e-05 loss: 1.7692636251449585\n",
      "lr: 2.548154141668262e-05 loss: 1.1705509424209595\n",
      "lr: 2.5397366651808176e-05 loss: 1.9686771631240845\n",
      "lr: 2.5312556894152582e-05 loss: 1.5189563035964966\n",
      "lr: 2.522711732325656e-05 loss: 1.8450895547866821\n",
      "lr: 2.5141053157125076e-05 loss: 1.5374724864959717\n",
      "lr: 2.5054369651908632e-05 loss: 1.6905335187911987\n",
      "lr: 2.4967072101582314e-05 loss: 1.1020500659942627\n",
      "lr: 2.4879165837622514e-05 loss: 1.751673936843872\n",
      "lr: 2.4790656228681145e-05 loss: 1.8759833574295044\n",
      "lr: 2.470154868025798e-05 loss: 1.4802931547164917\n",
      "lr: 2.461184863437039e-05 loss: 1.8555195331573486\n",
      "lr: 2.452156156922103e-05 loss: 1.5925500392913818\n",
      "lr: 2.443069299886327e-05 loss: 1.9587013721466064\n",
      "lr: 2.4339248472864444e-05 loss: 1.4660966396331787\n",
      "lr: 2.424723357596686e-05 loss: 1.7170393466949463\n",
      "lr: 2.415465392774689e-05 loss: 1.789520502090454\n",
      "lr: 2.4061515182271537e-05 loss: 1.9995503425598145\n",
      "lr: 2.3967823027753333e-05 loss: 1.345430850982666\n",
      "lr: 2.3873583186202837e-05 loss: 1.6569700241088867\n",
      "lr: 2.3778801413079198e-05 loss: 2.0870401859283447\n",
      "lr: 2.368348349693867e-05 loss: 2.0248215198516846\n",
      "lr: 2.3587635259081077e-05 loss: 2.253700017929077\n",
      "lr: 2.349126255319431e-05 loss: 1.3439306020736694\n",
      "lr: 2.3394371264996753e-05 loss: 1.9486969709396362\n",
      "lr: 2.3296967311877982e-05 loss: 1.9187140464782715\n",
      "lr: 2.3199056642537245e-05 loss: 1.5298550128936768\n",
      "lr: 2.310064523662014e-05 loss: 1.4970173835754395\n",
      "lr: 2.3001739104353558e-05 loss: 1.614405870437622\n",
      "lr: 2.2902344286178514e-05 loss: 1.704452395439148\n",
      "lr: 2.280246685238127e-05 loss: 1.3244905471801758\n",
      "lr: 2.270211290272263e-05 loss: 1.6627908945083618\n",
      "lr: 2.2601288566065377e-05 loss: 1.854093074798584\n",
      "lr: 2.2499999999999998e-05 loss: 1.6099497079849243\n",
      "lr: 2.239825339046862e-05 loss: 1.5769860744476318\n",
      "lr: 2.229605495138718e-05 loss: 1.6113563776016235\n",
      "lr: 2.2193410924265958e-05 loss: 1.2268307209014893\n",
      "lr: 2.2090327577828487e-05 loss: 1.6014111042022705\n",
      "lr: 2.198681120762847e-05 loss: 1.672345519065857\n",
      "lr: 2.1882868135665536e-05 loss: 1.7883988618850708\n",
      "lr: 2.1778504709999008e-05 loss: 2.4308359622955322\n",
      "lr: 2.167372730436025e-05 loss: 1.6577248573303223\n",
      "lr: 2.156854231776339e-05 loss: 1.322615146636963\n",
      "lr: 2.146295617411455e-05 loss: 1.567366123199463\n",
      "lr: 2.1356975321819486e-05 loss: 1.848153829574585\n",
      "lr: 2.125060623338979e-05 loss: 1.7605377435684204\n",
      "lr: 2.1143855405047604e-05 loss: 1.7890764474868774\n",
      "lr: 2.103672935632881e-05 loss: 1.9413046836853027\n",
      "lr: 2.0929234629684983e-05 loss: 1.4398659467697144\n",
      "lr: 2.0821377790083748e-05 loss: 1.6582180261611938\n",
      "lr: 2.0713165424607853e-05 loss: 1.4030976295471191\n",
      "lr: 2.0604604142052894e-05 loss: 1.6205884218215942\n",
      "lr: 2.049570057252371e-05 loss: 1.5356993675231934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 2.0386461367029363e-05 loss: 1.7226585149765015\n",
      "lr: 2.027689319707716e-05 loss: 1.582245945930481\n",
      "lr: 2.0167002754265005e-05 loss: 1.5828804969787598\n",
      "lr: 2.005679674987277e-05 loss: 1.5434231758117676\n",
      "lr: 1.9946281914452505e-05 loss: 1.9564149379730225\n",
      "lr: 1.9835464997417313e-05 loss: 1.4157931804656982\n",
      "lr: 1.9724352766629157e-05 loss: 1.6475751399993896\n",
      "lr: 1.9612952007985554e-05 loss: 1.579906702041626\n",
      "lr: 1.9501269525005113e-05 loss: 1.483141303062439\n",
      "lr: 1.938931213841204e-05 loss: 1.6036076545715332\n",
      "lr: 1.9277086685719606e-05 loss: 1.9456888437271118\n",
      "lr: 1.916460002081252e-05 loss: 1.6984808444976807\n",
      "lr: 1.9051859013528323e-05 loss: 1.7289981842041016\n",
      "lr: 1.8938870549237963e-05 loss: 1.6166366338729858\n",
      "lr: 1.8825641528425153e-05 loss: 1.4063754081726074\n",
      "lr: 1.8712178866264935e-05 loss: 1.3733160495758057\n",
      "lr: 1.8598489492201472e-05 loss: 1.6373162269592285\n",
      "lr: 1.848458034952475e-05 loss: 2.349170684814453\n",
      "lr: 1.8370458394946576e-05 loss: 1.4723294973373413\n",
      "lr: 1.82561305981757e-05 loss: 1.4898185729980469\n",
      "lr: 1.8141603941492182e-05 loss: 1.7084381580352783\n",
      "lr: 1.8026885419320863e-05 loss: 1.723063588142395\n",
      "lr: 1.7911982037804427e-05 loss: 1.8419240713119507\n",
      "lr: 1.7796900814375225e-05 loss: 1.7199420928955078\n",
      "lr: 1.768164877732695e-05 loss: 1.4969308376312256\n",
      "lr: 1.7566232965385255e-05 loss: 1.9497390985488892\n",
      "lr: 1.7450660427277957e-05 loss: 1.9512104988098145\n",
      "lr: 1.7334938221304504e-05 loss: 1.5201963186264038\n",
      "lr: 1.721907341490495e-05 loss: 1.5444449186325073\n",
      "lr: 1.7103073084228284e-05 loss: 1.4203320741653442\n",
      "lr: 1.6986944313700314e-05 loss: 1.8478199243545532\n",
      "lr: 1.6870694195590985e-05 loss: 1.3778600692749023\n",
      "lr: 1.6754329829581252e-05 loss: 1.4749633073806763\n",
      "lr: 1.663785832232938e-05 loss: 1.4334198236465454\n",
      "lr: 1.6521286787037186e-05 loss: 2.042073965072632\n",
      "lr: 1.6404622343015268e-05 loss: 1.685060977935791\n",
      "lr: 1.6287872115248492e-05 loss: 1.6929761171340942\n",
      "lr: 1.6171043233960725e-05 loss: 1.5426338911056519\n",
      "lr: 1.6054142834179393e-05 loss: 1.8681678771972656\n",
      "lr: 1.593717805529974e-05 loss: 1.7546464204788208\n",
      "lr: 1.5820156040648804e-05 loss: 1.7196221351623535\n",
      "lr: 1.5703083937049096e-05 loss: 1.1429868936538696\n",
      "\n",
      "\n",
      "Eval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e09253abe26478db2055c9722f0aa2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=201, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'step': 1000, 'lr': 1.558596889438228e-05, 'tr_loss': 1.753317340016365, 'eval_val_loss': 1.657076544726073, 'eval_val_acc': 0.6706102117061021}\n",
      "lr: 1.558596889438228e-05 loss: 1.1052862405776978\n",
      "lr: 1.5468818065152374e-05 loss: 1.3753139972686768\n",
      "lr: 1.5351638604048917e-05 loss: 1.6101998090744019\n",
      "lr: 1.5234437667510125e-05 loss: 1.7813961505889893\n",
      "lr: 1.5117222413285762e-05 loss: 1.9557911157608032\n",
      "lr: 1.5e-05 loss: 1.3242790699005127\n",
      "lr: 1.488277758671424e-05 loss: 1.9400802850723267\n",
      "lr: 1.4765562332489879e-05 loss: 1.407860517501831\n",
      "lr: 1.4648361395951111e-05 loss: 2.0062034130096436\n",
      "lr: 1.4531181934847649e-05 loss: 1.4594990015029907\n",
      "lr: 1.44140311056177e-05 loss: 1.6171696186065674\n",
      "lr: 1.4296916062950888e-05 loss: 1.667248010635376\n",
      "lr: 1.4179843959351197e-05 loss: 1.669864535331726\n",
      "lr: 1.4062821944700265e-05 loss: 1.2722887992858887\n",
      "lr: 1.3945857165820612e-05 loss: 1.3222836256027222\n",
      "lr: 1.3828956766039278e-05 loss: 1.2336902618408203\n",
      "lr: 1.371212788475153e-05 loss: 1.1373406648635864\n",
      "lr: 1.3595377656984757e-05 loss: 1.6265686750411987\n",
      "lr: 1.3478713212962818e-05 loss: 1.6951849460601807\n",
      "lr: 1.3362141677670601e-05 loss: 1.8608167171478271\n",
      "lr: 1.3245670170418754e-05 loss: 1.923468828201294\n",
      "lr: 1.3129305804409018e-05 loss: 1.4806768894195557\n",
      "lr: 1.301305568629969e-05 loss: 1.7654974460601807\n",
      "lr: 1.289692691577172e-05 loss: 1.3126763105392456\n",
      "lr: 1.2780926585095053e-05 loss: 1.6437877416610718\n",
      "lr: 1.2665061778695496e-05 loss: 1.5638717412948608\n",
      "lr: 1.2549339572722044e-05 loss: 1.846445083618164\n",
      "lr: 1.2433767034614748e-05 loss: 1.8239191770553589\n",
      "lr: 1.2318351222673054e-05 loss: 1.4732933044433594\n",
      "lr: 1.2203099185624776e-05 loss: 1.9191304445266724\n",
      "lr: 1.2088017962195576e-05 loss: 1.2599115371704102\n",
      "lr: 1.197311458067914e-05 loss: 1.2326325178146362\n",
      "lr: 1.1858396058507843e-05 loss: 1.2813904285430908\n",
      "lr: 1.1743869401824302e-05 loss: 1.4564963579177856\n",
      "lr: 1.1629541605053447e-05 loss: 1.5092920064926147\n",
      "lr: 1.1515419650475234e-05 loss: 1.5114113092422485\n",
      "lr: 1.1401510507798532e-05 loss: 1.3491456508636475\n",
      "lr: 1.1287821133735067e-05 loss: 1.8152471780776978\n",
      "lr: 1.1174358471574849e-05 loss: 1.537239670753479\n",
      "lr: 1.1061129450762038e-05 loss: 1.4414429664611816\n",
      "lr: 1.0948140986471681e-05 loss: 1.5994995832443237\n",
      "lr: 1.08353999791875e-05 loss: 1.2760188579559326\n",
      "lr: 1.0722913314280398e-05 loss: 1.500128149986267\n",
      "lr: 1.0610687861587944e-05 loss: 1.364367961883545\n",
      "lr: 1.0498730474994891e-05 loss: 1.4011541604995728\n",
      "lr: 1.0387047992014452e-05 loss: 1.8524000644683838\n",
      "lr: 1.0275647233370842e-05 loss: 1.103814959526062\n",
      "lr: 1.0164535002582693e-05 loss: 1.506494402885437\n",
      "lr: 1.0053718085547496e-05 loss: 1.2332547903060913\n",
      "lr: 9.943203250127254e-06 loss: 1.642815113067627\n",
      "lr: 9.832997245735021e-06 loss: 1.1935420036315918\n",
      "lr: 9.723106802922825e-06 loss: 1.252974271774292\n",
      "lr: 9.61353863297062e-06 loss: 1.3976480960845947\n",
      "lr: 9.504299427476295e-06 loss: 1.5723029375076294\n",
      "lr: 9.395395857947107e-06 loss: 1.2494646310806274\n",
      "lr: 9.286834575392148e-06 loss: 1.6671956777572632\n",
      "lr: 9.178622209916253e-06 loss: 1.4194483757019043\n",
      "lr: 9.070765370315036e-06 loss: 1.0923458337783813\n",
      "lr: 8.963270643671213e-06 loss: 1.906030297279358\n",
      "lr: 8.856144594952398e-06 loss: 1.3931934833526611\n",
      "lr: 8.749393766610195e-06 loss: 1.8238160610198975\n",
      "lr: 8.643024678180496e-06 loss: 1.494491696357727\n",
      "lr: 8.53704382588545e-06 loss: 1.4850507974624634\n",
      "lr: 8.43145768223661e-06 loss: 1.5737249851226807\n",
      "lr: 8.326272695639759e-06 loss: 0.9407604932785034\n",
      "lr: 8.221495290000995e-06 loss: 1.3523675203323364\n",
      "lr: 8.117131864334465e-06 loss: 1.3748958110809326\n",
      "lr: 8.013188792371552e-06 loss: 1.343174934387207\n",
      "lr: 7.909672422171519e-06 loss: 1.2555454969406128\n",
      "lr: 7.806589075734026e-06 loss: 1.4866231679916382\n",
      "lr: 7.703945048612827e-06 loss: 1.37704336643219\n",
      "lr: 7.601746609531378e-06 loss: 1.8601315021514893\n",
      "lr: 7.50000000000001e-06 loss: 1.385830044746399\n",
      "lr: 7.398711433934627e-06 loss: 1.880774736404419\n",
      "lr: 7.297887097277373e-06 loss: 1.0771863460540771\n",
      "lr: 7.197533147618752e-06 loss: 1.4013336896896362\n",
      "lr: 7.097655713821475e-06 loss: 1.171547770500183\n",
      "lr: 6.998260895646444e-06 loss: 1.113874077796936\n",
      "lr: 6.899354763379859e-06 loss: 1.1358965635299683\n",
      "lr: 6.800943357462757e-06 loss: 1.1890147924423218\n",
      "lr: 6.70303268812202e-06 loss: 1.382832646369934\n",
      "lr: 6.605628735003251e-06 loss: 1.4095851182937622\n",
      "lr: 6.5087374468057085e-06 loss: 1.3107906579971313\n",
      "lr: 6.412364740918924e-06 loss: 1.304049015045166\n",
      "lr: 6.3165165030613165e-06 loss: 1.4152090549468994\n",
      "lr: 6.221198586920804e-06 loss: 1.8696833848953247\n",
      "lr: 6.126416813797164e-06 loss: 1.1410325765609741\n",
      "lr: 6.032176972246664e-06 loss: 1.6934072971343994\n",
      "lr: 5.9384848177284656e-06 loss: 1.6919413805007935\n",
      "lr: 5.8453460722531155e-06 loss: 1.8521627187728882\n",
      "lr: 5.752766424033141e-06 loss: 1.6570967435836792\n",
      "lr: 5.660751527135571e-06 loss: 1.424939751625061\n",
      "lr: 5.569307001136735e-06 loss: 1.317847490310669\n",
      "lr: 5.478438430778958e-06 loss: 2.0398964881896973\n",
      "lr: 5.388151365629597e-06 loss: 1.2523914575576782\n",
      "lr: 5.29845131974202e-06 loss: 1.807621717453003\n",
      "lr: 5.209343771318858e-06 loss: 1.5591583251953125\n",
      "lr: 5.120834162377493e-06 loss: 1.3048570156097412\n",
      "lr: 5.032927898417685e-06 loss: 1.257597804069519\n",
      "lr: 4.945630348091382e-06 loss: 1.7326672077178955\n",
      "lr: 4.8589468428749415e-06 loss: 1.7567217350006104\n",
      "lr: 4.772882676743427e-06 loss: 1.753601312637329\n",
      "lr: 4.6874431058474025e-06 loss: 1.6167880296707153\n",
      "lr: 4.602633348191823e-06 loss: 1.9778143167495728\n",
      "lr: 4.51845858331738e-06 loss: 1.4255114793777466\n",
      "lr: 4.434923951984218e-06 loss: 1.768133521080017\n",
      "lr: 4.3520345558579505e-06 loss: 1.6265133619308472\n",
      "lr: 4.2697954571980855e-06 loss: 1.3949434757232666\n",
      "lr: 4.1882116785488776e-06 loss: 1.2808650732040405\n",
      "lr: 4.10728820243251e-06 loss: 1.3675233125686646\n",
      "lr: 4.027029971044959e-06 loss: 1.3930195569992065\n",
      "lr: 3.94744188595401e-06 loss: 1.4265716075897217\n",
      "lr: 3.868528807799979e-06 loss: 1.511557698249817\n",
      "lr: 3.790295555998849e-06 loss: 1.4548940658569336\n",
      "lr: 3.712746908447905e-06 loss: 1.5843172073364258\n",
      "lr: 3.6358876012340013e-06 loss: 1.6615889072418213\n",
      "lr: 3.559722328344286e-06 loss: 1.5136088132858276\n",
      "lr: 3.484255741379468e-06 loss: 1.4015954732894897\n",
      "lr: 3.4094924492698263e-06 loss: 1.3487181663513184\n",
      "lr: 3.3354370179937155e-06 loss: 1.1164854764938354\n",
      "lr: 3.2620939702986465e-06 loss: 1.2479379177093506\n",
      "lr: 3.1894677854251276e-06 loss: 1.66628098487854\n",
      "lr: 3.1175628988330653e-06 loss: 1.6013833284378052\n",
      "lr: 3.0463837019309326e-06 loss: 1.6943897008895874\n",
      "lr: 2.975934541807544e-06 loss: 1.3393865823745728\n",
      "lr: 2.906219720966554e-06 loss: 1.1912598609924316\n",
      "lr: 2.8372434970637247e-06 loss: 1.3045670986175537\n",
      "lr: 2.7690100826468757e-06 loss: 1.3138622045516968\n",
      "lr: 2.7015236448986313e-06 loss: 1.3130688667297363\n",
      "lr: 2.6347883053818962e-06 loss: 1.608324408531189\n",
      "lr: 2.5688081397881708e-06 loss: 1.3710697889328003\n",
      "lr: 2.5035871776886323e-06 loss: 1.6137690544128418\n",
      "lr: 2.4391294022879978e-06 loss: 1.3234540224075317\n",
      "lr: 2.3754387501813196e-06 loss: 1.3705998659133911\n",
      "lr: 2.312519111113532e-06 loss: 1.5961357355117798\n",
      "lr: 2.250374327741927e-06 loss: 1.2377171516418457\n",
      "lr: 2.1890081954014397e-06 loss: 1.349735975265503\n",
      "lr: 2.1284244618728356e-06 loss: 1.2100868225097656\n",
      "lr: 2.0686268271539012e-06 loss: 1.4689044952392578\n",
      "lr: 2.009618943233426e-06 loss: 1.6155825853347778\n",
      "lr: 1.9514044138681536e-06 loss: 1.5611718893051147\n",
      "lr: 1.8939867943627242e-06 loss: 1.6123046875\n",
      "lr: 1.8373695913525352e-06 loss: 1.617200493812561\n",
      "lr: 1.781556262589566e-06 loss: 1.4746967554092407\n",
      "lr: 1.726550216731253e-06 loss: 1.1767828464508057\n",
      "lr: 1.6723548131322176e-06 loss: 1.415622353553772\n",
      "lr: 1.6189733616392027e-06 loss: 1.4524133205413818\n",
      "lr: 1.566409122388886e-06 loss: 1.2265164852142334\n",
      "lr: 1.5146653056087811e-06 loss: 1.9710966348648071\n",
      "lr: 1.463745071421182e-06 loss: 1.3450038433074951\n",
      "lr: 1.4136515296501469e-06 loss: 1.1019108295440674\n",
      "lr: 1.3643877396316118e-06 loss: 1.695995807647705\n",
      "lr: 1.3159567100265425e-06 loss: 1.4136842489242554\n",
      "lr: 1.268361398637155e-06 loss: 1.488500714302063\n",
      "lr: 1.2216047122263052e-06 loss: 1.4351587295532227\n",
      "lr: 1.1756895063399392e-06 loss: 1.4769413471221924\n",
      "lr: 1.1306185851327404e-06 loss: 1.4686052799224854\n",
      "lr: 1.0863947011968416e-06 loss: 1.366025447845459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 1.0430205553937072e-06 loss: 1.6889115571975708\n",
      "lr: 1.0004987966892121e-06 loss: 0.9739058017730713\n",
      "lr: 9.588320219918729e-07 loss: 1.0533488988876343\n",
      "lr: 9.180227759942012e-07 loss: 1.5810863971710205\n",
      "lr: 8.780735510173332e-07 loss: 1.7274752855300903\n",
      "lr: 8.389867868587847e-07 loss: 1.6269550323486328\n",
      "lr: 8.0076487064349e-07 loss: 1.8814901113510132\n",
      "lr: 7.634101366779755e-07 loss: 1.5058834552764893\n",
      "lr: 7.269248663078187e-07 loss: 1.3413608074188232\n",
      "lr: 6.913112877783146e-07 loss: 1.413533091545105\n",
      "lr: 6.565715760983875e-07 loss: 1.765961766242981\n",
      "lr: 6.227078529077634e-07 loss: 1.6814017295837402\n",
      "lr: 5.897221863473956e-07 loss: 1.246124267578125\n",
      "lr: 5.576165909331532e-07 loss: 1.5007271766662598\n",
      "lr: 5.263930274328077e-07 loss: 0.9956753253936768\n",
      "lr: 4.960534027462421e-07 loss: 1.5795514583587646\n",
      "lr: 4.6659956978904405e-07 loss: 1.1649224758148193\n",
      "lr: 4.38033327379318e-07 loss: 1.5482925176620483\n",
      "lr: 4.103564201277998e-07 loss: 1.8759106397628784\n",
      "lr: 3.8357053833139034e-07 loss: 1.0041700601577759\n",
      "lr: 3.576773178698195e-07 loss: 1.650506615638733\n",
      "lr: 3.326783401058214e-07 loss: 1.78753662109375\n",
      "lr: 3.0857513178852324e-07 loss: 1.1372456550598145\n",
      "lr: 2.853691649601897e-07 loss: 0.9129154682159424\n",
      "lr: 2.630618568663601e-07 loss: 0.9857602715492249\n",
      "lr: 2.4165456986926073e-07 loss: 1.3358510732650757\n",
      "lr: 2.2114861136461328e-07 loss: 1.689143419265747\n",
      "lr: 2.0154523370180366e-07 loss: 1.372606873512268\n",
      "lr: 1.8284563410735978e-07 loss: 1.4827386140823364\n",
      "lr: 1.6505095461187337e-07 loss: 1.3661998510360718\n",
      "lr: 1.4816228198023419e-07 loss: 1.2500625848770142\n",
      "lr: 1.3218064764526472e-07 loss: 1.5625756978988647\n",
      "lr: 1.1710702764472225e-07 loss: 1.1933962106704712\n",
      "lr: 1.0294234256168821e-07 loss: 1.552381992340088\n",
      "lr: 8.968745746835816e-08 loss: 1.500739336013794\n",
      "lr: 7.734318187320399e-08 loss: 1.6828693151474\n",
      "lr: 6.591026967152691e-08 loss: 1.1502888202667236\n",
      "lr: 5.538941909942585e-08 loss: 1.470483660697937\n",
      "lr: 4.578127269114829e-08 loss: 1.6334220170974731\n",
      "lr: 3.7086417239848294e-08 loss: 1.8434135913848877\n",
      "lr: 2.930538376176184e-08 loss: 1.2389582395553589\n",
      "lr: 2.2438647463757788e-08 loss: 1.4555087089538574\n",
      "\n",
      "\n",
      "Eval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d31e5f84d5465b856fdafe7f04d668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=201, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'step': 1200, 'lr': 1.6486627714329405e-08, 'tr_loss': 1.472582503259182, 'eval_val_loss': 1.5690955596776743, 'eval_val_acc': 0.6911581569115816}\n",
      "lr: 1.6486627714329405e-08 loss: 1.3507620096206665\n",
      "lr: 1.1449688017976522e-08 loss: 1.4017614126205444\n",
      "lr: 7.328135993011631e-09 loss: 1.677243709564209\n",
      "lr: 4.1222233527615786e-09 loss: 1.535178303718567\n",
      "lr: 1.8321458902048616e-09 loss: 1.3112167119979858\n",
      "lr: 4.580434660111932e-10 loss: 1.3888052701950073\n",
      "lr: 3e-05 loss: 1.2917571067810059\n",
      "lr: 2.999954195653399e-05 loss: 1.2426388263702393\n",
      "lr: 2.9998167854109796e-05 loss: 1.1890645027160645\n",
      "lr: 2.999587777664724e-05 loss: 1.0823265314102173\n",
      "lr: 2.9992671864006992e-05 loss: 1.2939746379852295\n",
      "lr: 2.9988550311982024e-05 loss: 1.343679428100586\n",
      "lr: 2.9983513372285672e-05 loss: 1.2301249504089355\n",
      "lr: 2.9977561352536243e-05 loss: 1.556870698928833\n",
      "lr: 2.997069461623824e-05 loss: 1.3590136766433716\n",
      "lr: 2.9962913582760155e-05 loss: 1.690641164779663\n",
      "lr: 2.9954218727308856e-05 loss: 1.3253902196884155\n",
      "lr: 2.9944610580900574e-05 loss: 2.0108535289764404\n",
      "lr: 2.9934089730328472e-05 loss: 1.2143689393997192\n",
      "lr: 2.9922656818126796e-05 loss: 1.2894870042800903\n",
      "lr: 2.991031254253164e-05 loss: 1.569204330444336\n",
      "lr: 2.9897057657438315e-05 loss: 1.3211123943328857\n",
      "lr: 2.988289297235528e-05 loss: 1.4228579998016357\n",
      "lr: 2.9867819352354736e-05 loss: 1.0674965381622314\n",
      "lr: 2.9851837718019766e-05 loss: 0.9688699245452881\n",
      "lr: 2.983494904538813e-05 loss: 1.197078824043274\n",
      "lr: 2.9817154365892643e-05 loss: 1.2359230518341064\n",
      "lr: 2.97984547662982e-05 loss: 1.1745773553848267\n",
      "lr: 2.9778851388635386e-05 loss: 1.3728257417678833\n",
      "lr: 2.975834543013074e-05 loss: 1.4709718227386475\n",
      "lr: 2.973693814313364e-05 loss: 1.8292144536972046\n",
      "lr: 2.971463083503982e-05 loss: 1.8763121366500854\n",
      "lr: 2.9691424868211482e-05 loss: 1.4815309047698975\n",
      "lr: 2.9667321659894178e-05 loss: 1.339057445526123\n",
      "lr: 2.9642322682130177e-05 loss: 1.9130531549453735\n",
      "lr: 2.961642946166861e-05 loss: 1.594765067100525\n",
      "lr: 2.9589643579872193e-05 loss: 1.2847371101379395\n",
      "lr: 2.9561966672620687e-05 loss: 1.6672223806381226\n",
      "lr: 2.9533400430210955e-05 loss: 1.653146505355835\n",
      "lr: 2.9503946597253763e-05 loss: 1.644534945487976\n",
      "lr: 2.94736069725672e-05 loss: 1.6997194290161133\n",
      "lr: 2.944238340906685e-05 loss: 1.1272786855697632\n",
      "lr: 2.94102778136526e-05 loss: 1.5972784757614136\n",
      "lr: 2.9377292147092234e-05 loss: 1.1259747743606567\n",
      "lr: 2.9343428423901614e-05 loss: 1.4575014114379883\n",
      "lr: 2.9308688712221687e-05 loss: 1.6473110914230347\n",
      "lr: 2.927307513369218e-05 loss: 1.2128715515136719\n",
      "lr: 2.9236589863322025e-05 loss: 1.124605655670166\n",
      "lr: 2.9199235129356513e-05 loss: 1.0302926301956177\n",
      "lr: 2.916101321314122e-05 loss: 1.646033525466919\n",
      "lr: 2.9121926448982673e-05 loss: 1.3351370096206665\n",
      "lr: 2.9081977224005792e-05 loss: 1.0555047988891602\n",
      "lr: 2.904116797800812e-05 loss: 1.2845674753189087\n",
      "lr: 2.899950120331078e-05 loss: 1.751179814338684\n",
      "lr: 2.8956979444606303e-05 loss: 1.3302879333496094\n",
      "lr: 2.891360529880317e-05 loss: 1.508581280708313\n",
      "lr: 2.886938141486726e-05 loss: 1.4317842721939087\n",
      "lr: 2.882431049366007e-05 loss: 1.3246450424194336\n",
      "lr: 2.8778395287773703e-05 loss: 1.653163194656372\n",
      "lr: 2.8731638601362837e-05 loss: 1.478232502937317\n",
      "lr: 2.868404328997345e-05 loss: 1.437312126159668\n",
      "lr: 2.8635612260368384e-05 loss: 1.7330827713012695\n",
      "lr: 2.8586348470349863e-05 loss: 1.2843401432037354\n",
      "lr: 2.853625492857883e-05 loss: 1.134691596031189\n",
      "lr: 2.8485334694391223e-05 loss: 1.81464421749115\n",
      "lr: 2.8433590877611112e-05 loss: 1.5399199724197388\n",
      "lr: 2.8381026638360796e-05 loss: 1.6240882873535156\n",
      "lr: 2.8327645186867783e-05 loss: 1.5799436569213867\n",
      "lr: 2.8273449783268748e-05 loss: 1.4881950616836548\n",
      "lr: 2.8218443737410426e-05 loss: 1.5734424591064453\n",
      "lr: 2.8162630408647467e-05 loss: 0.7490465641021729\n",
      "lr: 2.810601320563728e-05 loss: 1.2151210308074951\n",
      "lr: 2.8048595586131862e-05 loss: 1.4676545858383179\n",
      "lr: 2.7990381056766583e-05 loss: 1.312929630279541\n",
      "lr: 2.79313731728461e-05 loss: 1.5655999183654785\n",
      "lr: 2.7871575538127154e-05 loss: 1.4019112586975098\n",
      "lr: 2.7810991804598552e-05 loss: 0.8651439547538757\n",
      "lr: 2.7749625672258064e-05 loss: 1.3896244764328003\n",
      "lr: 2.7687480888886462e-05 loss: 1.3250383138656616\n",
      "lr: 2.7624561249818683e-05 loss: 1.1403250694274902\n",
      "lr: 2.7560870597712018e-05 loss: 1.2390961647033691\n",
      "lr: 2.749641282231138e-05 loss: 1.2011512517929077\n",
      "lr: 2.743119186021183e-05 loss: 1.0736801624298096\n",
      "lr: 2.7365211694618107e-05 loss: 1.4814242124557495\n",
      "lr: 2.7298476355101362e-05 loss: 1.0207946300506592\n",
      "lr: 2.7230989917353125e-05 loss: 1.9906054735183716\n",
      "lr: 2.7162756502936278e-05 loss: 1.2001129388809204\n",
      "lr: 2.7093780279033446e-05 loss: 1.2084474563598633\n",
      "lr: 2.7024065458192463e-05 loss: 1.6344631910324097\n",
      "lr: 2.6953616298069067e-05 loss: 1.4117106199264526\n",
      "lr: 2.688243710116695e-05 loss: 1.341405987739563\n",
      "lr: 2.681053221457489e-05 loss: 1.6634262800216675\n",
      "lr: 2.6737906029701346e-05 loss: 1.1519653797149658\n",
      "lr: 2.6664562982006273e-05 loss: 1.1954854726791382\n",
      "lr: 2.659050755073016e-05 loss: 1.2364544868469238\n",
      "lr: 2.651574425862055e-05 loss: 1.2800298929214478\n",
      "lr: 2.6440277671655733e-05 loss: 0.9651089310646057\n",
      "lr: 2.6364112398765998e-05 loss: 0.967583417892456\n",
      "lr: 2.628725309155211e-05 loss: 1.3695236444473267\n",
      "lr: 2.620970444400117e-05 loss: 1.151809573173523\n",
      "lr: 2.6131471192200012e-05 loss: 1.3437318801879883\n",
      "lr: 2.6052558114045973e-05 loss: 1.6737141609191895\n",
      "lr: 2.5972970028955027e-05 loss: 1.0929409265518188\n",
      "lr: 2.5892711797567494e-05 loss: 1.380368709564209\n",
      "lr: 2.581178832145114e-05 loss: 1.1160924434661865\n",
      "lr: 2.5730204542801915e-05 loss: 1.3229224681854248\n",
      "lr: 2.564796544414205e-05 loss: 1.0532978773117065\n",
      "lr: 2.5565076048015785e-05 loss: 1.7029144763946533\n",
      "lr: 2.5481541416682638e-05 loss: 1.4945838451385498\n",
      "lr: 2.5397366651808176e-05 loss: 1.1265473365783691\n",
      "lr: 2.5312556894152582e-05 loss: 1.4915390014648438\n",
      "lr: 2.522711732325656e-05 loss: 1.3583704233169556\n",
      "lr: 2.5141053157125063e-05 loss: 0.9241330623626709\n",
      "lr: 2.5054369651908632e-05 loss: 1.6173062324523926\n",
      "lr: 2.4967072101582334e-05 loss: 1.5288687944412231\n",
      "lr: 2.4879165837622514e-05 loss: 1.5196409225463867\n",
      "lr: 2.4790656228681165e-05 loss: 1.373472809791565\n",
      "lr: 2.4701548680257966e-05 loss: 0.9031091928482056\n",
      "lr: 2.461184863437039e-05 loss: 1.3617699146270752\n",
      "lr: 2.452156156922103e-05 loss: 1.1162452697753906\n",
      "lr: 2.443069299886327e-05 loss: 1.4400196075439453\n",
      "lr: 2.4339248472864444e-05 loss: 1.0663014650344849\n",
      "lr: 2.4247233575966877e-05 loss: 1.4622431993484497\n",
      "lr: 2.415465392774689e-05 loss: 1.1857635974884033\n",
      "lr: 2.4061515182271537e-05 loss: 1.4628510475158691\n",
      "lr: 2.3967823027753323e-05 loss: 1.1228593587875366\n",
      "lr: 2.3873583186202837e-05 loss: 1.3851048946380615\n",
      "lr: 2.3778801413079198e-05 loss: 1.405953288078308\n",
      "lr: 2.368348349693867e-05 loss: 1.4574615955352783\n",
      "lr: 2.3587635259081077e-05 loss: 1.1514946222305298\n",
      "lr: 2.349126255319429e-05 loss: 1.6970853805541992\n",
      "lr: 2.3394371264996766e-05 loss: 1.312575340270996\n",
      "lr: 2.3296967311878003e-05 loss: 1.0863754749298096\n",
      "lr: 2.3199056642537245e-05 loss: 1.23903226852417\n",
      "lr: 2.310064523662012e-05 loss: 1.1069172620773315\n",
      "lr: 2.300173910435354e-05 loss: 1.4808495044708252\n",
      "lr: 2.2902344286178514e-05 loss: 1.8200523853302002\n",
      "lr: 2.280246685238127e-05 loss: 1.330766201019287\n",
      "lr: 2.270211290272263e-05 loss: 1.4168521165847778\n",
      "lr: 2.2601288566065394e-05 loss: 1.1814645528793335\n",
      "lr: 2.2500000000000015e-05 loss: 1.4646204710006714\n",
      "lr: 2.239825339046862e-05 loss: 1.2628742456436157\n",
      "lr: 2.2296054951387158e-05 loss: 1.4236083030700684\n",
      "lr: 2.2193410924265958e-05 loss: 1.1028611660003662\n",
      "lr: 2.2090327577828487e-05 loss: 1.3776204586029053\n",
      "lr: 2.198681120762847e-05 loss: 1.0630011558532715\n",
      "lr: 2.1882868135665536e-05 loss: 1.3159006834030151\n",
      "lr: 2.1778504709999008e-05 loss: 1.4416545629501343\n",
      "lr: 2.167372730436025e-05 loss: 1.6931958198547363\n",
      "lr: 2.1568542317763407e-05 loss: 2.008538007736206\n",
      "lr: 2.146295617411455e-05 loss: 1.2754403352737427\n",
      "lr: 2.1356975321819486e-05 loss: 1.4404128789901733\n",
      "lr: 2.125060623338979e-05 loss: 1.4019049406051636\n",
      "lr: 2.1143855405047584e-05 loss: 1.121048927307129\n",
      "lr: 2.103672935632881e-05 loss: 1.3677456378936768\n",
      "lr: 2.0929234629684983e-05 loss: 1.1466269493103027\n",
      "lr: 2.0821377790083748e-05 loss: 1.546329140663147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 2.0713165424607873e-05 loss: 1.6137975454330444\n",
      "lr: 2.0604604142052874e-05 loss: 1.8956928253173828\n",
      "lr: 2.049570057252369e-05 loss: 0.9930052161216736\n",
      "lr: 2.0386461367029363e-05 loss: 1.0075912475585938\n",
      "lr: 2.027689319707716e-05 loss: 1.092515230178833\n",
      "lr: 2.0167002754265005e-05 loss: 1.427863359451294\n",
      "lr: 2.005679674987277e-05 loss: 1.5780080556869507\n",
      "lr: 1.9946281914452505e-05 loss: 1.3777259588241577\n",
      "lr: 1.9835464997417313e-05 loss: 1.0001018047332764\n",
      "lr: 1.9724352766629157e-05 loss: 1.6249068975448608\n",
      "lr: 1.9612952007985554e-05 loss: 1.4428383111953735\n",
      "lr: 1.9501269525005113e-05 loss: 1.098059892654419\n",
      "lr: 1.938931213841204e-05 loss: 0.8390958309173584\n",
      "lr: 1.9277086685719606e-05 loss: 1.2951819896697998\n",
      "lr: 1.91646000208125e-05 loss: 1.131016492843628\n",
      "lr: 1.9051859013528344e-05 loss: 1.2903237342834473\n",
      "lr: 1.8938870549237983e-05 loss: 0.918491005897522\n",
      "lr: 1.8825641528425153e-05 loss: 1.6218292713165283\n",
      "lr: 1.871217886626492e-05 loss: 1.112268328666687\n",
      "lr: 1.859848949220145e-05 loss: 1.446559190750122\n",
      "lr: 1.848458034952475e-05 loss: 0.9569506645202637\n",
      "lr: 1.8370458394946576e-05 loss: 1.1637548208236694\n",
      "lr: 1.82561305981757e-05 loss: 1.282888412475586\n",
      "lr: 1.8141603941492182e-05 loss: 1.0566819906234741\n",
      "lr: 1.8026885419320883e-05 loss: 1.3019267320632935\n",
      "lr: 1.7911982037804427e-05 loss: 1.0445284843444824\n",
      "lr: 1.7796900814375205e-05 loss: 1.0162416696548462\n",
      "lr: 1.7681648777326928e-05 loss: 0.9391316771507263\n",
      "lr: 1.7566232965385255e-05 loss: 1.2110323905944824\n",
      "lr: 1.7450660427277957e-05 loss: 0.8418504595756531\n",
      "lr: 1.7334938221304504e-05 loss: 1.2664189338684082\n",
      "lr: 1.721907341490495e-05 loss: 1.208213210105896\n",
      "lr: 1.7103073084228284e-05 loss: 1.0707589387893677\n",
      "lr: 1.6986944313700338e-05 loss: 1.2881885766983032\n",
      "lr: 1.6870694195591005e-05 loss: 1.3881840705871582\n",
      "lr: 1.675432982958123e-05 loss: 1.4356927871704102\n",
      "lr: 1.663785832232938e-05 loss: 1.3812273740768433\n",
      "lr: 1.6521286787037162e-05 loss: 1.0868710279464722\n",
      "lr: 1.6404622343015268e-05 loss: 1.138109803199768\n",
      "lr: 1.6287872115248492e-05 loss: 1.0451408624649048\n",
      "lr: 1.6171043233960725e-05 loss: 1.42546546459198\n",
      "lr: 1.6054142834179416e-05 loss: 1.3317877054214478\n",
      "lr: 1.593717805529976e-05 loss: 1.5190603733062744\n",
      "\n",
      "\n",
      "Eval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992091bbc2a243a69516611c9cababec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=201, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'step': 1400, 'lr': 1.5820156040648784e-05, 'tr_loss': 1.3404594537615777, 'eval_val_loss': 1.4281075025968883, 'eval_val_acc': 0.7110834371108343}\n",
      "lr: 1.5820156040648784e-05 loss: 1.461356282234192\n",
      "lr: 1.5703083937049096e-05 loss: 1.3282902240753174\n",
      "lr: 1.558596889438228e-05 loss: 0.7068631649017334\n",
      "lr: 1.5468818065152374e-05 loss: 1.170346736907959\n",
      "lr: 1.5351638604048917e-05 loss: 1.289129376411438\n",
      "lr: 1.5234437667510125e-05 loss: 1.16935133934021\n",
      "lr: 1.5117222413285762e-05 loss: 1.1232903003692627\n",
      "lr: 1.5e-05 loss: 1.2241171598434448\n",
      "lr: 1.488277758671424e-05 loss: 1.3161282539367676\n",
      "lr: 1.4765562332489879e-05 loss: 1.092039942741394\n",
      "lr: 1.4648361395951088e-05 loss: 1.3651349544525146\n",
      "lr: 1.453118193484763e-05 loss: 1.0846971273422241\n",
      "lr: 1.44140311056177e-05 loss: 1.2680110931396484\n",
      "lr: 1.4296916062950908e-05 loss: 0.8259886503219604\n",
      "lr: 1.417984395935122e-05 loss: 1.0394723415374756\n",
      "lr: 1.4062821944700265e-05 loss: 1.4929147958755493\n",
      "lr: 1.394585716582063e-05 loss: 1.0555217266082764\n",
      "lr: 1.3828956766039258e-05 loss: 1.1728636026382446\n",
      "lr: 1.371212788475151e-05 loss: 1.232412338256836\n",
      "lr: 1.3595377656984737e-05 loss: 1.1400915384292603\n",
      "lr: 1.3478713212962818e-05 loss: 1.2360857725143433\n",
      "lr: 1.3362141677670621e-05 loss: 0.8856210708618164\n",
      "lr: 1.3245670170418777e-05 loss: 1.2318305969238281\n",
      "lr: 1.3129305804409018e-05 loss: 0.9087162613868713\n",
      "lr: 1.301305568629969e-05 loss: 1.4312866926193237\n",
      "lr: 1.28969269157717e-05 loss: 1.0336482524871826\n",
      "lr: 1.2780926585095053e-05 loss: 1.1779723167419434\n",
      "lr: 1.2665061778695496e-05 loss: 0.9711151123046875\n",
      "lr: 1.2549339572722044e-05 loss: 1.4121968746185303\n",
      "lr: 1.2433767034614748e-05 loss: 1.0596461296081543\n",
      "lr: 1.2318351222673054e-05 loss: 1.1794018745422363\n",
      "lr: 1.2203099185624796e-05 loss: 1.03684663772583\n",
      "lr: 1.20880179621956e-05 loss: 1.399709701538086\n",
      "lr: 1.1973114580679121e-05 loss: 1.2800532579421997\n",
      "lr: 1.185839605850782e-05 loss: 1.2011609077453613\n",
      "lr: 1.1743869401824281e-05 loss: 1.1819816827774048\n",
      "lr: 1.1629541605053423e-05 loss: 1.2668229341506958\n",
      "lr: 1.1515419650475254e-05 loss: 1.4124319553375244\n",
      "lr: 1.1401510507798532e-05 loss: 0.8468381762504578\n",
      "lr: 1.1287821133735087e-05 loss: 1.267112135887146\n",
      "lr: 1.1174358471574871e-05 loss: 1.0799293518066406\n",
      "lr: 1.1061129450762038e-05 loss: 1.3641637563705444\n",
      "lr: 1.0948140986471659e-05 loss: 1.334214448928833\n",
      "lr: 1.0835399979187482e-05 loss: 1.0116041898727417\n",
      "lr: 1.0722913314280398e-05 loss: 1.0261900424957275\n",
      "lr: 1.061068786158796e-05 loss: 1.1561354398727417\n",
      "lr: 1.0498730474994891e-05 loss: 1.2846919298171997\n",
      "lr: 1.0387047992014452e-05 loss: 0.8530734181404114\n",
      "lr: 1.0275647233370842e-05 loss: 1.099050521850586\n",
      "lr: 1.0164535002582712e-05 loss: 0.923102855682373\n",
      "lr: 1.0053718085547496e-05 loss: 1.5527927875518799\n",
      "lr: 9.943203250127233e-06 loss: 0.9906477332115173\n",
      "lr: 9.832997245734997e-06 loss: 1.1188548803329468\n",
      "lr: 9.723106802922825e-06 loss: 0.7283861637115479\n",
      "lr: 9.613538632970638e-06 loss: 1.3983371257781982\n",
      "lr: 9.504299427476312e-06 loss: 1.1957769393920898\n",
      "lr: 9.395395857947107e-06 loss: 1.3639181852340698\n",
      "lr: 9.286834575392168e-06 loss: 1.2748639583587646\n",
      "lr: 9.178622209916236e-06 loss: 1.1419206857681274\n",
      "lr: 9.070765370315017e-06 loss: 1.1592390537261963\n",
      "lr: 8.96327064367119e-06 loss: 1.2325212955474854\n",
      "lr: 8.856144594952398e-06 loss: 0.9654591083526611\n",
      "lr: 8.749393766610214e-06 loss: 0.7876577377319336\n",
      "lr: 8.643024678180515e-06 loss: 1.1389968395233154\n",
      "lr: 8.53704382588545e-06 loss: 1.4134175777435303\n",
      "lr: 8.43145768223661e-06 loss: 1.3982564210891724\n",
      "lr: 8.32627269563974e-06 loss: 0.9910459518432617\n",
      "lr: 8.221495290000995e-06 loss: 0.9241329431533813\n",
      "lr: 8.117131864334465e-06 loss: 1.1637099981307983\n",
      "lr: 8.013188792371528e-06 loss: 0.9255372285842896\n",
      "lr: 7.909672422171519e-06 loss: 0.9068559408187866\n",
      "lr: 7.806589075734026e-06 loss: 1.0157890319824219\n",
      "lr: 7.703945048612844e-06 loss: 1.1079142093658447\n",
      "lr: 7.601746609531397e-06 loss: 1.0382241010665894\n",
      "lr: 7.50000000000001e-06 loss: 1.4520080089569092\n",
      "lr: 7.3987114339346105e-06 loss: 0.6535537838935852\n",
      "lr: 7.297887097277357e-06 loss: 1.0932607650756836\n",
      "lr: 7.19753314761873e-06 loss: 1.4754204750061035\n",
      "lr: 7.097655713821491e-06 loss: 1.140029788017273\n",
      "lr: 6.998260895646444e-06 loss: 1.5305949449539185\n",
      "lr: 6.8993547633798755e-06 loss: 1.4501651525497437\n",
      "lr: 6.800943357462773e-06 loss: 0.8683354258537292\n",
      "lr: 6.70303268812202e-06 loss: 1.337204933166504\n",
      "lr: 6.6056287350032345e-06 loss: 1.1189627647399902\n",
      "lr: 6.508737446805692e-06 loss: 1.0512255430221558\n",
      "lr: 6.412364740918924e-06 loss: 1.0269620418548584\n",
      "lr: 6.3165165030613335e-06 loss: 1.0866512060165405\n",
      "lr: 6.221198586920804e-06 loss: 0.8866609334945679\n",
      "lr: 6.126416813797164e-06 loss: 0.8060216903686523\n",
      "lr: 6.032176972246664e-06 loss: 1.1859419345855713\n",
      "lr: 5.938484817728481e-06 loss: 1.1957916021347046\n",
      "lr: 5.8453460722531155e-06 loss: 1.0313571691513062\n",
      "lr: 5.7527664240331245e-06 loss: 1.3103644847869873\n",
      "lr: 5.660751527135556e-06 loss: 1.355648159980774\n",
      "lr: 5.56930700113672e-06 loss: 1.0127692222595215\n",
      "lr: 5.478438430778973e-06 loss: 1.2278941869735718\n",
      "lr: 5.3881513656296115e-06 loss: 1.4485028982162476\n",
      "lr: 5.29845131974202e-06 loss: 1.062271237373352\n",
      "lr: 5.209343771318872e-06 loss: 1.0246177911758423\n",
      "lr: 5.1208341623775075e-06 loss: 0.953892171382904\n",
      "lr: 5.032927898417671e-06 loss: 1.2236343622207642\n",
      "lr: 4.945630348091369e-06 loss: 1.2456858158111572\n",
      "lr: 4.858946842874928e-06 loss: 1.492896556854248\n",
      "lr: 4.7728826767434424e-06 loss: 1.1355561017990112\n",
      "lr: 4.687443105847418e-06 loss: 0.9816626310348511\n",
      "lr: 4.602633348191823e-06 loss: 0.9670718908309937\n",
      "lr: 4.51845858331738e-06 loss: 1.3261610269546509\n",
      "lr: 4.434923951984218e-06 loss: 1.1376343965530396\n",
      "lr: 4.3520345558579505e-06 loss: 1.3216098546981812\n",
      "lr: 4.2697954571980855e-06 loss: 1.1827086210250854\n",
      "lr: 4.188211678548863e-06 loss: 0.7075449824333191\n",
      "lr: 4.10728820243251e-06 loss: 1.1376352310180664\n",
      "lr: 4.027029971044959e-06 loss: 0.839013934135437\n",
      "lr: 3.947441885954023e-06 loss: 0.9716500043869019\n",
      "lr: 3.8685288077999924e-06 loss: 0.8083478212356567\n",
      "lr: 3.790295555998849e-06 loss: 0.9921442270278931\n",
      "lr: 3.7127469084478915e-06 loss: 0.7087994813919067\n",
      "lr: 3.635887601233988e-06 loss: 0.8470199108123779\n",
      "lr: 3.5597223283442724e-06 loss: 1.1579664945602417\n",
      "lr: 3.4842557413794548e-06 loss: 1.1154131889343262\n",
      "lr: 3.4094924492698263e-06 loss: 0.766091525554657\n",
      "lr: 3.335437017993727e-06 loss: 1.3087024688720703\n",
      "lr: 3.262093970298658e-06 loss: 1.0875011682510376\n",
      "lr: 3.1894677854251276e-06 loss: 1.1159865856170654\n",
      "lr: 3.1175628988330653e-06 loss: 1.0746278762817383\n",
      "lr: 3.0463837019309208e-06 loss: 1.028499722480774\n",
      "lr: 2.975934541807544e-06 loss: 1.1866532564163208\n",
      "lr: 2.906219720966554e-06 loss: 1.3391820192337036\n",
      "lr: 2.8372434970637247e-06 loss: 0.8271535038948059\n",
      "lr: 2.7690100826468757e-06 loss: 1.1746642589569092\n",
      "lr: 2.7015236448986313e-06 loss: 1.230743646621704\n",
      "lr: 2.6347883053819077e-06 loss: 1.0443754196166992\n",
      "lr: 2.5688081397881822e-06 loss: 1.0638595819473267\n",
      "lr: 2.5035871776886204e-06 loss: 1.3608139753341675\n",
      "lr: 2.4391294022879863e-06 loss: 1.1756062507629395\n",
      "lr: 2.3754387501813094e-06 loss: 1.0630289316177368\n",
      "lr: 2.3125191111135417e-06 loss: 0.997312605381012\n",
      "lr: 2.250374327741937e-06 loss: 1.4166529178619385\n",
      "lr: 2.1890081954014397e-06 loss: 1.217998743057251\n",
      "lr: 2.128424461872847e-06 loss: 1.0645219087600708\n",
      "lr: 2.068626827153911e-06 loss: 1.250646710395813\n",
      "lr: 2.009618943233416e-06 loss: 1.3374758958816528\n",
      "lr: 1.9514044138681434e-06 loss: 0.9373730421066284\n",
      "lr: 1.8939867943627143e-06 loss: 1.0497536659240723\n",
      "lr: 1.8373695913525352e-06 loss: 1.0835108757019043\n",
      "lr: 1.781556262589576e-06 loss: 1.045663833618164\n",
      "lr: 1.726550216731253e-06 loss: 1.2674955129623413\n",
      "lr: 1.6723548131322176e-06 loss: 1.1691781282424927\n",
      "lr: 1.6189733616392027e-06 loss: 0.8900423049926758\n",
      "lr: 1.5664091223888944e-06 loss: 1.2945204973220825\n",
      "lr: 1.5146653056087811e-06 loss: 1.2347831726074219\n",
      "lr: 1.463745071421172e-06 loss: 0.9804661273956299\n",
      "lr: 1.4136515296501384e-06 loss: 0.8838231563568115\n",
      "lr: 1.3643877396316118e-06 loss: 0.6712470650672913\n",
      "lr: 1.3159567100265541e-06 loss: 1.0267215967178345\n",
      "lr: 1.2683613986371633e-06 loss: 0.8185840845108032\n",
      "lr: 1.2216047122263052e-06 loss: 1.0419650077819824\n",
      "lr: 1.1756895063399476e-06 loss: 1.4311262369155884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 1.1306185851327305e-06 loss: 0.9720602035522461\n",
      "lr: 1.086394701196835e-06 loss: 1.0204732418060303\n",
      "lr: 1.043020555393699e-06 loss: 0.9089721441268921\n",
      "lr: 1.0004987966892121e-06 loss: 0.9063559174537659\n",
      "lr: 9.588320219918828e-07 loss: 1.2345110177993774\n",
      "lr: 9.180227759942094e-07 loss: 0.9477089643478394\n",
      "lr: 8.780735510173332e-07 loss: 0.7845004796981812\n",
      "lr: 8.389867868587847e-07 loss: 0.952749490737915\n",
      "lr: 8.007648706434817e-07 loss: 1.0452957153320312\n",
      "lr: 7.634101366779755e-07 loss: 0.9529073238372803\n",
      "lr: 7.269248663078187e-07 loss: 0.8852163553237915\n",
      "lr: 6.913112877783146e-07 loss: 0.9379666447639465\n",
      "lr: 6.565715760983875e-07 loss: 1.45338773727417\n",
      "lr: 6.227078529077634e-07 loss: 1.2800532579421997\n",
      "lr: 5.897221863474006e-07 loss: 1.2607276439666748\n",
      "lr: 5.576165909331599e-07 loss: 0.8427186012268066\n",
      "lr: 5.263930274328011e-07 loss: 0.7848317623138428\n",
      "lr: 4.960534027462371e-07 loss: 1.1470668315887451\n",
      "lr: 4.665995697890407e-07 loss: 0.9613921046257019\n",
      "lr: 4.38033327379313e-07 loss: 1.4364873170852661\n",
      "lr: 4.1035642012780484e-07 loss: 1.0018093585968018\n",
      "lr: 3.8357053833139034e-07 loss: 1.2358002662658691\n",
      "lr: 3.576773178698245e-07 loss: 1.1015453338623047\n",
      "lr: 3.3267834010582475e-07 loss: 1.29391610622406\n",
      "lr: 3.0857513178852324e-07 loss: 0.8132695555686951\n",
      "lr: 2.853691649601864e-07 loss: 1.1028907299041748\n",
      "lr: 2.630618568663551e-07 loss: 1.0674787759780884\n",
      "lr: 2.4165456986926073e-07 loss: 1.2551491260528564\n",
      "lr: 2.2114861136461662e-07 loss: 1.252256989479065\n",
      "lr: 2.0154523370180366e-07 loss: 1.0903884172439575\n",
      "lr: 1.8284563410735978e-07 loss: 0.8420754075050354\n",
      "lr: 1.6505095461187337e-07 loss: 1.2066376209259033\n",
      "lr: 1.481622819802392e-07 loss: 1.1817948818206787\n",
      "lr: 1.3218064764526472e-07 loss: 1.8124316930770874\n",
      "lr: 1.1710702764471893e-07 loss: 1.0436620712280273\n",
      "lr: 1.0294234256168656e-07 loss: 1.1946898698806763\n",
      "lr: 8.968745746835816e-08 loss: 1.2031092643737793\n",
      "lr: 7.734318187320566e-08 loss: 1.1619515419006348\n",
      "lr: 6.591026967152858e-08 loss: 1.2552871704101562\n",
      "lr: 5.538941909942585e-08 loss: 1.5540049076080322\n",
      "lr: 4.578127269114829e-08 loss: 1.0661592483520508\n",
      "lr: 3.7086417239846626e-08 loss: 0.9495710730552673\n",
      "\n",
      "\n",
      "Eval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864018d7edcb4742acf3192909e54eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=201, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'step': 1600, 'lr': 2.9305383761760175e-08, 'tr_loss': 1.1224539572000503, 'eval_val_loss': 1.3547300574791372, 'eval_val_acc': 0.7272727272727273}\n",
      "lr: 2.9305383761760175e-08 loss: 1.1301831007003784\n",
      "lr: 2.2438647463756124e-08 loss: 1.0204837322235107\n",
      "lr: 1.648662771432774e-08 loss: 1.0388537645339966\n",
      "lr: 1.1449688017978188e-08 loss: 1.2510905265808105\n",
      "lr: 7.328135993011631e-09 loss: 0.9677672386169434\n",
      "lr: 4.1222233527615786e-09 loss: 1.215911626815796\n",
      "lr: 1.8321458902048616e-09 loss: 1.634945273399353\n",
      "lr: 4.580434660111932e-10 loss: 1.3206051588058472\n",
      "lr: 3e-05 loss: 1.4330514669418335\n",
      "lr: 2.999954195653399e-05 loss: 0.8355886936187744\n",
      "lr: 2.9998167854109796e-05 loss: 0.9153028726577759\n",
      "lr: 2.999587777664724e-05 loss: 0.8979041576385498\n",
      "lr: 2.9992671864006992e-05 loss: 1.295678973197937\n",
      "lr: 2.9988550311982024e-05 loss: 0.8101576566696167\n",
      "lr: 2.9983513372285672e-05 loss: 1.0914545059204102\n",
      "lr: 2.9977561352536243e-05 loss: 1.2756152153015137\n",
      "lr: 2.997069461623824e-05 loss: 1.1035513877868652\n",
      "lr: 2.996291358276015e-05 loss: 1.425502896308899\n",
      "lr: 2.995421872730885e-05 loss: 1.1904370784759521\n",
      "lr: 2.9944610580900574e-05 loss: 1.197685956954956\n",
      "lr: 2.9934089730328475e-05 loss: 0.9568477869033813\n",
      "lr: 2.9922656818126796e-05 loss: 1.348827600479126\n",
      "lr: 2.9910312542531643e-05 loss: 1.1370470523834229\n",
      "lr: 2.9897057657438315e-05 loss: 0.8127992749214172\n",
      "lr: 2.9882892972355276e-05 loss: 1.0751569271087646\n",
      "lr: 2.9867819352354736e-05 loss: 1.124094843864441\n",
      "lr: 2.9851837718019766e-05 loss: 1.1620086431503296\n",
      "lr: 2.983494904538813e-05 loss: 0.8018630146980286\n",
      "lr: 2.9817154365892643e-05 loss: 1.2188453674316406\n",
      "lr: 2.97984547662982e-05 loss: 1.1214354038238525\n",
      "lr: 2.9778851388635386e-05 loss: 0.9561479687690735\n",
      "lr: 2.975834543013074e-05 loss: 1.0090852975845337\n",
      "lr: 2.9736938143133647e-05 loss: 1.3847389221191406\n",
      "lr: 2.971463083503981e-05 loss: 0.5197550058364868\n",
      "lr: 2.9691424868211482e-05 loss: 0.7903630137443542\n",
      "lr: 2.9667321659894174e-05 loss: 1.0476219654083252\n",
      "lr: 2.9642322682130177e-05 loss: 0.9610093832015991\n",
      "lr: 2.9616429461668616e-05 loss: 1.1292608976364136\n",
      "lr: 2.95896435798722e-05 loss: 1.536429762840271\n",
      "lr: 2.9561966672620694e-05 loss: 1.0989793539047241\n",
      "lr: 2.9533400430210962e-05 loss: 1.0788847208023071\n",
      "lr: 2.9503946597253753e-05 loss: 1.1463208198547363\n",
      "lr: 2.94736069725672e-05 loss: 1.0891577005386353\n",
      "lr: 2.944238340906685e-05 loss: 1.1406368017196655\n",
      "lr: 2.9410277813652608e-05 loss: 0.9462178945541382\n",
      "lr: 2.937729214709224e-05 loss: 0.995772659778595\n",
      "lr: 2.9343428423901614e-05 loss: 1.2691186666488647\n",
      "lr: 2.9308688712221687e-05 loss: 1.099574089050293\n",
      "lr: 2.927307513369218e-05 loss: 0.7812067866325378\n",
      "lr: 2.9236589863322025e-05 loss: 1.098281741142273\n",
      "lr: 2.9199235129356513e-05 loss: 0.6988624334335327\n",
      "lr: 2.9161013213141214e-05 loss: 1.2247376441955566\n",
      "lr: 2.912192644898267e-05 loss: 1.0020992755889893\n",
      "lr: 2.9081977224005792e-05 loss: 1.0743213891983032\n",
      "lr: 2.904116797800812e-05 loss: 1.0243117809295654\n",
      "lr: 2.899950120331078e-05 loss: 1.324350357055664\n",
      "lr: 2.895697944460631e-05 loss: 1.2621591091156006\n",
      "lr: 2.8913605298803175e-05 loss: 1.059741497039795\n",
      "lr: 2.8869381414867254e-05 loss: 0.9137195348739624\n",
      "lr: 2.8824310493660053e-05 loss: 0.9972242712974548\n",
      "lr: 2.877839528777369e-05 loss: 1.1778783798217773\n",
      "lr: 2.8731638601362847e-05 loss: 1.4663108587265015\n",
      "lr: 2.868404328997346e-05 loss: 1.0341441631317139\n",
      "lr: 2.863561226036839e-05 loss: 1.2748926877975464\n",
      "lr: 2.8586348470349863e-05 loss: 1.4279252290725708\n",
      "lr: 2.853625492857883e-05 loss: 0.8288218379020691\n",
      "lr: 2.8485334694391223e-05 loss: 0.8487347960472107\n",
      "lr: 2.8433590877611112e-05 loss: 1.3045892715454102\n",
      "lr: 2.8381026638360796e-05 loss: 0.7390639185905457\n",
      "lr: 2.8327645186867783e-05 loss: 0.7826998233795166\n",
      "lr: 2.8273449783268748e-05 loss: 1.113024353981018\n",
      "lr: 2.8218443737410426e-05 loss: 0.8812228441238403\n",
      "lr: 2.8162630408647467e-05 loss: 0.7477806806564331\n",
      "lr: 2.81060132056373e-05 loss: 0.9585070013999939\n",
      "lr: 2.804859558613185e-05 loss: 0.9817945957183838\n",
      "lr: 2.7990381056766573e-05 loss: 1.235117793083191\n",
      "lr: 2.793137317284609e-05 loss: 1.4921361207962036\n",
      "lr: 2.7871575538127154e-05 loss: 1.1177892684936523\n",
      "lr: 2.7810991804598576e-05 loss: 1.046690583229065\n",
      "lr: 2.774962567225807e-05 loss: 1.1497100591659546\n",
      "lr: 2.7687480888886472e-05 loss: 1.1676474809646606\n",
      "lr: 2.7624561249818693e-05 loss: 1.0752050876617432\n",
      "lr: 2.7560870597711995e-05 loss: 1.4433512687683105\n",
      "lr: 2.749641282231138e-05 loss: 1.1326006650924683\n",
      "lr: 2.743119186021183e-05 loss: 0.682239294052124\n",
      "lr: 2.7365211694618107e-05 loss: 0.8129976987838745\n",
      "lr: 2.7298476355101372e-05 loss: 1.2103416919708252\n",
      "lr: 2.7230989917353125e-05 loss: 0.6729298830032349\n",
      "lr: 2.7162756502936278e-05 loss: 0.9833289980888367\n",
      "lr: 2.7093780279033446e-05 loss: 1.1284366846084595\n",
      "lr: 2.7024065458192463e-05 loss: 0.8460115194320679\n",
      "lr: 2.6953616298069067e-05 loss: 0.927605390548706\n",
      "lr: 2.6882437101166935e-05 loss: 1.004741907119751\n",
      "lr: 2.6810532214574873e-05 loss: 1.2491943836212158\n",
      "lr: 2.6737906029701346e-05 loss: 1.6747384071350098\n",
      "lr: 2.6664562982006273e-05 loss: 0.6864830851554871\n",
      "lr: 2.659050755073016e-05 loss: 1.2379271984100342\n",
      "lr: 2.651574425862056e-05 loss: 1.1442813873291016\n",
      "lr: 2.6440277671655743e-05 loss: 0.8662487864494324\n",
      "lr: 2.6364112398765985e-05 loss: 1.0892194509506226\n",
      "lr: 2.6287253091552084e-05 loss: 0.7941493988037109\n",
      "lr: 2.620970444400114e-05 loss: 0.5675273537635803\n",
      "lr: 2.6131471192200025e-05 loss: 1.1719893217086792\n",
      "lr: 2.605255811404599e-05 loss: 1.458450198173523\n",
      "lr: 2.5972970028955044e-05 loss: 0.9718736410140991\n",
      "lr: 2.5892711797567494e-05 loss: 0.8904135227203369\n",
      "lr: 2.581178832145114e-05 loss: 0.9342507123947144\n",
      "lr: 2.5730204542801915e-05 loss: 0.9801559448242188\n",
      "lr: 2.564796544414205e-05 loss: 1.2056318521499634\n",
      "lr: 2.5565076048015785e-05 loss: 1.2122589349746704\n",
      "lr: 2.548154141668262e-05 loss: 1.418637990951538\n",
      "lr: 2.5397366651808176e-05 loss: 1.1906020641326904\n",
      "lr: 2.5312556894152582e-05 loss: 0.7004014253616333\n",
      "lr: 2.522711732325656e-05 loss: 0.5668277144432068\n",
      "lr: 2.514105315712509e-05 loss: 0.9370222091674805\n",
      "lr: 2.505436965190865e-05 loss: 1.097888469696045\n",
      "lr: 2.4967072101582314e-05 loss: 0.7447882294654846\n",
      "lr: 2.4879165837622494e-05 loss: 0.8619219064712524\n",
      "lr: 2.479065622868113e-05 loss: 0.6376074552536011\n",
      "lr: 2.4701548680257996e-05 loss: 0.7714126110076904\n",
      "lr: 2.4611848634370403e-05 loss: 1.0700362920761108\n",
      "lr: 2.4521561569221047e-05 loss: 0.9936774969100952\n",
      "lr: 2.4430692998863286e-05 loss: 1.120481252670288\n",
      "lr: 2.4339248472864444e-05 loss: 1.001477837562561\n",
      "lr: 2.4247233575966877e-05 loss: 0.8739006519317627\n",
      "lr: 2.415465392774689e-05 loss: 0.7657736539840698\n",
      "lr: 2.4061515182271537e-05 loss: 1.1169573068618774\n",
      "lr: 2.3967823027753333e-05 loss: 1.2606042623519897\n",
      "lr: 2.3873583186202837e-05 loss: 1.0957977771759033\n",
      "lr: 2.3778801413079198e-05 loss: 1.1625252962112427\n",
      "lr: 2.368348349693867e-05 loss: 0.9505939483642578\n",
      "lr: 2.3587635259081077e-05 loss: 1.1246576309204102\n",
      "lr: 2.349126255319429e-05 loss: 0.8845837116241455\n",
      "lr: 2.3394371264996753e-05 loss: 1.0326218605041504\n",
      "lr: 2.3296967311877982e-05 loss: 1.0095162391662598\n",
      "lr: 2.3199056642537224e-05 loss: 0.9511983394622803\n",
      "lr: 2.310064523662012e-05 loss: 0.7457235455513\n",
      "lr: 2.300173910435354e-05 loss: 1.4889768362045288\n",
      "lr: 2.290234428617853e-05 loss: 0.8559707403182983\n",
      "lr: 2.2802466852381287e-05 loss: 1.0382938385009766\n",
      "lr: 2.2702112902722646e-05 loss: 1.1611062288284302\n",
      "lr: 2.260128856606536e-05 loss: 0.8045530915260315\n",
      "lr: 2.2499999999999978e-05 loss: 0.898943305015564\n",
      "lr: 2.239825339046862e-05 loss: 1.012066125869751\n",
      "lr: 2.229605495138718e-05 loss: 0.9690330028533936\n",
      "lr: 2.2193410924265974e-05 loss: 1.1777797937393188\n",
      "lr: 2.2090327577828487e-05 loss: 0.773927628993988\n",
      "lr: 2.198681120762847e-05 loss: 1.0046617984771729\n",
      "lr: 2.1882868135665536e-05 loss: 0.7352808117866516\n",
      "lr: 2.1778504709999008e-05 loss: 1.0932033061981201\n",
      "lr: 2.167372730436025e-05 loss: 1.2438313961029053\n",
      "lr: 2.156854231776339e-05 loss: 1.0791200399398804\n",
      "lr: 2.146295617411455e-05 loss: 0.8687880039215088\n",
      "lr: 2.1356975321819486e-05 loss: 1.1508557796478271\n",
      "lr: 2.125060623338979e-05 loss: 1.0160045623779297\n",
      "lr: 2.1143855405047624e-05 loss: 1.072274088859558\n",
      "lr: 2.103672935632883e-05 loss: 0.8845959305763245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 2.0929234629684966e-05 loss: 1.0361101627349854\n",
      "lr: 2.0821377790083728e-05 loss: 0.898747980594635\n",
      "lr: 2.0713165424607836e-05 loss: 1.2061364650726318\n",
      "lr: 2.0604604142052914e-05 loss: 1.2669261693954468\n",
      "lr: 2.049570057252371e-05 loss: 1.151842713356018\n",
      "lr: 2.0386461367029383e-05 loss: 0.8475236296653748\n",
      "lr: 2.0276893197077177e-05 loss: 0.9430534839630127\n",
      "lr: 2.0167002754265005e-05 loss: 0.9682321548461914\n",
      "lr: 2.005679674987277e-05 loss: 0.9969321489334106\n",
      "lr: 1.9946281914452505e-05 loss: 0.9888037443161011\n",
      "lr: 1.9835464997417313e-05 loss: 0.8406181931495667\n",
      "lr: 1.9724352766629157e-05 loss: 1.086153507232666\n",
      "lr: 1.9612952007985554e-05 loss: 1.1720316410064697\n",
      "lr: 1.9501269525005113e-05 loss: 0.8012633919715881\n",
      "lr: 1.938931213841204e-05 loss: 1.137161374092102\n",
      "lr: 1.9277086685719606e-05 loss: 0.8578922152519226\n",
      "lr: 1.91646000208125e-05 loss: 1.1720740795135498\n",
      "lr: 1.9051859013528323e-05 loss: 0.9505360126495361\n",
      "lr: 1.8938870549237963e-05 loss: 0.8681583404541016\n",
      "lr: 1.8825641528425133e-05 loss: 0.6250779628753662\n",
      "lr: 1.871217886626492e-05 loss: 0.8290479183197021\n",
      "lr: 1.859848949220145e-05 loss: 0.7424461245536804\n",
      "lr: 1.8484580349524765e-05 loss: 0.861016035079956\n",
      "lr: 1.8370458394946596e-05 loss: 0.8083614110946655\n",
      "lr: 1.825613059817572e-05 loss: 1.1692428588867188\n",
      "lr: 1.814160394149214e-05 loss: 0.8658817410469055\n",
      "lr: 1.8026885419320842e-05 loss: 0.8074442744255066\n",
      "lr: 1.7911982037804427e-05 loss: 1.264210820198059\n",
      "lr: 1.7796900814375225e-05 loss: 1.4848617315292358\n",
      "lr: 1.768164877732695e-05 loss: 0.6938842535018921\n",
      "lr: 1.7566232965385255e-05 loss: 0.8720884919166565\n",
      "lr: 1.7450660427277957e-05 loss: 1.3190059661865234\n",
      "lr: 1.7334938221304504e-05 loss: 0.9927486181259155\n",
      "lr: 1.721907341490495e-05 loss: 0.8336671590805054\n",
      "lr: 1.7103073084228284e-05 loss: 1.097951889038086\n",
      "lr: 1.6986944313700314e-05 loss: 1.4821895360946655\n",
      "lr: 1.6870694195590985e-05 loss: 0.937455952167511\n",
      "lr: 1.675432982958123e-05 loss: 1.5475119352340698\n",
      "lr: 1.663785832232938e-05 loss: 1.0724633932113647\n",
      "lr: 1.6521286787037203e-05 loss: 1.1371582746505737\n",
      "lr: 1.6404622343015288e-05 loss: 0.9991291165351868\n",
      "lr: 1.6287872115248513e-05 loss: 0.9508769512176514\n",
      "lr: 1.6171043233960708e-05 loss: 1.0026249885559082\n",
      "\n",
      "\n",
      "Eval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502779433a20478ba81a7632830028dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=201, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'step': 1800, 'lr': 1.6054142834179372e-05, 'tr_loss': 1.0415245082974434, 'eval_val_loss': 1.2507506309457086, 'eval_val_acc': 0.7434620174346201}\n",
      "lr: 1.6054142834179372e-05 loss: 0.9390932321548462\n",
      "lr: 1.593717805529976e-05 loss: 1.143384337425232\n",
      "lr: 1.5820156040648804e-05 loss: 1.3031623363494873\n",
      "lr: 1.5703083937049113e-05 loss: 0.7480264902114868\n",
      "lr: 1.5585968894382304e-05 loss: 0.8604408502578735\n",
      "lr: 1.5468818065152374e-05 loss: 0.9840025901794434\n",
      "lr: 1.5351638604048917e-05 loss: 0.6641530394554138\n",
      "lr: 1.5234437667510125e-05 loss: 0.8625996112823486\n",
      "lr: 1.5117222413285762e-05 loss: 0.8262100219726562\n",
      "lr: 1.5e-05 loss: 1.1441068649291992\n",
      "lr: 1.488277758671424e-05 loss: 1.2329127788543701\n",
      "lr: 1.4765562332489879e-05 loss: 0.6787736415863037\n",
      "lr: 1.4648361395951088e-05 loss: 0.9141740798950195\n",
      "lr: 1.453118193484763e-05 loss: 0.7936980724334717\n",
      "lr: 1.44140311056177e-05 loss: 0.7978560328483582\n",
      "lr: 1.4296916062950888e-05 loss: 1.2007322311401367\n",
      "lr: 1.4179843959351197e-05 loss: 0.7374128699302673\n",
      "lr: 1.4062821944700247e-05 loss: 0.7191410660743713\n",
      "lr: 1.3945857165820588e-05 loss: 0.9551156759262085\n",
      "lr: 1.3828956766039258e-05 loss: 0.5927775502204895\n",
      "lr: 1.371212788475153e-05 loss: 0.6373763084411621\n",
      "lr: 1.3595377656984757e-05 loss: 0.633736252784729\n",
      "lr: 1.3478713212962842e-05 loss: 1.1424880027770996\n",
      "lr: 1.3362141677670621e-05 loss: 0.8112658858299255\n",
      "lr: 1.3245670170418733e-05 loss: 1.321908712387085\n",
      "lr: 1.3129305804409018e-05 loss: 0.955704927444458\n",
      "lr: 1.301305568629969e-05 loss: 0.9675122499465942\n",
      "lr: 1.289692691577172e-05 loss: 0.9280775189399719\n",
      "lr: 1.2780926585095053e-05 loss: 0.8739150762557983\n",
      "lr: 1.2665061778695496e-05 loss: 0.6401370763778687\n",
      "lr: 1.2549339572722044e-05 loss: 1.2519274950027466\n",
      "lr: 1.2433767034614748e-05 loss: 0.8077155351638794\n",
      "lr: 1.2318351222673054e-05 loss: 0.72660893201828\n",
      "lr: 1.2203099185624776e-05 loss: 0.7654901742935181\n",
      "lr: 1.2088017962195576e-05 loss: 0.6790668368339539\n",
      "lr: 1.1973114580679121e-05 loss: 0.8359133005142212\n",
      "lr: 1.185839605850782e-05 loss: 0.7247201800346375\n",
      "lr: 1.1743869401824322e-05 loss: 0.8455745577812195\n",
      "lr: 1.1629541605053447e-05 loss: 0.9062280058860779\n",
      "lr: 1.1515419650475273e-05 loss: 0.7430083155632019\n",
      "lr: 1.1401510507798512e-05 loss: 0.8235881924629211\n",
      "lr: 1.1287821133735048e-05 loss: 0.5699304342269897\n",
      "lr: 1.1174358471574871e-05 loss: 0.8880180716514587\n",
      "lr: 1.1061129450762038e-05 loss: 0.9684479236602783\n",
      "lr: 1.0948140986471681e-05 loss: 0.8997089862823486\n",
      "lr: 1.08353999791875e-05 loss: 0.44836175441741943\n",
      "lr: 1.0722913314280398e-05 loss: 0.7910798788070679\n",
      "lr: 1.061068786158796e-05 loss: 0.941874086856842\n",
      "lr: 1.0498730474994891e-05 loss: 0.8617370128631592\n",
      "lr: 1.0387047992014452e-05 loss: 0.7847431302070618\n",
      "lr: 1.0275647233370842e-05 loss: 0.9419078230857849\n",
      "lr: 1.0164535002582693e-05 loss: 0.7723057270050049\n",
      "lr: 1.0053718085547496e-05 loss: 1.0345407724380493\n",
      "lr: 9.943203250127233e-06 loss: 0.877759575843811\n",
      "lr: 9.832997245734997e-06 loss: 0.9758612513542175\n",
      "lr: 9.723106802922825e-06 loss: 0.7070244550704956\n",
      "lr: 9.61353863297066e-06 loss: 0.7273836731910706\n",
      "lr: 9.504299427476295e-06 loss: 1.1318124532699585\n",
      "lr: 9.395395857947087e-06 loss: 0.6903613805770874\n",
      "lr: 9.286834575392131e-06 loss: 0.6794716119766235\n",
      "lr: 9.178622209916236e-06 loss: 0.9127718210220337\n",
      "lr: 9.070765370315036e-06 loss: 0.9619558453559875\n",
      "lr: 8.963270643671213e-06 loss: 0.9382725954055786\n",
      "lr: 8.856144594952417e-06 loss: 1.0398505926132202\n",
      "lr: 8.749393766610214e-06 loss: 1.0942096710205078\n",
      "lr: 8.643024678180479e-06 loss: 0.6288122534751892\n",
      "lr: 8.53704382588545e-06 loss: 0.5987646579742432\n",
      "lr: 8.43145768223661e-06 loss: 0.7668930292129517\n",
      "lr: 8.326272695639759e-06 loss: 1.3239459991455078\n",
      "lr: 8.221495290000995e-06 loss: 1.024049997329712\n",
      "lr: 8.117131864334465e-06 loss: 0.7312038540840149\n",
      "lr: 8.013188792371528e-06 loss: 1.2552212476730347\n",
      "lr: 7.909672422171519e-06 loss: 1.5615712404251099\n",
      "lr: 7.806589075734026e-06 loss: 0.8119305968284607\n",
      "lr: 7.703945048612827e-06 loss: 0.6400287747383118\n",
      "lr: 7.601746609531378e-06 loss: 1.0024588108062744\n",
      "lr: 7.499999999999992e-06 loss: 0.6159818768501282\n",
      "lr: 7.3987114339346105e-06 loss: 0.7321624755859375\n",
      "lr: 7.29788709727739e-06 loss: 0.9597983956336975\n",
      "lr: 7.197533147618752e-06 loss: 0.6579565405845642\n",
      "lr: 7.09765571382151e-06 loss: 1.187665343284607\n",
      "lr: 6.99826089564646e-06 loss: 0.8689409494400024\n",
      "lr: 6.8993547633798425e-06 loss: 0.9476383924484253\n",
      "lr: 6.800943357462773e-06 loss: 0.7977104187011719\n",
      "lr: 6.70303268812202e-06 loss: 0.9147412776947021\n",
      "lr: 6.605628735003251e-06 loss: 1.1850937604904175\n",
      "lr: 6.5087374468057085e-06 loss: 0.8656830191612244\n",
      "lr: 6.412364740918924e-06 loss: 0.4720615744590759\n",
      "lr: 6.3165165030613335e-06 loss: 1.153678059577942\n",
      "lr: 6.221198586920804e-06 loss: 0.8453993201255798\n",
      "lr: 6.126416813797164e-06 loss: 0.6536783576011658\n",
      "lr: 6.032176972246664e-06 loss: 0.5775091648101807\n",
      "lr: 5.9384848177284656e-06 loss: 0.7237544655799866\n",
      "lr: 5.8453460722531155e-06 loss: 0.7653273940086365\n",
      "lr: 5.7527664240331245e-06 loss: 0.7889248132705688\n",
      "lr: 5.660751527135556e-06 loss: 0.6063271760940552\n",
      "lr: 5.56930700113672e-06 loss: 0.7756036520004272\n",
      "lr: 5.478438430778988e-06 loss: 1.0887668132781982\n",
      "lr: 5.388151365629597e-06 loss: 0.7929855585098267\n",
      "lr: 5.2984513197420045e-06 loss: 1.0181021690368652\n",
      "lr: 5.2093437713188375e-06 loss: 0.7403073906898499\n",
      "lr: 5.120834162377478e-06 loss: 0.599166750907898\n",
      "lr: 5.032927898417685e-06 loss: 0.9048283696174622\n",
      "lr: 4.945630348091382e-06 loss: 0.7764095664024353\n",
      "lr: 4.8589468428749415e-06 loss: 0.7144021391868591\n",
      "lr: 4.7728826767434424e-06 loss: 0.7234353423118591\n",
      "lr: 4.687443105847418e-06 loss: 1.141839861869812\n",
      "lr: 4.602633348191823e-06 loss: 1.1214631795883179\n",
      "lr: 4.51845858331738e-06 loss: 0.542669415473938\n",
      "lr: 4.434923951984218e-06 loss: 0.7521895170211792\n",
      "lr: 4.3520345558579505e-06 loss: 1.0553052425384521\n",
      "lr: 4.2697954571980855e-06 loss: 0.5732764601707458\n",
      "lr: 4.188211678548863e-06 loss: 1.1788382530212402\n",
      "lr: 4.10728820243251e-06 loss: 0.6209441423416138\n",
      "lr: 4.027029971044987e-06 loss: 1.2027167081832886\n",
      "lr: 3.94744188595401e-06 loss: 0.64900803565979\n",
      "lr: 3.868528807799979e-06 loss: 1.0075874328613281\n",
      "lr: 3.7902955559988357e-06 loss: 0.9645320773124695\n",
      "lr: 3.7127469084478915e-06 loss: 0.8250259160995483\n",
      "lr: 3.6358876012340183e-06 loss: 0.7080977559089661\n",
      "lr: 3.559722328344286e-06 loss: 0.8131917715072632\n",
      "lr: 3.484255741379468e-06 loss: 0.7787313461303711\n",
      "lr: 3.40949244926984e-06 loss: 0.5464307069778442\n",
      "lr: 3.335437017993702e-06 loss: 0.7925918102264404\n",
      "lr: 3.262093970298658e-06 loss: 0.9761983156204224\n",
      "lr: 3.1894677854251276e-06 loss: 0.9334304332733154\n",
      "lr: 3.1175628988330653e-06 loss: 0.7591357231140137\n",
      "lr: 3.0463837019309326e-06 loss: 0.9456170201301575\n",
      "lr: 2.975934541807544e-06 loss: 1.1294806003570557\n",
      "lr: 2.906219720966554e-06 loss: 1.003077745437622\n",
      "lr: 2.8372434970637247e-06 loss: 1.1308513879776\n",
      "lr: 2.7690100826468757e-06 loss: 0.9848297238349915\n",
      "lr: 2.7015236448986313e-06 loss: 0.8358206748962402\n",
      "lr: 2.6347883053818962e-06 loss: 1.100207805633545\n",
      "lr: 2.5688081397881708e-06 loss: 0.9975349307060242\n",
      "lr: 2.5035871776886204e-06 loss: 0.8430293798446655\n",
      "lr: 2.4391294022879863e-06 loss: 1.2475793361663818\n",
      "lr: 2.3754387501813094e-06 loss: 0.8020051121711731\n",
      "lr: 2.312519111113552e-06 loss: 0.5403661131858826\n",
      "lr: 2.2503743277419486e-06 loss: 0.9182878136634827\n",
      "lr: 2.189008195401426e-06 loss: 0.7675201296806335\n",
      "lr: 2.1284244618728255e-06 loss: 0.6281465291976929\n",
      "lr: 2.068626827153891e-06 loss: 1.0169031620025635\n",
      "lr: 2.009618943233426e-06 loss: 0.7730711698532104\n",
      "lr: 1.9514044138681536e-06 loss: 1.2555433511734009\n",
      "lr: 1.8939867943627242e-06 loss: 0.7120733261108398\n",
      "lr: 1.8373695913525352e-06 loss: 0.8064517378807068\n",
      "lr: 1.781556262589576e-06 loss: 0.9080224633216858\n",
      "lr: 1.726550216731253e-06 loss: 0.7549341320991516\n",
      "lr: 1.6723548131322176e-06 loss: 0.7042229175567627\n",
      "lr: 1.6189733616392027e-06 loss: 0.8302329182624817\n",
      "lr: 1.566409122388886e-06 loss: 0.8203451037406921\n",
      "lr: 1.5146653056087811e-06 loss: 0.8769662380218506\n",
      "lr: 1.463745071421172e-06 loss: 0.8752363324165344\n",
      "lr: 1.4136515296501384e-06 loss: 1.1242783069610596\n",
      "lr: 1.3643877396316317e-06 loss: 0.7434664368629456\n",
      "lr: 1.3159567100265607e-06 loss: 0.7307887077331543\n",
      "lr: 1.268361398637155e-06 loss: 0.5244036912918091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 1.221604712226297e-06 loss: 0.8466988205909729\n",
      "lr: 1.175689506339931e-06 loss: 0.7859343886375427\n",
      "lr: 1.130618585132749e-06 loss: 0.9456207752227783\n",
      "lr: 1.0863947011968416e-06 loss: 1.0410540103912354\n",
      "lr: 1.0430205553937072e-06 loss: 0.8813964128494263\n",
      "lr: 1.0004987966892204e-06 loss: 0.41960692405700684\n",
      "lr: 9.588320219918828e-07 loss: 1.3342686891555786\n",
      "lr: 9.180227759942094e-07 loss: 0.9895574450492859\n",
      "lr: 8.780735510173332e-07 loss: 0.8275874853134155\n",
      "lr: 8.389867868587847e-07 loss: 1.0713410377502441\n",
      "lr: 8.0076487064349e-07 loss: 1.0646922588348389\n",
      "lr: 7.634101366779755e-07 loss: 0.7419031858444214\n",
      "lr: 7.269248663078187e-07 loss: 0.9344905018806458\n",
      "lr: 6.913112877783146e-07 loss: 0.9199684262275696\n",
      "lr: 6.565715760983875e-07 loss: 1.1121400594711304\n",
      "lr: 6.227078529077634e-07 loss: 1.1721009016036987\n",
      "lr: 5.897221863473956e-07 loss: 1.0163780450820923\n",
      "lr: 5.576165909331532e-07 loss: 0.8850266933441162\n",
      "lr: 5.263930274328011e-07 loss: 0.6096881031990051\n",
      "lr: 4.960534027462371e-07 loss: 1.223719596862793\n",
      "lr: 4.665995697890407e-07 loss: 0.9157614707946777\n",
      "lr: 4.38033327379318e-07 loss: 0.6225845813751221\n",
      "lr: 4.103564201278098e-07 loss: 0.856596827507019\n",
      "lr: 3.835705383313853e-07 loss: 0.9578974843025208\n",
      "lr: 3.576773178698162e-07 loss: 0.6428301930427551\n",
      "lr: 3.3267834010581644e-07 loss: 0.787691593170166\n",
      "lr: 3.0857513178852324e-07 loss: 0.7684937715530396\n",
      "lr: 2.853691649601897e-07 loss: 0.6365127563476562\n",
      "lr: 2.630618568663601e-07 loss: 0.8039895296096802\n",
      "lr: 2.4165456986926073e-07 loss: 0.5891141295433044\n",
      "lr: 2.2114861136461662e-07 loss: 0.5265387296676636\n",
      "lr: 2.0154523370180366e-07 loss: 0.9807608723640442\n",
      "lr: 1.8284563410735978e-07 loss: 0.6528249979019165\n",
      "lr: 1.6505095461187337e-07 loss: 1.2096974849700928\n",
      "lr: 1.4816228198023419e-07 loss: 0.9925897717475891\n",
      "lr: 1.3218064764526472e-07 loss: 1.2419822216033936\n",
      "lr: 1.1710702764471893e-07 loss: 1.5009193420410156\n",
      "lr: 1.0294234256168656e-07 loss: 1.0686075687408447\n",
      "lr: 8.968745746836315e-08 loss: 0.6201391816139221\n",
      "lr: 7.734318187320899e-08 loss: 1.3020193576812744\n",
      "lr: 6.591026967152691e-08 loss: 0.761263370513916\n",
      "lr: 5.538941909942419e-08 loss: 0.9130788445472717\n",
      "\n",
      "\n",
      "Eval\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f1e75e12f546118255da448b492dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=201, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'step': 2000, 'lr': 4.578127269114496e-08, 'tr_loss': 0.87503807336092, 'eval_val_loss': 1.2128462560141264, 'eval_val_acc': 0.7440846824408468}\n",
      "lr: 4.578127269114496e-08 loss: 0.5721219778060913\n",
      "lr: 3.7086417239849955e-08 loss: 1.0149517059326172\n",
      "lr: 2.930538376176184e-08 loss: 1.4307959079742432\n",
      "lr: 2.2438647463757788e-08 loss: 1.1971979141235352\n",
      "lr: 1.6486627714329405e-08 loss: 0.9176814556121826\n",
      "lr: 1.1449688017978188e-08 loss: 1.1759567260742188\n",
      "lr: 7.328135993011631e-09 loss: 0.8048155307769775\n",
      "lr: 4.1222233527615786e-09 loss: 1.1651575565338135\n",
      "lr: 1.8321458902048616e-09 loss: 0.8125404715538025\n",
      "lr: 4.580434660111932e-10 loss: 0.6704853773117065\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2010, 1.772390159178729)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model=model, train_dataset=train_dataset, tokenizer=tokenizer, train_batch_size=32,\n",
    "      learning_rate=3e-5, num_train_epochs=10, evaluate_during_training=True, logging_steps=200,\n",
    "      max_grad_norm=1.0, save_steps=-1, num_cycles=5.0) #  , max_steps=3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pred on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CamembertForSequenceClassification.from_pretrained(\"model_2019-12-08_14:59:37/checkpoint-400/checkpoint-800/checkpoint-1200/\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features from cached file data/cached_test_128\n",
      "***** Running evaluation  *****\n",
      "  Num examples = %d 1606\n",
      "  Batch size = %d 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6b4d334a0647c39002d14840891af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=201, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 1.2128427743318662, 'val_acc': 0.7440846824408468}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pred on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features from dataset file at data/\n",
      "['ID', 'question']\n",
      "{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '10': 10, '11': 11, '12': 12, '13': 13, '14': 14, '15': 15, '16': 16, '17': 17, '18': 18, '19': 19, '20': 20, '21': 21, '22': 22, '23': 23, '24': 24, '25': 25, '26': 26, '27': 27, '28': 28, '29': 29, '30': 30, '31': 31, '32': 32, '33': 33, '34': 34, '35': 35, '36': 36, '37': 37, '38': 38, '39': 39, '40': 40, '41': 41, '42': 42, '43': 43, '44': 44, '45': 45, '46': 46, '47': 47, '48': 48, '49': 49, '50': 50}\n",
      "Writing example 0\n",
      "Saving features into cached file data/cached_dev_128\n"
     ]
    }
   ],
   "source": [
    "eval_batch_size=8\n",
    "\n",
    "eval_dataset = load_and_cache_examples(tokenizer=tokenizer, max_seq_length=128, data_dir=\"data/\",\n",
    "                                       which_data=\"dev\", overwrite_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "  Num examples = %d 2035\n",
      "  Batch size = %d 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2573b6fada3043e292afd75b3b0aa4b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=255, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_batch_size = eval_batch_size\n",
    "# Note that DistributedSampler samples randomly\n",
    "eval_sampler = SequentialSampler(eval_dataset)\n",
    "eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "# Eval!\n",
    "print(\"***** Running evaluation *****\")\n",
    "print(\"  Num examples = %d\", len(eval_dataset))\n",
    "print(\"  Batch size = %d\", eval_batch_size)\n",
    "\n",
    "eval_loss = 0.0\n",
    "nb_eval_steps = 0\n",
    "preds = None\n",
    "out_label_ids = None\n",
    "for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    model.eval()\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[3]}\n",
    "        inputs['token_type_ids'] = batch[2] #or None\n",
    "        outputs = model(**inputs)\n",
    "        tmp_eval_loss, logits = outputs[:2]\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "    nb_eval_steps += 1\n",
    "    if preds is None:\n",
    "        preds = logits.detach().cpu().numpy()\n",
    "        out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "    else:\n",
    "        preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "        out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "preds_class = np.argmax(preds, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = pd.read_csv(path_data + \"dev.tsv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({\"ID\": dev_data[\"ID\"].values,  \"intention\": preds_class})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>intention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8028</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8029</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8030</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8031</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8032</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID  intention\n",
       "0  8028         32\n",
       "1  8029         32\n",
       "2  8030         32\n",
       "3  8031         31\n",
       "4  8032         44"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(\"sub/sub1_same_as_0.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
